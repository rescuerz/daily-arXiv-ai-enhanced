<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 23]
- [cs.AI](#cs.AI) [Total: 56]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Mind the Motions: Benchmarking Theory-of-Mind in Everyday Body Language](https://arxiv.org/abs/2511.15887)
*Seungbeen Lee,Jinhong Jeong,Donghyun Kim,Yejin Son,Youngjae Yu*

Main category: cs.CL

TL;DR: Motion2Mind is a new framework and dataset for evaluating AI's Theory of Mind capabilities in interpreting nonverbal cues, revealing current systems struggle significantly with detecting and explaining nonverbal communication compared to humans.


<details>
  <summary>Details</summary>
Motivation: Existing Theory of Mind benchmarks focus primarily on false-belief tasks and asymmetric information reasoning, overlooking other mental states and the rich complexity of human nonverbal communication.

Method: Created Motion2Mind framework using expert-curated body-language reference as knowledge base, building a curated video dataset with fine-grained nonverbal cue annotations and manually verified psychological interpretations covering 222 cue types and 397 mind states.

Result: Current AI systems show substantial performance gap in nonverbal cue detection and exhibit patterns of over-interpretation in explanation compared to human annotators.

Conclusion: The Motion2Mind framework reveals significant limitations in current AI's ability to interpret nonverbal communication, highlighting the need for improved Theory of Mind capabilities in machines.

Abstract: Our ability to interpret others' mental states through nonverbal cues (NVCs) is fundamental to our survival and social cohesion. While existing Theory of Mind (ToM) benchmarks have primarily focused on false-belief tasks and reasoning with asymmetric information, they overlook other mental states beyond belief and the rich tapestry of human nonverbal communication. We present Motion2Mind, a framework for evaluating the ToM capabilities of machines in interpreting NVCs. Leveraging an expert-curated body-language reference as a proxy knowledge base, we build Motion2Mind, a carefully curated video dataset with fine-grained nonverbal cue annotations paired with manually verified psychological interpretations. It encompasses 222 types of nonverbal cues and 397 mind states. Our evaluation reveals that current AI systems struggle significantly with NVC interpretation, exhibiting not only a substantial performance gap in Detection, as well as patterns of over-interpretation in Explanation compared to human annotators.

</details>


### [2] [TOD-ProcBench: Benchmarking Complex Instruction-Following in Task-Oriented Dialogues](https://arxiv.org/abs/2511.15976)
*Sarik Ghazarian,Abhinav Gullapalli,Swair Shah,Anurag Beniwal,Nanyun Peng,Narayanan Sadagopan,Zhou Yu*

Main category: cs.CL

TL;DR: TOD-ProcBench is a challenging benchmark for evaluating LLMs' ability to follow complex process instructions in multi-turn task-oriented dialogues, featuring intricate constraints and three evaluation tasks.


<details>
  <summary>Details</summary>
Motivation: Existing TOD benchmarks oversimplify complex real-world instructions by reducing them to simple schemas, creating a gap in systematically evaluating LLMs' instruction-following capabilities for complex process instructions.

Method: Created benchmark from ABCD dataset with human quality control, formulated constraints as multi-level condition-action statements, and designed three tasks: relevant statement retrieval, instruction-violation detection, and conditional response generation.

Result: The benchmark evaluates various LLMs' abilities to understand and follow complex instructions in multi-turn TODs, with additional studies on multilingual settings and instruction text formats.

Conclusion: TOD-ProcBench provides a comprehensive framework for benchmarking LLMs' complex instruction-following capabilities in realistic task-oriented dialogue scenarios, addressing limitations of existing simplified benchmarks.

Abstract: In real-world task-oriented dialogue (TOD) settings, agents are required to strictly adhere to complex instructions while conducting multi-turn conversations with customers. These instructions are typically presented in natural language format and include general guidelines and step-by-step procedures with complex constraints. Existing TOD benchmarks often oversimplify the complex nature of these instructions by reducing them to simple schemas composed of intents, slots, and API call configurations. To address this gap and systematically benchmark LLMs' instruction-following capabilities, we propose TOD-ProcBench, a challenging benchmark featuring complex process instructions with intricate, fine-grained constraints that evaluates various LLMs' abilities to understand and follow instructions in multi-turn TODs. Our benchmark dataset comprises instruction documents derived from the high-quality ABCD dataset with corresponding conversations under human quality control. We formulate fine-grained constraints and action procedures as multi-level condition-action instruction statements. We design three tasks to comprehensively benchmark LLMs' complex instruction-following capabilities in multi-turn TODs. Task 1 evaluates how LLMs retrieve the most relevant statement from a complex instruction and predict the corresponding next action. In Task 2, we synthesize instruction-violating responses by injecting inconsistencies and manipulating the original instructions, and then we analyze how effectively LLMs can identify instruction-violating responses. Task 3 investigates LLMs' abilities in conditional generation of instruction-following responses based on the original complex instructions. Additionally, we conduct studies on the impact of multilingual settings and different instruction text formats on compliance performance. We release our benchmark under the Llama 3.3 Community License Agreement.

</details>


### [3] [Liars' Bench: Evaluating Lie Detectors for Language Models](https://arxiv.org/abs/2511.16035)
*Kieron Kretschmar,Walter Laurito,Sharan Maiya,Samuel Marks*

Main category: cs.CL

TL;DR: LIARS' BENCH is a comprehensive testbed with 72,863 examples of lies and honest responses from four models across seven datasets, designed to evaluate lie detection techniques for LLMs across diverse lying scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing lie detection techniques for LLMs are validated in narrow settings that don't capture the diverse types of lies models can generate, limiting their practical effectiveness.

Method: Created LIARS' BENCH testbed with examples varying along two dimensions: model's reason for lying and object of belief targeted. Evaluated three black- and white-box lie detection techniques on this comprehensive dataset.

Result: Existing techniques systematically fail to identify certain types of lies, particularly in settings where determining if the model lied cannot be done from the transcript alone.

Conclusion: LIARS' BENCH reveals limitations in prior lie detection techniques and provides a practical testbed to guide future progress in detecting when LLMs generate false statements they believe to be false.

Abstract: Prior work has introduced techniques for detecting when large language models (LLMs) lie, that is, generating statements they believe are false. However, these techniques are typically validated in narrow settings that do not capture the diverse lies LLMs can generate. We introduce LIARS' BENCH, a testbed consisting of 72,863 examples of lies and honest responses generated by four open-weight models across seven datasets. Our settings capture qualitatively different types of lies and vary along two dimensions: the model's reason for lying and the object of belief targeted by the lie. Evaluating three black- and white-box lie detection techniques on LIARS' BENCH, we find that existing techniques systematically fail to identify certain types of lies, especially in settings where it's not possible to determine whether the model lied from the transcript alone. Overall, LIARS' BENCH reveals limitations in prior techniques and provides a practical testbed for guiding progress in lie detection.

</details>


### [4] [Learning Tractable Distributions Of Language Model Continuations](https://arxiv.org/abs/2511.16054)
*Gwen Yidou-Weng,Ian Li,Anji Liu,Oliver Broadrick,Guy Van den Broeck,Benjie Wang*

Main category: cs.CL

TL;DR: LTLA is a hybrid approach that combines a base language model with a fixed tractable surrogate model to enable controlled text generation under sequence-level constraints, addressing efficiency issues in neural context integration while improving constraint satisfaction.


<details>
  <summary>Details</summary>
Motivation: Existing methods for controlled language generation using tractable surrogates like HMMs are weakly context aware, which reduces query quality. There's a need for approaches that can handle constraints depending on future tokens while maintaining efficiency.

Method: LTLA pairs a base language model for rich prefix encoding with a fixed tractable surrogate model that computes exact continuation probabilities. It uses a single batched HMM update for all next-token candidates and conditions only the surrogate's latent state prior on the LM's hidden representations while keeping the surrogate decoder fixed.

Result: LTLA attains higher conditional likelihood than unconditional HMMs, approximates continuation distributions for vision-language models where standalone HMMs cannot encode visual context, and improves constraint satisfaction at comparable fluency on controlled-generation tasks with minimal inference overhead.

Conclusion: The proposed LTLA method effectively addresses efficiency pitfalls in neural context integration for controlled language generation, demonstrating improved performance over existing surrogate-based approaches while maintaining computational efficiency.

Abstract: Controlled language generation conditions text on sequence-level constraints (for example, syntax, style, or safety). These constraints may depend on future tokens, which makes directly conditioning an autoregressive language model (LM) generally intractable. Prior work uses tractable surrogates such as hidden Markov models (HMMs) to approximate the distribution over continuations and adjust the model's next-token logits at decoding time. However, we find that these surrogates are often weakly context aware, which reduces query quality. We propose Learning to Look Ahead (LTLA), a hybrid approach that pairs the same base language model for rich prefix encoding with a fixed tractable surrogate model that computes exact continuation probabilities. Two efficiency pitfalls arise when adding neural context: (i) naively rescoring the prefix with every candidate next token requires a sweep over the entire vocabulary at each step, and (ii) predicting fresh surrogate parameters for each prefix, although tractable at a single step, forces recomputation of future probabilities for every new prefix and eliminates reuse. LTLA avoids both by using a single batched HMM update to account for all next-token candidates at once, and by conditioning only the surrogate's latent state prior on the LM's hidden representations while keeping the surrogate decoder fixed, so computations can be reused across prefixes. Empirically, LTLA attains higher conditional likelihood than an unconditional HMM, approximates continuation distributions for vision-language models where a standalone HMM cannot encode visual context, and improves constraint satisfaction at comparable fluency on controlled-generation tasks, with minimal inference overhead.

</details>


### [5] [Early science acceleration experiments with GPT-5](https://arxiv.org/abs/2511.16072)
*Sébastien Bubeck,Christian Coester,Ronen Eldan,Timothy Gowers,Yin Tat Lee,Alexandru Lupsasca,Mehtaab Sawhney,Robert Scherrer,Mark Sellke,Brian K. Spears,Derya Unutmaz,Kevin Weil,Steven Yin,Nikita Zhivotovskiy*

Main category: cs.CL

TL;DR: GPT-5 demonstrates practical utility in scientific research across multiple disciplines, producing new mathematical results and accelerating research processes while highlighting the importance of human-AI collaboration.


<details>
  <summary>Details</summary>
Motivation: To showcase the concrete capabilities of frontier AI like GPT-5 in accelerating scientific research across various fields and demonstrate successful human-AI collaboration patterns.

Method: Collection of case studies across mathematics, physics, astronomy, computer science, biology, and materials science documenting human-GPT-5 interactions and collaboration patterns.

Result: GPT-5 produced new, concrete research steps and four new verified mathematical results, accelerating work while requiring human guidance and verification.

Conclusion: Frontier AI like GPT-5 can meaningfully contribute to scientific discovery and accelerate research, but human expertise remains essential for guidance, verification, and collaboration.

Abstract: AI models like GPT-5 are an increasingly valuable tool for scientists, but many remain unaware of the capabilities of frontier AI. We present a collection of short case studies in which GPT-5 produced new, concrete steps in ongoing research across mathematics, physics, astronomy, computer science, biology, and materials science. In these examples, the authors highlight how AI accelerated their work, and where it fell short; where expert time was saved, and where human input was still key. We document the interactions of the human authors with GPT-5, as guiding examples of fruitful collaboration with AI. Of note, this paper includes four new results in mathematics (carefully verified by the human authors), underscoring how GPT-5 can help human mathematicians settle previously unsolved problems. These contributions are modest in scope but profound in implication, given the rate at which frontier AI is progressing.

</details>


### [6] [ELPO: Ensemble Learning Based Prompt Optimization for Large Language Models](https://arxiv.org/abs/2511.16122)
*Qing Zhang,Bing Xu,Xudong Zhang,Yifan Shi,Yang Li,Chen Zhang,Yik Chung Wu,Ngai Wong,Yijie Chen,Hong Dai,Xiansen Chen,Mian Zhang*

Main category: cs.CL

TL;DR: ELPO is a novel ensemble learning-based framework for automatic prompt optimization that outperforms existing methods by using voting mechanisms and diverse search strategies.


<details>
  <summary>Details</summary>
Motivation: Manual prompt engineering is laborious and existing automatic prompt optimization methods are limited by using single models/algorithms, which restricts performance on complex tasks.

Method: Proposes ELPO framework using ensemble learning with voting mechanism, shared generation strategies, and different search methods for prompt optimization, along with more efficient algorithms for generation and search.

Result: ELPO outperforms state-of-the-art methods across different tasks, achieving improvements like 7.6 F1 score increase on ArSarcasm dataset.

Conclusion: ELPO provides more accurate and robust automatic prompt optimization through ensemble learning approach, effectively addressing limitations of single-algorithm methods.

Abstract: The remarkable performance of Large Language Models (LLMs) highly relies on crafted prompts. However, manual prompt engineering is a laborious process, creating a core bottleneck for practical application of LLMs. This phenomenon has led to the emergence of a new research area known as Automatic Prompt Optimization (APO), which develops rapidly in recent years. Existing APO methods such as those based on evolutionary algorithms or trial-and-error approaches realize an efficient and accurate prompt optimization to some extent. However, those researches focus on a single model or algorithm for the generation strategy and optimization process, which limits their performance when handling complex tasks. To address this, we propose a novel framework called Ensemble Learning based Prompt Optimization (ELPO) to achieve more accurate and robust results. Motivated by the idea of ensemble learning, ELPO conducts voting mechanism and introduces shared generation strategies along with different search methods for searching superior prompts. Moreover, ELPO creatively presents more efficient algorithms for the prompt generation and search process. Experimental results demonstrate that ELPO outperforms state-of-the-art prompt optimization methods across different tasks, e.g., improving F1 score by 7.6 on ArSarcasm dataset.

</details>


### [7] [TS-PEFT: Token-Selective Parameter-Efficient Fine-Tuning with Learnable Threshold Gating](https://arxiv.org/abs/2511.16147)
*Dabiao Ma,Ziming Dai,Zhimin Xin,Shu Wang,Ye Wang,Haojun Fei*

Main category: cs.CL

TL;DR: This paper introduces Token-Selective PEFT (TS-PEFT), a new paradigm that selectively applies PEFT modifications to specific position indices rather than all indices, challenging the traditional approach.


<details>
  <summary>Details</summary>
Motivation: To question the necessity of applying PEFT modifications to all position indices and explore whether selective application could improve performance and efficiency.

Method: Proposed Token-Selective PEFT (TS-PEFT) where a function S selectively applies PEFT modifications to a subset of position indices rather than all indices.

Result: Experimental results show that indiscriminate PEFT application to all indices is unnecessary and potentially counterproductive, while selective application can enhance performance.

Conclusion: The study advocates for a more targeted approach to PEFT modifications and provides a framework for optimizing fine-tuning processes in large models.

Abstract: In the field of large models (LMs) for natural language processing (NLP) and computer vision (CV), Parameter-Efficient Fine-Tuning (PEFT) has emerged as a resource-efficient method that modifies a limited number of parameters while keeping the pretrained weights fixed. This paper investigates the traditional PEFT approach, which applies modifications to all position indices, and questions its necessity. We introduce a new paradigm called Token-Selective PEFT (TS-PEFT), in which a function S selectively applies PEFT modifications to a subset of position indices, potentially enhancing performance on downstream tasks. Our experimental results reveal that the indiscriminate application of PEFT to all indices is not only superfluous, but may also be counterproductive. This study offers a fresh perspective on PEFT, advocating for a more targeted approach to modifications and providing a framework for future research to optimize the fine-tuning process for large models.

</details>


### [8] [SemanticCite: Citation Verification with AI-Powered Full-Text Analysis and Evidence-Based Reasoning](https://arxiv.org/abs/2511.16198)
*Sebastian Haan*

Main category: cs.CL

TL;DR: SemanticCite is an AI-powered system that verifies citation accuracy through full-text analysis, classifies claim-source relationships into four categories, and provides transparent explanations with relevant text snippets.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in academic literature including semantic citation errors, AI-generated hallucinated references, and traditional citation formats that lack specificity about which paper sections support claims.

Method: Combines multiple retrieval methods with a four-class classification system (Supported, Partially Supported, Unsupported, Uncertain) using fine-tuned lightweight language models for full-text source analysis.

Result: Fine-tuned lightweight models achieve performance comparable to large commercial systems with significantly lower computational requirements, making large-scale citation verification practically feasible.

Conclusion: SemanticCite provides a scalable solution for research integrity through citation verification, peer review streamlining, and quality control for AI-generated content, with open-source framework and comprehensive dataset.

Abstract: Effective scientific communication depends on accurate citations that validate sources and guide readers to supporting evidence. Yet academic literature faces mounting challenges: semantic citation errors that misrepresent sources, AI-generated hallucinated references, and traditional citation formats that point to entire papers without indicating which sections substantiate specific claims. We introduce SemanticCite, an AI-powered system that verifies citation accuracy through full-text source analysis while providing rich contextual information via detailed reasoning and relevant text snippets. Our approach combines multiple retrieval methods with a four-class classification system (Supported, Partially Supported, Unsupported, Uncertain) that captures nuanced claim-source relationships and enables appropriate remedial actions for different error types. Our experiments show that fine-tuned lightweight language models achieve performance comparable to large commercial systems with significantly lower computational requirements, making large-scale citation verification practically feasible. The system provides transparent, evidence-based explanations that support user understanding and trust. We contribute a comprehensive dataset of over 1,000 citations with detailed alignments, functional classifications, semantic annotations, and bibliometric metadata across eight disciplines, alongside fine-tuned models and the complete verification framework as open-source software. SemanticCite addresses critical challenges in research integrity through scalable citation verification, streamlined peer review, and quality control for AI-generated content, providing an open-source foundation for maintaining citation accuracy at scale.

</details>


### [9] [SeSE: A Structural Information-Guided Uncertainty Quantification Framework for Hallucination Detection in LLMs](https://arxiv.org/abs/2511.16275)
*Xingtao Zhao,Hao Peng,Dingli Su,Xianghua Zeng,Chunyang Liu,Jinzhi Liao,Philip S. Yu*

Main category: cs.CL

TL;DR: Semantic Structural Entropy (SeSE) is a new uncertainty quantification framework for LLMs that uses structural information from semantic graphs to detect hallucinations more effectively than existing methods.


<details>
  <summary>Details</summary>
Motivation: Current UQ methods for LLMs focus on probability distributions or pairwise distances but ignore latent semantic structural information, which could provide more precise uncertainty estimates for safety-critical applications.

Method: Developed adaptively sparsified directed semantic graphs to capture semantic dependencies, then defined SeSE as structural entropy of optimal semantic encoding trees through hierarchical abstraction, and extended it for fine-grained uncertainty quantification in long-form generation.

Result: Extensive experiments across 29 model-dataset combinations show SeSE significantly outperforms advanced UQ baselines, including strong supervised methods and the recently proposed KLE.

Conclusion: SeSE provides a principled framework for quantifying LLM uncertainty from a structural information perspective, enabling more reliable hallucination detection and outperforming state-of-the-art methods.

Abstract: Reliable uncertainty quantification (UQ) is essential for deploying large language models (LLMs) in safety-critical scenarios, as it enables them to abstain from responding when uncertain, thereby avoiding hallucinating falsehoods. However, state-of-the-art UQ methods primarily rely on semantic probability distributions or pairwise distances, overlooking latent semantic structural information that could enable more precise uncertainty estimates. This paper presents Semantic Structural Entropy (SeSE), a principled UQ framework that quantifies the inherent semantic uncertainty of LLMs from a structural information perspective for hallucination detection. Specifically, to effectively model semantic spaces, we first develop an adaptively sparsified directed semantic graph construction algorithm that captures directional semantic dependencies while automatically pruning unnecessary connections that introduce negative interference. We then exploit latent semantic structural information through hierarchical abstraction: SeSE is defined as the structural entropy of the optimal semantic encoding tree, formalizing intrinsic uncertainty within semantic spaces after optimal compression. A higher SeSE value corresponds to greater uncertainty, indicating that LLMs are highly likely to generate hallucinations. In addition, to enhance fine-grained UQ in long-form generation -- where existing methods often rely on heuristic sample-and-count techniques -- we extend SeSE to quantify the uncertainty of individual claims by modeling their random semantic interactions, providing theoretically explicable hallucination detection. Extensive experiments across 29 model-dataset combinations show that SeSE significantly outperforms advanced UQ baselines, including strong supervised methods and the recently proposed KLE.

</details>


### [10] [SDA: Steering-Driven Distribution Alignment for Open LLMs without Fine-Tuning](https://arxiv.org/abs/2511.16324)
*Wei Xia,Zhi-Hong Deng*

Main category: cs.CL

TL;DR: SDA is a training-free, model-agnostic alignment framework that dynamically redistributes LLM output probabilities based on user instructions to improve alignment with human intent across helpfulness, harmlessness, and honesty dimensions.


<details>
  <summary>Details</summary>
Motivation: As LLMs are increasingly deployed in real-world applications, ensuring their responses align with human intent without costly retraining or extensive supervision remains a critical challenge.

Method: SDA dynamically redistributes model output probabilities based on user-defined alignment instructions, functioning as a lightweight, resource-efficient framework that works during inference without fine-tuning.

Result: SDA achieved average gains of 64.4% in helpfulness, 30% in honesty, and 11.5% in harmlessness across 8 open-source LLMs of varying scales and origins.

Conclusion: SDA effectively improves alignment performance across diverse models and application scenarios, supporting personalized preference alignment and integration with training-based strategies.

Abstract: With the rapid advancement of large language models (LLMs), their deployment in real-world applications has become increasingly widespread. LLMs are expected to deliver robust performance across diverse tasks, user preferences, and practical scenarios. However, as demands grow, ensuring that LLMs produce responses aligned with human intent remains a foundational challenge. In particular, aligning model behavior effectively and efficiently during inference, without costly retraining or extensive supervision, is both a critical requirement and a non-trivial technical endeavor. To address the challenge, we propose SDA (Steering-Driven Distribution Alignment), a training-free and model-agnostic alignment framework designed for open-source LLMs. SDA dynamically redistributes model output probabilities based on user-defined alignment instructions, enhancing alignment between model behavior and human intents without fine-tuning. The method is lightweight, resource-efficient, and compatible with a wide range of open-source LLMs. It can function independently during inference or be integrated with training-based alignment strategies. Moreover, SDA supports personalized preference alignment, enabling flexible control over the model response behavior. Empirical results demonstrate that SDA consistently improves alignment performance across 8 open-source LLMs with varying scales and diverse origins, evaluated on three key alignment dimensions, helpfulness, harmlessness, and honesty (3H). Specifically, SDA achieves average gains of 64.4% in helpfulness, 30% in honesty and 11.5% in harmlessness across the tested models, indicating its effectiveness and generalization across diverse models and application scenarios.

</details>


### [11] [Incorporating Self-Rewriting into Large Language Model Reasoning Reinforcement](https://arxiv.org/abs/2511.16331)
*Jiashu Yao,Heyan Huang,Shuang Zeng,Chuwei Luo,WangJie You,Jie Tang,Qingsong Liu,Yuhang Guo,Yangyang Kang*

Main category: cs.CL

TL;DR: Self-rewriting framework improves reasoning quality in large reasoning models by allowing models to rewrite their own reasoning texts, addressing issues like over-thinking and disordered-thinking while maintaining RL scalability.


<details>
  <summary>Details</summary>
Motivation: Traditional RL with outcome correctness rewards provides limited supervision over internal reasoning processes, leading to suboptimal reasoning quality with issues like over-thinking, under-thinking, redundant-thinking, and disordered-thinking.

Method: Proposes self-rewriting framework where models rewrite their own reasoning texts, using selective rewriting on 'simple' samples (defined by consistent correctness) to preserve original reward signals, with rewriting and vanilla generation compiled within single batches.

Result: Achieves improved accuracy (+0.6) with substantially shorter reasoning (-46%) without explicit length reduction instructions, and significantly higher internal reasoning quality scores (+7.2) under LLM-as-a-judge metric.

Conclusion: Self-rewriting effectively mitigates internal reasoning flaws and improves reasoning quality while maintaining RL scalability with minimal overhead (~10%), outperforming existing baselines on accuracy-length tradeoff.

Abstract: Through reinforcement learning (RL) with outcome correctness rewards, large reasoning models (LRMs) with scaled inference computation have demonstrated substantial success on complex reasoning tasks. However, the one-sided reward, focused solely on final correctness, limits its ability to provide detailed supervision over internal reasoning process. This deficiency leads to suboptimal internal reasoning quality, manifesting as issues like over-thinking, under-thinking, redundant-thinking, and disordered-thinking. Inspired by the recent progress in LRM self-rewarding, we introduce self-rewriting framework, where a model rewrites its own reasoning texts, and subsequently learns from the rewritten reasoning to improve the internal thought process quality. For algorithm design, we propose a selective rewriting approach wherein only "simple" samples, defined by the model's consistent correctness, are rewritten, thereby preserving all original reward signals of GRPO. For practical implementation, we compile rewriting and vanilla generation within one single batch, maintaining the scalability of the RL algorithm and introducing only ~10% overhead. Extensive experiments on diverse tasks with different model sizes validate the effectiveness of self-rewriting. In terms of the accuracy-length tradeoff, the self-rewriting approach achieves improved accuracy (+0.6) with substantially shorter reasoning (-46%) even without explicit instructions in rewriting prompts to reduce reasoning length, outperforming existing strong baselines. In terms of internal reasoning quality, self-rewriting achieves significantly higher scores (+7.2) under the LLM-as-a-judge metric, successfully mitigating internal reasoning flaws.

</details>


### [12] [NLP Datasets for Idiom and Figurative Language Tasks](https://arxiv.org/abs/2511.16345)
*Blake Matheny,Phuong Minh Nguyen,Minh Le Nguyen,Stephanie Reynolds*

Main category: cs.CL

TL;DR: This paper addresses the challenge of idiomatic and figurative language understanding in LLMs by creating new datasets for idiom recognition and evaluation.


<details>
  <summary>Details</summary>
Motivation: Idioms and figurative language remain difficult for LLMs despite large corpora, requiring specialized datasets to improve model performance.

Method: Combined existing idiom datasets to create idiom lists, retrieved context sequences from large corpus, created three datasets (one large-scale potential idioms, two human-annotated definite idioms), and used them for idiom detection tasks.

Result: Developed comprehensive datasets for evaluating LLMs on figurative language understanding through slot labeling and sequence tagging tasks.

Conclusion: The created datasets provide resources to narrow the gap in LLMs' figurative language understanding and enable development of new approaches for idiom recognition.

Abstract: Idiomatic and figurative language form a large portion of colloquial speech and writing. With social media, this informal language has become more easily observable to people and trainers of large language models (LLMs) alike. While the advantage of large corpora seems like the solution to all machine learning and Natural Language Processing (NLP) problems, idioms and figurative language continue to elude LLMs. Finetuning approaches are proving to be optimal, but better and larger datasets can help narrow this gap even further. The datasets presented in this paper provide one answer, while offering a diverse set of categories on which to build new models and develop new approaches. A selection of recent idiom and figurative language datasets were used to acquire a combined idiom list, which was used to retrieve context sequences from a large corpus. One large-scale dataset of potential idiomatic and figurative language expressions and two additional human-annotated datasets of definite idiomatic and figurative language expressions were created to evaluate the baseline ability of pre-trained language models in handling figurative meaning through idiom recognition (detection) tasks. The resulting datasets were post-processed for model agnostic training compatibility, utilized in training, and evaluated on slot labeling and sequence tagging.

</details>


### [13] [Learning from Sufficient Rationales: Analysing the Relationship Between Explanation Faithfulness and Token-level Regularisation Strategies](https://arxiv.org/abs/2511.16353)
*Jonathan Kamp,Lisa Beinborn,Antske Fokkens*

Main category: cs.CL

TL;DR: This paper analyzes the limitations of sufficiency as a metric for evaluating rationales in natural language processing, relating it to token classification and attention regularization to better understand how rationale information affects model performance.


<details>
  <summary>Details</summary>
Motivation: Current metrics like sufficiency provide limited insight into how rationales actually impact model performance, and there's a need to better understand the relationship between rationale informativeness and model behavior across different modeling paradigms.

Method: The authors relate sufficiency to two modeling approaches: token classification (identifying rationale tokens) and attention regularization (incorporating rationales in inputs). They analyze how these approaches affect model performance, particularly in cross-domain classification scenarios.

Result: Highly informative rationales don't necessarily help with correct classification, sufficiency captures interference from non-rationalized context, rationale incorporation can boost cross-domain performance but inconsistently, and sufficiency appears unrelated to token classification performance.

Conclusion: Rationale evaluation is complex, and current metrics like sufficiency don't systematically capture the multifaceted nature of how rationales influence model behavior, highlighting the need for more comprehensive evaluation approaches.

Abstract: Human explanations of natural language, rationales, form a tool to assess whether models learn a label for the right reasons or rely on dataset-specific shortcuts. Sufficiency is a common metric for estimating the informativeness of rationales, but it provides limited insight into the effects of rationale information on model performance. We address this limitation by relating sufficiency to two modelling paradigms: the ability of models to identify which tokens are part of the rationale (through token classification) and the ability of improving model performance by incorporating rationales in the input (through attention regularisation). We find that highly informative rationales are not likely to help classify the instance correctly. Sufficiency conversely captures the classification impact of the non-rationalised context, which interferes with rationale information in the same input. We also find that incorporating rationale information in model inputs can boost cross-domain classification, but results are inconsistent per task and model type. Finally, sufficiency and token classification appear to be unrelated. These results exemplify the complexity of rationales, showing that metrics capable of systematically capturing this type of information merit further investigation.

</details>


### [14] [AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser](https://arxiv.org/abs/2511.16397)
*Ren Ma,Jiantao Qiu,Chao Xu,Pei Chu,Kaiwen Liu,Pengli Ren,Yuan Qu,Jiahui Peng,Linfeng Hou,Mengjie Liu,Lindong Lu,Wenchang Ning,Jia Yu,Rui Min,Jin Shi,Haojiong Chen,Peng Zhang,Wenjian Zhang,Qian Jiang,Zengjie Hu,Guoqiang Yang,Zhenxiang Li,Fukai Shang,Zhongying Tu,Wentao Zhang,Dahua Lin,Conghui He*

Main category: cs.CL

TL;DR: MinerU-HTML is a novel HTML-to-text extraction pipeline that uses a 0.6B-parameter language model to preserve document structure and structured elements (formulas, codes, tables) better than heuristic methods like Trafilatura, achieving 81.8% ROUGE-N F1 vs 63.6% and enabling creation of higher quality web corpora.


<details>
  <summary>Details</summary>
Motivation: Current web data curation focuses on filtering and deduplication while treating HTML extraction as fixed preprocessing, but heuristic extractors struggle to preserve document structure and corrupt structured elements, limiting downstream model performance.

Method: Reformulates content extraction as sequence labeling using a 0.6B-parameter language model with semantic understanding, employing a two-stage formatting pipeline that categorizes semantic elements before converting to Markdown.

Result: Achieves 81.8% ROUGE-N F1 vs Trafilatura's 63.6% on MainWebBench (7,887 pages), with exceptional structured element preservation (90.9% for code blocks, 94.0% for formulas). Models trained on AICC corpus (62B tokens) achieve 50.8% average accuracy across 13 benchmarks, outperforming TfCC by 1.08pp.

Conclusion: HTML extraction quality significantly impacts model capabilities and is a critical, often underestimated component of web corpus construction. MinerU-HTML's model-based approach provides scalable improvement over heuristic methods.

Abstract: While web data quality is crucial for large language models, most curation efforts focus on filtering and deduplication,treating HTML-to-text extraction as a fixed pre-processing step. Existing web corpora rely on heuristic-based extractors like Trafilatura, which struggle to preserve document structure and frequently corrupt structured elements such as formulas, codes, and tables. We hypothesize that improving extraction quality can be as impactful as aggressive filtering strategies for downstream performance. We introduce MinerU-HTML, a novel extraction pipeline that reformulates content extraction as a sequence labeling problem solved by a 0.6B-parameter language model. Unlike text-density heuristics, MinerU-HTML leverages semantic understanding and employs a two-stage formatting pipeline that explicitly categorizes semantic elements before converting to Markdown. Crucially, its model-based approach is inherently scalable, whereas heuristic methods offer limited improvement pathways. On MainWebBench, our benchmark of 7,887 annotated web pages, MinerU-HTML achieves 81.8\% ROUGE-N F1 compared to Trafilatura's 63.6\%, with exceptional structured element preservation (90.9\% for code blocks, 94.0\% for formulas). Using MinerU-HTML, we construct AICC (AI-ready Common Crawl), a 7.3-trillion token multilingual corpus from two Common Crawl snapshots. In controlled pretraining experiments where AICC and Trafilatura-extracted TfCC undergo identical filtering, models trained on AICC (62B tokens) achieve 50.8\% average accuracy across 13 benchmarks, outperforming TfCC by 1.08pp-providing direct evidence that extraction quality significantly impacts model capabilities. AICC also surpasses RefinedWeb and FineWeb on key benchmarks. We publicly release MainWebBench, MinerU-HTML, and AICC, demonstrating that HTML extraction is a critical, often underestimated component of web corpus construction.

</details>


### [15] [Classification of worldwide news articles by perceived quality, 2018-2024](https://arxiv.org/abs/2511.16416)
*Connor McElroy,Thiago E. A. de Oliveira,Chris Brogly*

Main category: cs.CL

TL;DR: This study demonstrates that both traditional machine learning and deep learning models can effectively distinguish between perceived low-quality and high-quality news articles, with deep learning models achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: To explore whether machine learning and deep learning models can effectively distinguish perceived lower-quality news articles from perceived higher-quality news articles.

Method: Used 3 machine learning classifiers and 3 deep learning models on a dataset of 1,412,272 English news articles from Common Crawl (2018-2024). Articles were classified based on expert consensus ratings split at median, with 194 linguistic features per website-level labeled article.

Result: Random Forest achieved 0.7355 accuracy and 0.8131 ROC AUC. ModernBERT-large (256 context) performed best with 0.8744 accuracy, 0.9593 ROC-AUC, and 0.8739 F1. DistilBERT-base (512 context) achieved 0.8685 accuracy and 0.9554 ROC-AUC.

Conclusion: Perceived quality of worldwide news articles can be effectively differentiated by both traditional CPU-based machine learning classifiers and deep learning classifiers, with deep learning models showing superior performance.

Abstract: This study explored whether supervised machine learning and deep learning models can effectively distinguish perceived lower-quality news articles from perceived higher-quality news articles. 3 machine learning classifiers and 3 deep learning models were assessed using a newly created dataset of 1,412,272 English news articles from the Common Crawl over 2018-2024. Expert consensus ratings on 579 source websites were split at the median, creating perceived low and high-quality classes of about 706,000 articles each, with 194 linguistic features per website-level labelled article. Traditional machine learning classifiers such as the Random Forest demonstrated capable performance (0.7355 accuracy, 0.8131 ROC AUC). For deep learning, ModernBERT-large (256 context length) achieved the best performance (0.8744 accuracy; 0.9593 ROC-AUC; 0.8739 F1), followed by DistilBERT-base (512 context length) at 0.8685 accuracy and 0.9554 ROC-AUC. DistilBERT-base (256 context length) reached 0.8478 accuracy and 0.9407 ROC-AUC, while ModernBERT-base (256 context length) attained 0.8569 accuracy and 0.9470 ROC-AUC. These results suggest that the perceived quality of worldwide news articles can be effectively differentiated by traditional CPU-based machine learning classifiers and deep learning classifiers.

</details>


### [16] [ESGBench: A Benchmark for Explainable ESG Question Answering in Corporate Sustainability Reports](https://arxiv.org/abs/2511.16438)
*Sherine George,Nithish Saji*

Main category: cs.CL

TL;DR: ESGBench is a benchmark dataset and evaluation framework for assessing explainable ESG question answering systems using corporate sustainability reports, with domain-grounded questions, human-curated answers, and supporting evidence.


<details>
  <summary>Details</summary>
Motivation: To accelerate research in transparent and accountable ESG-focused AI systems by providing a standardized way to evaluate model performance on ESG question answering.

Method: Created a benchmark dataset consisting of domain-grounded questions across multiple ESG themes, paired with human-curated answers and supporting evidence from corporate sustainability reports.

Result: Analysis of state-of-the-art LLMs on ESGBench revealed key challenges in factual consistency, traceability, and domain alignment.

Conclusion: ESGBench provides a valuable framework for evaluating explainable ESG question answering systems and identifying areas for improvement in factual consistency and reasoning transparency.

Abstract: We present ESGBench, a benchmark dataset and evaluation framework designed to assess explainable ESG question answering systems using corporate sustainability reports. The benchmark consists of domain-grounded questions across multiple ESG themes, paired with human-curated answers and supporting evidence to enable fine-grained evaluation of model reasoning. We analyze the performance of state-of-the-art LLMs on ESGBench, highlighting key challenges in factual consistency, traceability, and domain alignment. ESGBench aims to accelerate research in transparent and accountable ESG-focused AI systems.

</details>


### [17] [Anatomy of an Idiom: Tracing Non-Compositionality in Language Models](https://arxiv.org/abs/2511.16467)
*Andrew Gomes*

Main category: cs.CL

TL;DR: The paper investigates how transformer-based language models process idiomatic expressions using circuit discovery techniques, identifying specific attention patterns and mechanisms that balance computational efficiency and robustness.


<details>
  <summary>Details</summary>
Motivation: To understand how transformers handle non-compositional language like idioms and identify the computational mechanisms that enable efficient processing of such linguistic constructions.

Method: Used a modified path patching algorithm for circuit discovery and analysis, identifying "Idiom Heads" (attention heads that activate across idioms) and "augmented reception" (enhanced attention between idiom tokens).

Result: Found distinct computational patterns in idiom processing, identified specific attention heads that frequently activate across different idioms, and discovered enhanced attention mechanisms between idiom tokens due to earlier processing.

Conclusion: The findings provide insights into how transformers handle non-compositional language and suggest pathways for understanding the processing of more complex grammatical constructions, revealing mechanisms that balance computational efficiency and robustness.

Abstract: We investigate the processing of idiomatic expressions in transformer-based language models using a novel set of techniques for circuit discovery and analysis. First discovering circuits via a modified path patching algorithm, we find that idiom processing exhibits distinct computational patterns. We identify and investigate ``Idiom Heads,'' attention heads that frequently activate across different idioms, as well as enhanced attention between idiom tokens due to earlier processing, which we term ``augmented reception.'' We analyze these phenomena and the general features of the discovered circuits as mechanisms by which transformers balance computational efficiency and robustness. Finally, these findings provide insights into how transformers handle non-compositional language and suggest pathways for understanding the processing of more complex grammatical constructions.

</details>


### [18] [Arctic-Extract Technical Report](https://arxiv.org/abs/2511.16470)
*Mateusz Chiliński,Julita Ołtusek,Wojciech Jaśkowski*

Main category: cs.CL

TL;DR: Arctic-Extract is a lightweight (6.6 GiB) state-of-the-art model for extracting structured data from business documents, deployable on resource-constrained hardware like A10 GPUs and capable of processing up to 125 A4 pages.


<details>
  <summary>Details</summary>
Motivation: To create an efficient document understanding model that can extract structural data (QA, entities, tables) from business documents while being deployable on resource-constrained hardware.

Method: Developed Arctic-Extract model with specific training protocols designed for structural data extraction from both scanned and digital-born business documents.

Result: Achieved state-of-the-art performance in document understanding while maintaining a small size (6.6 GiB) and high processing capacity (125 A4 pages on A10 GPUs with 24GB memory).

Conclusion: Arctic-Extract successfully demonstrates that high-performance document understanding can be achieved with lightweight models deployable on resource-constrained hardware, making it suitable for practical business applications.

Abstract: Arctic-Extract is a state-of-the-art model designed for extracting structural data (question answering, entities and tables) from scanned or digital-born business documents. Despite its SoTA capabilities, the model is deployable on resource-constrained hardware, weighting only 6.6 GiB, making it suitable for deployment on devices with limited resources, such as A10 GPUs with 24 GB of memory. Arctic-Extract can process up to 125 A4 pages on those GPUs, making suitable for long document processing. This paper highlights Arctic-Extract's training protocols and evaluation results, demonstrating its strong performance in document understanding.

</details>


### [19] [TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval](https://arxiv.org/abs/2511.16528)
*Özay Ezerceli,Mahmoud El Hussieni,Selva Taş,Reyhan Bayraktar,Fatma Betül Terzioğlu,Yusuf Çelebi,Yağız Asker*

Main category: cs.CL

TL;DR: TurkColBERT is the first comprehensive benchmark comparing dense encoders and late-interaction models for Turkish retrieval, showing that smaller late-interaction models significantly outperform larger dense encoders while being more parameter-efficient.


<details>
  <summary>Details</summary>
Motivation: Neural information retrieval systems are underexplored for morphologically rich, lower-resource languages like Turkish, and late-interaction models have not been systematically evaluated despite dense bi-encoders dominating Turkish IR.

Method: Two-stage adaptation pipeline: fine-tuning English and multilingual encoders on Turkish NLI/STS tasks, then converting them into ColBERT-style retrievers using PyLate trained on MS MARCO-TR. Evaluated 10 models across five Turkish BEIR datasets.

Result: Strong parameter efficiency: 1.0M-parameter colbert-hash-nano-tr is 600× smaller than 600M turkish-e5-large while preserving over 71% of its average mAP. Late-interaction models 3-5× smaller than dense encoders significantly outperform them, with ColmmBERT-base-TR yielding up to +13.8% mAP on domain-specific tasks.

Conclusion: Late-interaction models offer superior performance and efficiency for Turkish retrieval. MUVERA+Rerank enables low-latency retrieval with 0.54 ms query times. Limitations include reliance on moderately sized datasets and translated benchmarks.

Abstract: Neural information retrieval systems excel in high-resource languages but remain underexplored for morphologically rich, lower-resource languages such as Turkish. Dense bi-encoders currently dominate Turkish IR, yet late-interaction models -- which retain token-level representations for fine-grained matching -- have not been systematically evaluated. We introduce TurkColBERT, the first comprehensive benchmark comparing dense encoders and late-interaction models for Turkish retrieval. Our two-stage adaptation pipeline fine-tunes English and multilingual encoders on Turkish NLI/STS tasks, then converts them into ColBERT-style retrievers using PyLate trained on MS MARCO-TR. We evaluate 10 models across five Turkish BEIR datasets covering scientific, financial, and argumentative domains. Results show strong parameter efficiency: the 1.0M-parameter colbert-hash-nano-tr is 600$\times$ smaller than the 600M turkish-e5-large dense encoder while preserving over 71\% of its average mAP. Late-interaction models that are 3--5$\times$ smaller than dense encoders significantly outperform them; ColmmBERT-base-TR yields up to +13.8\% mAP on domain-specific tasks. For production-readiness, we compare indexing algorithms: MUVERA+Rerank is 3.33$\times$ faster than PLAID and offers +1.7\% relative mAP gain. This enables low-latency retrieval, with ColmmBERT-base-TR achieving 0.54 ms query times under MUVERA. We release all checkpoints, configs, and evaluation scripts. Limitations include reliance on moderately sized datasets ($\leq$50K documents) and translated benchmarks, which may not fully reflect real-world Turkish retrieval conditions; larger-scale MUVERA evaluations remain necessary.

</details>


### [20] [Beyond Tokens in Language Models: Interpreting Activations through Text Genre Chunks](https://arxiv.org/abs/2511.16540)
*Éloïse Benito-Rodriguez,Einar Urdshals,Jasmina Nasufi,Nicky Pochinkov*

Main category: cs.CL

TL;DR: The paper presents a framework for predicting text genre from LLM activations using shallow learning models, achieving high F1-scores of up to 98% and 71% on two datasets with Mistral-7B.


<details>
  <summary>Details</summary>
Motivation: Understanding LLMs is crucial for safe deployment, but complicated by interpretability challenges and the impracticality of human-evaluating all outputs. The research aims to develop predictive capabilities for analyzing LLM behavior.

Method: Used Mistral-7B and two datasets to extract genre information from LLM activations, employing scikit-learn classifiers to predict text genre based on these activations.

Result: Achieved F1-scores of up to 98% and 71% across two datasets, consistently outperforming control tasks, demonstrating that text genres can be effectively inferred from LLM activations.

Conclusion: This provides a proof of concept that shallow learning models can successfully extract genre information from LLM activations, offering a promising approach for understanding LLM behavior without extensive human evaluation.

Abstract: Understanding Large Language Models (LLMs) is key to ensure their safe and beneficial deployment. This task is complicated by the difficulty of interpretability of LLM structures, and the inability to have all their outputs human-evaluated. In this paper, we present the first step towards a predictive framework, where the genre of a text used to prompt an LLM, is predicted based on its activations. Using Mistral-7B and two datasets, we show that genre can be extracted with F1-scores of up to 98% and 71% using scikit-learn classifiers. Across both datasets, results consistently outperform the control task, providing a proof of concept that text genres can be inferred from LLMs with shallow learning models.

</details>


### [21] [WER is Unaware: Assessing How ASR Errors Distort Clinical Understanding in Patient Facing Dialogue](https://arxiv.org/abs/2511.16544)
*Zachary Ellis,Jared Joselowitz,Yash Deo,Yajie He,Anna Kalygina,Aisling Higham,Mana Rahimzadeh,Yan Jia,Ibrahim Habli,Ernest Lim*

Main category: cs.CL

TL;DR: This paper challenges the use of Word Error Rate (WER) for evaluating ASR in clinical settings, showing poor correlation with clinical impact. It introduces an LLM-based automated framework that achieves human-comparable performance in assessing clinical risk from transcription errors.


<details>
  <summary>Details</summary>
Motivation: Standard ASR evaluations rely heavily on WER, but this may not reflect the actual clinical impact of transcription errors in medical dialogues. There's a need for metrics that better capture safety implications in healthcare settings.

Method: Established gold-standard benchmark using expert clinicians to label clinical impact of ASR errors. Introduced LLM-as-a-Judge approach optimized with GEPA to replicate expert clinical assessments, using Gemini-2.5-Pro model.

Result: WER and existing metrics showed poor correlation with clinician-assigned risk labels. The optimized LLM judge achieved 90% accuracy and Cohen's κ of 0.816, demonstrating human-comparable performance in clinical risk assessment.

Conclusion: The work provides a validated automated framework for moving beyond simple textual fidelity to scalable safety assessment of ASR in clinical dialogue, addressing the gap between technical accuracy and clinical impact.

Abstract: As Automatic Speech Recognition (ASR) is increasingly deployed in clinical dialogue, standard evaluations still rely heavily on Word Error Rate (WER). This paper challenges that standard, investigating whether WER or other common metrics correlate with the clinical impact of transcription errors. We establish a gold-standard benchmark by having expert clinicians compare ground-truth utterances to their ASR-generated counterparts, labeling the clinical impact of any discrepancies found in two distinct doctor-patient dialogue datasets. Our analysis reveals that WER and a comprehensive suite of existing metrics correlate poorly with the clinician-assigned risk labels (No, Minimal, or Significant Impact). To bridge this evaluation gap, we introduce an LLM-as-a-Judge, programmatically optimized using GEPA to replicate expert clinical assessment. The optimized judge (Gemini-2.5-Pro) achieves human-comparable performance, obtaining 90% accuracy and a strong Cohen's $κ$ of 0.816. This work provides a validated, automated framework for moving ASR evaluation beyond simple textual fidelity to a necessary, scalable assessment of safety in clinical dialogue.

</details>


### [22] [Integrating Symbolic Natural Language Understanding and Language Models for Word Sense Disambiguation](https://arxiv.org/abs/2511.16577)
*Kexin Zhao,Ken Forbus*

Main category: cs.CL

TL;DR: A method using statistical language models as oracles for word sense disambiguation without hand-annotated training data, by converting candidate meanings into natural language alternatives and querying LLMs to select appropriate interpretations.


<details>
  <summary>Details</summary>
Motivation: Current WSD methods focus on coarse-grained representations and require hand-annotated training data, making it difficult to automatically disambiguate richer representations needed for sophisticated inference.

Method: Convert multiple candidate meanings from symbolic NLU systems into distinguishable natural language alternatives, query LLMs to select appropriate interpretations given context, and propagate selected meanings back to the symbolic system.

Result: The method was evaluated against human-annotated gold answers to demonstrate effectiveness.

Conclusion: The proposed approach enables automatic disambiguation of richer semantic representations without requiring hand-annotated training data.

Abstract: Word sense disambiguation is a fundamental challenge in natural language understanding. Current methods are primarily aimed at coarse-grained representations (e.g. WordNet synsets or FrameNet frames) and require hand-annotated training data to construct. This makes it difficult to automatically disambiguate richer representations (e.g. built on OpenCyc) that are needed for sophisticated inference. We propose a method that uses statistical language models as oracles for disambiguation that does not require any hand-annotation of training data. Instead, the multiple candidate meanings generated by a symbolic NLU system are converted into distinguishable natural language alternatives, which are used to query an LLM to select appropriate interpretations given the linguistic context. The selected meanings are propagated back to the symbolic NLU system. We evaluate our method against human-annotated gold answers to demonstrate its effectiveness.

</details>


### [23] [Comparison of Text-Based and Image-Based Retrieval in Multimodal Retrieval Augmented Generation Large Language Model Systems](https://arxiv.org/abs/2511.16654)
*Elias Lumer,Alex Cardenas,Matt Melich,Myles Mason,Sara Dieter,Vamse Kumar Subbiah,Pradeep Honaganahalli Basavaraju,Roberto Hernandez*

Main category: cs.CL

TL;DR: Direct multimodal embedding retrieval outperforms text-based approaches in RAG systems by preserving visual context, achieving 13% absolute improvement in mAP@5 and 11% in nDCG@5 on financial documents.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal RAG systems lose critical visual information by converting images to text through LLM summarization, limiting retrieval and QA performance.

Method: Comparative analysis of text-based chunk retrieval (images summarized to text) vs direct multimodal embedding retrieval (images stored natively in vector space), evaluated across 6 LLM models and 2 multimodal embedding models on a financial earnings call benchmark.

Result: Direct multimodal embedding retrieval significantly outperforms LLM-summary-based approaches with 13% absolute improvement in mAP@5 and 11% in nDCG@5, corresponding to 32% relative improvement in mAP@5 and 20% in nDCG@5.

Conclusion: Direct multimodal embeddings preserve visual context better than LLM summarization, leading to more accurate and factually consistent answers in multimodal RAG systems.

Abstract: Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models (LLMs) to access multimodal knowledge bases containing both text and visual information such as charts, diagrams, and tables in financial documents. However, existing multimodal RAG systems rely on LLM-based summarization to convert images into text during preprocessing, storing only text representations in vector databases, which causes loss of contextual information and visual details critical for downstream retrieval and question answering. To address this limitation, we present a comprehensive comparative analysis of two retrieval approaches for multimodal RAG systems, including text-based chunk retrieval (where images are summarized into text before embedding) and direct multimodal embedding retrieval (where images are stored natively in the vector space). We evaluate all three approaches across 6 LLM models and a two multi-modal embedding models on a newly created financial earnings call benchmark comprising 40 question-answer pairs, each paired with 2 documents (1 image and 1 text chunk). Experimental results demonstrate that direct multimodal embedding retrieval significantly outperforms LLM-summary-based approaches, achieving absolute improvements of 13% in mean average precision (mAP@5) and 11% in normalized discounted cumulative gain. These gains correspond to relative improvements of 32% in mAP@5 and 20% in nDCG@5, providing stronger evidence of their practical impact. We additionally find that direct multimodal retrieval produces more accurate and factually consistent answers as measured by LLM-as-a-judge pairwise comparisons. We demonstrate that LLM summarization introduces information loss during preprocessing, whereas direct multimodal embeddings preserve visual context for retrieval and inference.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [24] [Majority Rules: LLM Ensemble is a Winning Approach for Content Categorization](https://arxiv.org/abs/2511.15714)
*Ariel Kamen,Yakov Kamen*

Main category: cs.AI

TL;DR: An ensemble framework (eLLM) that combines multiple large language models for text categorization, achieving up to 65% F1-score improvement over single models and near human-expert performance.


<details>
  <summary>Details</summary>
Motivation: To address weaknesses in individual LLMs including inconsistency, hallucination, category inflation, and misclassification in text categorization tasks.

Method: Developed an ensemble framework integrating multiple LLMs with mathematical modeling of collective decision-making and principled aggregation criteria. Evaluated 10 state-of-the-art LLMs on 8,660 human-annotated samples using IAB hierarchical taxonomy under zero-shot conditions.

Result: eLLM achieved substantial performance improvement of up to 65% in F1-score over the strongest single model. Individual models plateaued due to text compression issues, while eLLM improved both robustness and accuracy, reaching near human-expert-level performance.

Conclusion: The ensemble framework offers a scalable and reliable solution for taxonomy-based classification that can significantly reduce dependence on human expert labeling, overcoming limitations of individual LLMs.

Abstract: This study introduces an ensemble framework for unstructured text categorization using large language models (LLMs). By integrating multiple models, the ensemble large language model (eLLM) framework addresses common weaknesses of individual systems, including inconsistency, hallucination, category inflation, and misclassification. The eLLM approach yields a substantial performance improvement of up to 65\% in F1-score over the strongest single model. We formalize the ensemble process through a mathematical model of collective decision-making and establish principled aggregation criteria. Using the Interactive Advertising Bureau (IAB) hierarchical taxonomy, we evaluate ten state-of-the-art LLMs under identical zero-shot conditions on a human-annotated corpus of 8{,}660 samples. Results show that individual models plateau in performance due to the compression of semantically rich text into sparse categorical representations, while eLLM improves both robustness and accuracy. With a diverse consortium of models, eLLM achieves near human-expert-level performance, offering a scalable and reliable solution for taxonomy-based classification that may significantly reduce dependence on human expert labeling.

</details>


### [25] [Graph-Memoized Reasoning: Foundations Structured Workflow Reuse in Intelligent Systems](https://arxiv.org/abs/2511.15715)
*Yash Raj Singh*

Main category: cs.AI

TL;DR: Graph-Memoized Reasoning is a framework that stores and reuses reasoning workflows as graph-structured memory to avoid redundant computations in large language models, improving efficiency and reproducibility.


<details>
  <summary>Details</summary>
Motivation: Current LLM reasoning systems waste computational resources by recomputing similar reasoning steps across tasks, increasing inference latency and limiting reproducibility, necessitating persistent reasoning mechanisms.

Method: The approach encodes past decision graphs and retrieves them through structural and semantic similarity, enabling compositional reuse of subgraphs across new reasoning tasks with an optimization objective that minimizes reasoning cost while maintaining consistency.

Result: The framework provides a theoretical foundation for efficiency-consistency trade-offs in intelligent systems and establishes groundwork for interpretable, cost-efficient reasoning architectures.

Conclusion: Graph-Memoized Reasoning offers a step toward persistent memory in large-scale agentic systems, enabling more efficient and self-improving reasoning architectures.

Abstract: Modern large language model-based reasoning systems frequently recompute similar reasoning steps across tasks, wasting computational resources, inflating inference latency, and limiting reproducibility. These inefficiencies underscore the need for persistent reasoning mechanisms that can recall and reuse prior computational traces.
  We introduce Graph-Memoized Reasoning, a formal framework for representing, storing, and reusing reasoning workflows as graph-structured memory. By encoding past decision graphs and retrieving them through structural and semantic similarity, our approach enables compositional reuse of subgraphs across new reasoning tasks.
  We formulate an optimization objective that minimizes total reasoning cost regularized by inconsistency between stored and generated workflows, providing a theoretical foundation for efficiency-consistency trade-offs in intelligent systems. We outline a conceptual evaluation protocol aligned with the proposed optimization objective.
  This framework establishes the groundwork for interpretable, cost-efficient, and self-improving reasoning architectures, offering a step toward persistent memory in large-scale agentic systems.

</details>


### [26] [MACIE: Multi-Agent Causal Intelligence Explainer for Collective Behavior Understanding](https://arxiv.org/abs/2511.15716)
*Abraham Itzhak Weinberg*

Main category: cs.AI

TL;DR: MACIE is a multi-agent explainable AI framework that uses causal models, counterfactuals, and Shapley values to provide comprehensive explanations for multi-agent reinforcement learning systems, addressing individual contributions, emergent behaviors, and complex interactions.


<details>
  <summary>Details</summary>
Motivation: As multi-agent reinforcement learning systems are increasingly used in safety-critical applications, understanding why agents make decisions and how they achieve collective behavior is crucial. Existing explainable AI methods struggle in multi-agent settings, failing to attribute collective outcomes to individuals, quantify emergent behaviors, or capture complex interactions.

Method: MACIE combines structural causal models, interventional counterfactuals, and Shapley values to provide comprehensive explanations. It addresses three key questions: individual agent causal contributions using interventional attribution scores, system-level emergent intelligence through synergy metrics, and actionable explanations using natural language narratives synthesizing causal insights.

Result: Evaluation across four MARL scenarios (cooperative, competitive, and mixed motive) showed accurate outcome attribution (mean phi_i = 5.07, standard deviation < 0.05), detection of positive emergence in cooperative tasks (synergy index up to 0.461), and efficient computation (0.79 seconds per dataset on CPU).

Conclusion: MACIE uniquely combines causal rigor, emergence quantification, and multi-agent support while remaining practical for real-time use, representing a step toward interpretable, trustworthy, and accountable multi-agent AI.

Abstract: As Multi Agent Reinforcement Learning systems are used in safety critical applications. Understanding why agents make decisions and how they achieve collective behavior is crucial. Existing explainable AI methods struggle in multi agent settings. They fail to attribute collective outcomes to individuals, quantify emergent behaviors, or capture complex interactions. We present MACIE Multi Agent Causal Intelligence Explainer, a framework combining structural causal models, interventional counterfactuals, and Shapley values to provide comprehensive explanations. MACIE addresses three questions. First, each agent's causal contribution using interventional attribution scores. Second, system level emergent intelligence through synergy metrics separating collective effects from individual contributions. Third, actionable explanations using natural language narratives synthesizing causal insights. We evaluate MACIE across four MARL scenarios: cooperative, competitive, and mixed motive. Results show accurate outcome attribution, mean phi_i equals 5.07, standard deviation less than 0.05, detection of positive emergence in cooperative tasks, synergy index up to 0.461, and efficient computation, 0.79 seconds per dataset on CPU. MACIE uniquely combines causal rigor, emergence quantification, and multi agent support while remaining practical for real time use. This represents a step toward interpretable, trustworthy, and accountable multi agent AI.

</details>


### [27] [How Modality Shapes Perception and Reasoning: A Study of Error Propagation in ARC-AGI](https://arxiv.org/abs/2511.15717)
*Bo Wen,Chen Wang,Erhan Bilal*

Main category: cs.AI

TL;DR: The paper examines how different text and image modalities affect perception and reasoning on ARC-AGI tasks, finding that structured text captures precise coordinates while images preserve 2D shapes, and combining modalities improves execution accuracy.


<details>
  <summary>Details</summary>
Motivation: Current systems for ARC-AGI tasks use generate-execute-select loops but lack understanding of how different encodings (text vs. images) shape model perception and how to separate instruction errors from execution errors.

Method: Isolate perception from reasoning across nine text and image modalities using weighted set-disagreement metric and two-stage reasoning pipeline, testing how modality affects perception of grid features.

Result: Structured text yields precise coordinates on sparse features, images capture 2D shapes but are resolution-sensitive, and combining modalities improves execution by about 8 perception points and 0.20 median similarity.

Conclusion: Aligning representations with transformer inductive biases and enabling cross-validation between text and image yields more accurate instructions and more reliable execution without changing the underlying model.

Abstract: ARC-AGI and ARC-AGI-2 measure generalization-through-composition on small color-quantized grids, and their prize competitions make progress on these harder held-out tasks a meaningful proxy for systematic generalization. Recent instruction-first systems translate grids into concise natural-language or DSL rules executed in generate-execute-select loops, yet we lack a principled account of how encodings shape model perception and how to separate instruction errors from execution errors. We hypothesize that modality imposes perceptual bottlenecks -- text flattens 2D structure into 1D tokens while images preserve layout but can introduce patch-size aliasing -- thereby shaping which grid features are reliably perceived. To test this, we isolate perception from reasoning across nine text and image modalities using a weighted set-disagreement metric and a two-stage reasoning pipeline, finding that structured text yields precise coordinates on sparse features, images capture 2D shapes yet are resolution-sensitive, and combining them improves execution (about 8 perception points; about 0.20 median similarity). Overall, aligning representations with transformer inductive biases and enabling cross-validation between text and image yields more accurate instructions and more reliable execution without changing the underlying model.

</details>


### [28] [ToolMind Technical Report: A Large-Scale, Reasoning-Enhanced Tool-Use Dataset](https://arxiv.org/abs/2511.15718)
*Chen Yang,Ran Le,Yun Xing,Zhenwei An,Zongchao Chen,Wayne Xin Zhao,Yang Song,Tao Zhang*

Main category: cs.AI

TL;DR: ToolMind is a large-scale, high-quality tool-agentic dataset with 160K synthetic and 200K augmented instances, featuring turn-level validation to prevent error propagation and improve LLM agent training.


<details>
  <summary>Details</summary>
Motivation: Current LLM agent development is hindered by scarce high-quality trajectories, and existing validation methods only check correctness at trajectory level, overlooking turn-level errors that degrade model performance.

Method: Constructs function graphs based on parameter correlations, uses multi-agent framework for realistic user-assistant-tool interactions, and employs fine-grained turn-level filtering to remove erroneous steps while preserving self-corrective reasoning signals.

Result: Models fine-tuned on ToolMind show significant improvements over baselines on several benchmarks.

Conclusion: ToolMind's turn-level validation approach effectively mitigates error amplification during training while preserving essential reasoning signals, leading to better performing LLM agents.

Abstract: Large Language Model (LLM) agents have developed rapidly in recent years to solve complex real-world problems using external tools. However, the scarcity of high-quality trajectories still hinders the development of stronger LLM agents. Most existing works on multi-turn dialogue synthesis validate correctness only at the trajectory level, which may overlook turn-level errors that can propagate during training and degrade model performance. To address these limitations, we introduce ToolMind, a large-scale, high-quality tool-agentic dataset with 160k synthetic data instances generated using over 20k tools and 200k augmented open-source data instances. Our data synthesis pipeline first constructs a function graph based on parameter correlations and then uses a multi-agent framework to simulate realistic user-assistant-tool interactions. Beyond trajectory-level validation, we employ fine-grained turn-level filtering to remove erroneous or suboptimal steps, ensuring that only high-quality reasoning traces are retained. This approach mitigates error amplification during training while preserving self-corrective reasoning signals essential for robust tool-use learning. Models fine-tuned on ToolMind show significant improvements over baselines on several benchmarks.

</details>


### [29] [Chain of Summaries: Summarization Through Iterative Questioning](https://arxiv.org/abs/2511.15719)
*William Brach,Lukas Galke Poech*

Main category: cs.AI

TL;DR: CoS generates information-dense web summaries using Hegel's dialectical method, improving LLM performance on Q&A tasks while reducing token usage.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with web content due to unfriendly formats and context length limits, requiring better summarization methods.

Method: Chain of Summaries (CoS) iteratively refines summaries through thesis-antithesis-synthesis cycles inspired by Hegel's dialectic.

Result: CoS outperforms zero-shot LLMs by 66% and specialized methods by 27% on TriviaQA, TruthfulQA, and SQUAD datasets.

Conclusion: CoS provides an effective way to make web content more accessible to LLMs while maintaining human oversight capabilities.

Abstract: Large Language Models (LLMs) are increasingly using external web content. However, much of this content is not easily digestible by LLMs due to LLM-unfriendly formats and limitations of context length. To address this issue, we propose a method for generating general-purpose, information-dense summaries that act as plain-text repositories of web content. Inspired by Hegel's dialectical method, our approach, denoted as Chain of Summaries (CoS), iteratively refines an initial summary (thesis) by identifying its limitations through questioning (antithesis), leading to a general-purpose summary (synthesis) that can satisfy current and anticipate future information needs. Experiments on the TriviaQA, TruthfulQA, and SQUAD datasets demonstrate that CoS outperforms zero-shot LLM baselines by up to 66% and specialized summarization methods such as BRIO and PEGASUS by up to 27%. CoS-generated summaries yield higher Q&A performance compared to the source content, while requiring substantially fewer tokens and being agnostic to the specific downstream LLM. CoS thus resembles an appealing option for website maintainers to make their content more accessible for LLMs, while retaining possibilities for human oversight.

</details>


### [30] [Automated Hazard Detection in Construction Sites Using Large Language and Vision-Language Models](https://arxiv.org/abs/2511.15720)
*Islem Sahraoui*

Main category: cs.AI

TL;DR: This thesis proposes a multimodal AI framework combining text and image analysis to identify construction safety hazards, using both proprietary and open-source models for automated hazard detection from accident reports and site imagery.


<details>
  <summary>Details</summary>
Motivation: Construction sites generate safety data in multiple formats (text reports, images), making traditional hazard identification challenging. A multimodal approach is needed to synthesize these diverse data sources for comprehensive safety monitoring.

Method: Two case studies: 1) Hybrid pipeline using GPT-4o models to analyze 28,000 OSHA accident reports (2000-2025); 2) Evaluation of lightweight open-source VLMs (Molmo 7B and Qwen2 VL 2B) on ConstructionSite10k dataset for rule-level safety violation detection using natural language prompts.

Result: Open-source models showed competitive performance despite smaller size, demonstrating feasibility of low-resource multimodal systems for safety monitoring. The framework successfully extracted structured insights from accident reports and detected safety violations from site imagery.

Conclusion: Multimodal AI frameworks combining text and image analysis are viable for construction safety hazard identification, with lightweight open-source models offering cost-effective alternatives to proprietary solutions while maintaining competitive performance.

Abstract: This thesis explores a multimodal AI framework for enhancing construction safety through the combined analysis of textual and visual data. In safety-critical environments such as construction sites, accident data often exists in multiple formats, such as written reports, inspection records, and site imagery, making it challenging to synthesize hazards using traditional approaches. To address this, this thesis proposed a multimodal AI framework that combines text and image analysis to assist in identifying safety hazards on construction sites. Two case studies were consucted to evaluate the capabilities of large language models (LLMs) and vision-language models (VLMs) for automated hazard identification.The first case study introduces a hybrid pipeline that utilizes GPT 4o and GPT 4o mini to extract structured insights from a dataset of 28,000 OSHA accident reports (2000-2025). The second case study extends this investigation using Molmo 7B and Qwen2 VL 2B, lightweight, open-source VLMs. Using the public ConstructionSite10k dataset, the performance of the two models was evaluated on rule-level safety violation detection using natural language prompts. This experiment served as a cost-aware benchmark against proprietary models and allowed testing at scale with ground-truth labels. Despite their smaller size, Molmo 7B and Quen2 VL 2B showed competitive performance in certain prompt configurations, reinforcing the feasibility of low-resource multimodal systems for rule-aware safety monitoring.

</details>


### [31] [Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods](https://arxiv.org/abs/2511.15722)
*Weichen Liu,Qiyao Xue,Haoming Wang,Xiangyu Yin,Boyuan Yang,Wei Gao*

Main category: cs.AI

TL;DR: This survey paper introduces a cognitive-based taxonomy for spatial reasoning in multimodal large language models, organizing tasks by reasoning complexity and cognitive functions rather than input modality, and analyzes evaluation methods and improvement approaches.


<details>
  <summary>Details</summary>
Motivation: Spatial reasoning remains a persistent challenge for MLLMs, and existing surveys categorize progress based on input modality rather than cognitive aspects. The authors aim to provide a more principled framework for understanding spatial intelligence.

Method: The survey introduces a taxonomy that organizes spatial intelligence from a cognitive perspective, divides tasks by reasoning complexity, maps existing benchmarks across different settings (text, vision language, embodied), and analyzes evaluation metrics and improvement methods.

Result: The cognitive perspective enables more principled cross-task comparisons and reveals critical gaps between current model capabilities and human-like reasoning. The analysis clarifies strengths of both training-based and reasoning-based approaches and uncovers complementary mechanisms.

Conclusion: The survey provides new researchers with a comprehensive understanding of spatial reasoning in MLLMs and actionable directions for future research through its cognitive-based taxonomy and dual perspective analysis of improvement methods.

Abstract: Spatial reasoning, which requires ability to perceive and manipulate spatial relationships in the 3D world, is a fundamental aspect of human intelligence, yet remains a persistent challenge for Multimodal large language models (MLLMs). While existing surveys often categorize recent progress based on input modality (e.g., text, image, video, or 3D), we argue that spatial ability is not solely determined by the input format. Instead, our survey introduces a taxonomy that organizes spatial intelligence from cognitive aspect and divides tasks in terms of reasoning complexity, linking them to several cognitive functions. We map existing benchmarks across text only, vision language, and embodied settings onto this taxonomy, and review evaluation metrics and methodologies for assessing spatial reasoning ability. This cognitive perspective enables more principled cross-task comparisons and reveals critical gaps between current model capabilities and human-like reasoning. In addition, we analyze methods for improving spatial ability, spanning both training-based and reasoning-based approaches. This dual perspective analysis clarifies their respective strengths, uncovers complementary mechanisms. By surveying tasks, benchmarks, and recent advances, we aim to provide new researchers with a comprehensive understanding of the field and actionable directions for future research.

</details>


### [32] [Uncertainty-Resilient Multimodal Learning via Consistency-Guided Cross-Modal Transfer](https://arxiv.org/abs/2511.15741)
*Hyo-Jeong Jang*

Main category: cs.AI

TL;DR: This thesis proposes a consistency-guided cross-modal transfer framework for uncertainty-resilient multimodal learning, using cross-modal semantic consistency to improve robustness against noisy data, low-quality labels, and heterogeneous modality characteristics.


<details>
  <summary>Details</summary>
Motivation: Multimodal learning systems face substantial uncertainty from noisy data, low-quality labels, and heterogeneous modality characteristics, especially in human-computer interaction settings where data quality and annotation consistency vary across users and conditions.

Method: The framework uses cross-modal semantic consistency to project heterogeneous modalities into a shared latent space, mitigating modality gaps and uncovering structural relations that support uncertainty estimation and stable feature learning.

Result: Experiments on multimodal affect-recognition benchmarks show significant improvements in model stability, discriminative ability, and robustness to noisy or incomplete supervision. Latent space analyses confirm reliable cross-modal structure capture under challenging conditions.

Conclusion: The thesis offers a unified perspective on resilient multimodal learning by integrating uncertainty modeling, semantic alignment, and data-efficient supervision, providing practical insights for developing reliable and adaptive brain-computer interface systems.

Abstract: Multimodal learning systems often face substantial uncertainty due to noisy data, low-quality labels, and heterogeneous modality characteristics. These issues become especially critical in human-computer interaction settings, where data quality, semantic reliability, and annotation consistency vary across users and recording conditions. This thesis tackles these challenges by exploring uncertainty-resilient multimodal learning through consistency-guided cross-modal transfer. The central idea is to use cross-modal semantic consistency as a basis for robust representation learning. By projecting heterogeneous modalities into a shared latent space, the proposed framework mitigates modality gaps and uncovers structural relations that support uncertainty estimation and stable feature learning. Building on this foundation, the thesis investigates strategies to enhance semantic robustness, improve data efficiency, and reduce the impact of noise and imperfect supervision without relying on large, high-quality annotations. Experiments on multimodal affect-recognition benchmarks demonstrate that consistency-guided cross-modal transfer significantly improves model stability, discriminative ability, and robustness to noisy or incomplete supervision. Latent space analyses further show that the framework captures reliable cross-modal structure even under challenging conditions. Overall, this thesis offers a unified perspective on resilient multimodal learning by integrating uncertainty modeling, semantic alignment, and data-efficient supervision, providing practical insights for developing reliable and adaptive brain-computer interface systems.

</details>


### [33] [Build AI Assistants using Large Language Models and Agents to Enhance the Engineering Education of Biomechanics](https://arxiv.org/abs/2511.15752)
*Hanzhi Yan,Qin Lu,Xianqiao Wang,Xiaoming Zhai,Tianming Liu,He Li*

Main category: cs.AI

TL;DR: This paper proposes a dual-module framework combining Retrieval-Augmented Generation (RAG) and Multi-Agent Systems (MAS) to enhance LLM performance for biomechanics education, addressing knowledge gaps and multi-step reasoning challenges.


<details>
  <summary>Details</summary>
Motivation: LLMs show limitations in domain-specific applications like biomechanics education due to knowledge gaps and reduced performance in complex multi-step reasoning tasks requiring analysis of force and moment in musculoskeletal systems.

Method: Developed a dual-module framework: 1) RAG for improving specificity and logical consistency in conceptual true/false questions, 2) MAS for solving calculation-oriented problems requiring multi-step reasoning and code execution. Evaluated Qwen-1.0-32B, Qwen-2.5-32B, and Llama-70B on 100 biomechanics questions.

Result: RAG significantly enhanced LLM performance and stability in conceptual questions, surpassing vanilla models. MAS successfully performed multi-step reasoning, equation derivation, code execution, and generated explainable solutions for calculation tasks.

Conclusion: The combination of RAG and MAS demonstrates strong potential for enhancing LLM performance in specialized engineering courses, providing a promising approach for developing intelligent tutoring systems in engineering education.

Abstract: While large language models (LLMs) have demonstrated remarkable versatility across a wide range of general tasks, their effectiveness often diminishes in domain-specific applications due to inherent knowledge gaps. Moreover, their performance typically declines when addressing complex problems that require multi-step reasoning and analysis. In response to these challenges, we propose leveraging both LLMs and AI agents to develop education assistants aimed at enhancing undergraduate learning in biomechanics courses that focus on analyzing the force and moment in the musculoskeletal system of the human body. To achieve our goal, we construct a dual-module framework to enhance LLM performance in biomechanics educational tasks: 1) we apply Retrieval-Augmented Generation (RAG) to improve the specificity and logical consistency of LLM's responses to the conceptual true/false questions; 2) we build a Multi-Agent System (MAS) to solve calculation-oriented problems involving multi-step reasoning and code execution. Specifically, we evaluate the performance of several LLMs, i.e., Qwen-1.0-32B, Qwen-2.5-32B, and Llama-70B, on a biomechanics dataset comprising 100 true/false conceptual questions and problems requiring equation derivation and calculation. Our results demonstrate that RAG significantly enhances the performance and stability of LLMs in answering conceptual questions, surpassing those of vanilla models. On the other hand, the MAS constructed using multiple LLMs demonstrates its ability to perform multi-step reasoning, derive equations, execute code, and generate explainable solutions for tasks that require calculation. These findings demonstrate the potential of applying RAG and MAS to enhance LLM performance for specialized courses in engineering curricula, providing a promising direction for developing intelligent tutoring in engineering education.

</details>


### [34] [Multi-Agent LLM Orchestration Achieves Deterministic, High-Quality Decision Support for Incident Response](https://arxiv.org/abs/2511.15755)
*Philip Drammeh*

Main category: cs.AI

TL;DR: Multi-agent orchestration achieves 100% actionable recommendations vs 1.7% for single-agent approaches, with 80x improvement in specificity and 140x improvement in correctness, enabling production SLAs impossible with inconsistent single-agent outputs.


<details>
  <summary>Details</summary>
Motivation: Single-agent LLM approaches generate vague, unusable recommendations for incident response, limiting their practical deployment in production systems despite their potential to accelerate response times.

Method: MyAntFarm.ai framework using 348 controlled trials comparing single-agent copilot versus multi-agent systems on identical incident scenarios, with novel Decision Quality (DQ) metric capturing validity, specificity, and correctness.

Result: Multi-agent orchestration achieved 100% actionable recommendation rate versus 1.7% for single-agent, with zero quality variance across all trials and similar comprehension latency (~40s) for both approaches.

Conclusion: Multi-agent orchestration transforms from performance optimization to production-readiness requirement for LLM-based incident response, enabling deterministic quality essential for operational deployment.

Abstract: Large language models (LLMs) promise to accelerate incident response in production systems, yet single-agent approaches generate vague, unusable recommendations. We present MyAntFarm.ai, a reproducible containerized framework demonstrating that multi-agent orchestration fundamentally transforms LLM-based incident response quality. Through 348 controlled trials comparing single-agent copilot versus multi-agent systems on identical incident scenarios, we find that multi-agent orchestration achieves 100% actionable recommendation rate versus 1.7% for single-agent approaches, an 80 times improvement in action specificity and 140 times improvement in solution correctness. Critically, multi-agent systems exhibit zero quality variance across all trials, enabling production SLA commitments impossible with inconsistent single-agent outputs. Both architectures achieve similar comprehension latency (approx.40s), establishing that the architectural value lies in deterministic quality, not speed. We introduce Decision Quality (DQ), a novel metric capturing validity, specificity, and correctness properties essential for operational deployment that existing LLM metrics do not address. These findings reframe multi-agent orchestration from a performance optimization to a production-readiness requirement for LLM-based incident response. All code, Docker configurations, and trial data are publicly available for reproduction.

</details>


### [35] [Identifying the Supply Chain of AI for Trustworthiness and Risk Management in Critical Applications](https://arxiv.org/abs/2511.15763)
*Raymond K. Sheh,Karen Geappen*

Main category: cs.AI

TL;DR: The paper addresses the gap in systematic assessment of AI supply chain risks and proposes a taxonomy to help stakeholders identify and manage these risks in critical applications.


<details>
  <summary>Details</summary>
Motivation: There is a significant gap in assessing supply chain risks in AI systems, particularly problematic for critical applications like healthcare, utilities, and transport, where current risk assessments focus mainly on algorithmic bias and model hallucinations but neglect the complex web of dependencies in AI supply chains.

Method: The authors survey current AI risk assessment practices and develop a taxonomy specifically for categorizing AI supply chain entities to help stakeholders systematically inventory dependencies and ask the right questions about their AI systems.

Result: The proposed taxonomy enables stakeholders, especially those without extensive AI expertise, to systematically assess and manage AI supply chain risks by providing a structured framework for identifying dependencies across data sources, pre-trained models, agents, services, and other components.

Conclusion: This work bridges the gap between current AI governance practices and the urgent need for actionable risk assessment and management of AI use in critical applications by providing a practical taxonomy for AI supply chain risk evaluation.

Abstract: Risks associated with the use of AI, ranging from algorithmic bias to model hallucinations, have received much attention and extensive research across the AI community, from researchers to end-users. However, a gap exists in the systematic assessment of supply chain risks associated with the complex web of data sources, pre-trained models, agents, services, and other systems that contribute to the output of modern AI systems. This gap is particularly problematic when AI systems are used in critical applications, such as the food supply, healthcare, utilities, law, insurance, and transport.
  We survey the current state of AI risk assessment and management, with a focus on the supply chain of AI and risks relating to the behavior and outputs of the AI system. We then present a proposed taxonomy specifically for categorizing AI supply chain entities. This taxonomy helps stakeholders, especially those without extensive AI expertise, to "consider the right questions" and systematically inventory dependencies across their organization's AI systems. Our contribution bridges a gap between the current state of AI governance and the urgent need for actionable risk assessment and management of AI use in critical applications.

</details>


### [36] [Balancing Natural Language Processing Accuracy and Normalisation in Extracting Medical Insights](https://arxiv.org/abs/2511.15778)
*Paulina Tworek,Miłosz Bargieł,Yousef Khan,Tomasz Pełech-Pilichowski,Marek Mikołajczyk,Roman Lewandowski,Jose Sousa*

Main category: cs.AI

TL;DR: Comparative analysis of rule-based NLP methods vs LLMs for extracting medical information from Polish EHRs, showing rule-based methods are more accurate for demographics while LLMs excel at drug recognition and scalability.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of extracting structured medical insights from unstructured clinical text in non-English contexts where NLP resources are scarce, particularly in healthcare settings.

Method: Comparative analysis using both rule-based NLP methods and Large Language Models to extract patient demographics, clinical findings, and prescribed medications from Polish electronic health records, examining effects of text normalization and translation.

Result: Rule-based methods provided higher accuracy for information retrieval tasks, especially for age and sex extraction, while LLMs offered greater adaptability and scalability, excelling in drug name recognition. Translation from Polish to English caused information loss.

Conclusion: Hybrid approaches combining the precision of rule-based systems with the adaptability of LLMs offer a practical path toward more reliable and resource-efficient clinical NLP in real-world hospital settings.

Abstract: Extracting structured medical insights from unstructured clinical text using Natural Language Processing (NLP) remains an open challenge in healthcare, particularly in non-English contexts where resources are scarce. This study presents a comparative analysis of NLP low-compute rule-based methods and Large Language Models (LLMs) for information extraction from electronic health records (EHR) obtained from the Voivodeship Rehabilitation Hospital for Children in Ameryka, Poland. We evaluate both approaches by extracting patient demographics, clinical findings, and prescribed medications while examining the effects of lack of text normalisation and translation-induced information loss. Results demonstrate that rule-based methods provide higher accuracy in information retrieval tasks, particularly for age and sex extraction. However, LLMs offer greater adaptability and scalability, excelling in drug name recognition. The effectiveness of the LLMs was compared with texts originally in Polish and those translated into English, assessing the impact of translation. These findings highlight the trade-offs between accuracy, normalisation, and computational cost when deploying NLP in healthcare settings. We argue for hybrid approaches that combine the precision of rule-based systems with the adaptability of LLMs, offering a practical path toward more reliable and resource-efficient clinical NLP in real-world hospitals.

</details>


### [37] [IMACT-CXR - An Interactive Multi-Agent Conversational Tutoring System for Chest X-Ray Interpretation](https://arxiv.org/abs/2511.15825)
*Tuan-Anh Le,Anh Mai Vu,David Yang,Akash Awasthi,Hien Van Nguyen*

Main category: cs.AI

TL;DR: IMACT-CXR is an interactive multi-agent conversational tutor for chest X-ray interpretation that integrates spatial annotation, gaze analysis, knowledge retrieval, and vision-language reasoning in an AutoGen-based workflow to provide personalized coaching for trainees.


<details>
  <summary>Details</summary>
Motivation: To create an intelligent tutoring system that helps medical trainees improve their chest X-ray interpretation skills by combining multiple modalities (spatial, gaze, text) and providing personalized, evidence-based feedback while preventing premature disclosure of answers.

Method: Uses AutoGen-based multi-agent workflow with specialized agents for localization evaluation, Socratic coaching, PubMed evidence retrieval, similar case suggestion from REFLACX, and NV-Reason-CXR-3B for vision-language reasoning. Incorporates Bayesian Knowledge Tracing for skill mastery tracking and lung-lobe segmentation for anatomically aware gaze feedback.

Result: The system demonstrates responsive tutoring flows with bounded latency, precise control over answer leakage, and extensibility for live residency deployment. Preliminary evaluation shows improved localization and diagnostic reasoning compared to baselines.

Conclusion: IMACT-CXR successfully integrates multiple AI components into a unified tutoring system for chest X-ray interpretation, showing promise for medical education with its ability to provide personalized, evidence-based feedback while maintaining educational integrity.

Abstract: IMACT-CXR is an interactive multi-agent conversational tutor that helps trainees interpret chest X-rays by unifying spatial annotation, gaze analysis, knowledge retrieval, and image-grounded reasoning in a single AutoGen-based workflow. The tutor simultaneously ingests learner bounding boxes, gaze samples, and free-text observations. Specialized agents evaluate localization quality, generate Socratic coaching, retrieve PubMed evidence, suggest similar cases from REFLACX, and trigger NV-Reason-CXR-3B for vision-language reasoning when mastery remains low or the learner explicitly asks. Bayesian Knowledge Tracing (BKT) maintains skill-specific mastery estimates that drive both knowledge reinforcement and case similarity retrieval. A lung-lobe segmentation module derived from a TensorFlow U-Net enables anatomically aware gaze feedback, and safety prompts prevent premature disclosure of ground-truth labels. We describe the system architecture, implementation highlights, and integration with the REFLACX dataset for real DICOM cases. IMACT-CXR demonstrates responsive tutoring flows with bounded latency, precise control over answer leakage, and extensibility toward live residency deployment. Preliminary evaluation shows improved localization and diagnostic reasoning compared to baselines.

</details>


### [38] [Mini Amusement Parks (MAPs): A Testbed for Modelling Business Decisions](https://arxiv.org/abs/2511.15830)
*Stéphane Aroca-Ouellette,Ian Berlot-Attwell,Panagiotis Lymperopoulos,Abhiramon Rajasekharan,Tongqi Zhu,Herin Kang,Kaheer Suleman,Sam Pasupalak*

Main category: cs.AI

TL;DR: Mini Amusement Parks (MAPs) is a new simulator that evaluates AI agents' holistic decision-making abilities in complex business management scenarios, revealing significant performance gaps between humans and current LLM agents.


<details>
  <summary>Details</summary>
Motivation: Current AI systems struggle with interconnected real-world decision-making challenges like multi-faceted optimization, active learning from sparse experience, long-horizon planning under uncertainty, and spatial reasoning, while existing benchmarks isolate these capabilities.

Method: Developed MAPs, an amusement-park simulator that unifies multiple decision-making challenges, providing human baselines and evaluating state-of-the-art LLM agents across different difficulty modes.

Result: Humans significantly outperform LLM agents by 6.5x on easy mode and 9.8x on medium mode, revealing persistent weaknesses in long-horizon optimization, sample-efficient learning, spatial reasoning, and world modeling.

Conclusion: MAPs provides a new foundation for benchmarking agents capable of adaptable decision making by unifying multiple real-world challenges in a single environment, highlighting the need for improved holistic decision-making capabilities in AI systems.

Abstract: Despite rapid progress in artificial intelligence, current systems struggle with the interconnected challenges that define real-world decision making. Practical domains, such as business management, require optimizing an open-ended and multi-faceted objective, actively learning environment dynamics from sparse experience, planning over long horizons in stochastic settings, and reasoning over spatial information. Yet existing human--AI benchmarks isolate subsets of these capabilities, limiting our ability to assess holistic decision-making competence. We introduce Mini Amusement Parks (MAPs), an amusement-park simulator designed to evaluate an agent's ability to model its environment, anticipate long-term consequences under uncertainty, and strategically operate a complex business. We provide human baselines and a comprehensive evaluation of state-of-the-art LLM agents, finding that humans outperform these systems by 6.5x on easy mode and 9.8x on medium mode. Our analysis reveals persistent weaknesses in long-horizon optimization, sample-efficient learning, spatial reasoning, and world modelling. By unifying these challenges within a single environment, MAPs offers a new foundation for benchmarking agents capable of adaptable decision making. Code: https://github.com/Skyfall-Research/MAPs

</details>


### [39] [Step-Audio-R1 Technical Report](https://arxiv.org/abs/2511.15848)
*Fei Tian,Xiangyu Tony Zhang,Yuxin Zhang,Haoyang Zhang,Yuxin Li,Daijiao Liu,Yayue Deng,Donghang Wu,Jun Chen,Liang Zhao,Chengyuan Yao,Hexin Liu,Eng Siong Chng,Xuerui Yang,Xiangyu Zhang,Daxin Jiang,Gang Yu*

Main category: cs.AI

TL;DR: Step-Audio-R1 is the first successful audio reasoning model that enables genuine reasoning in audio domains through modality-grounded reasoning distillation, achieving performance comparable to state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: To address the perplexing phenomenon where audio language models perform better with minimal reasoning, and to determine if audio intelligence can truly benefit from deliberate thinking.

Method: Modality-Grounded Reasoning Distillation (MGRD) framework that teaches the model to generate audio-relevant reasoning chains grounded in acoustic features rather than disconnected deliberations.

Result: Step-Audio-R1 surpasses Gemini 2.5 Pro and achieves performance comparable to Gemini 3 Pro across comprehensive audio understanding and reasoning benchmarks spanning speech, environmental sounds, and music.

Conclusion: Reasoning is a transferable capability across modalities when appropriately anchored, transforming extended deliberation from a liability into a powerful asset for audio intelligence.

Abstract: Recent advances in reasoning models have demonstrated remarkable success in text and vision domains through extended chain-of-thought deliberation. However, a perplexing phenomenon persists in audio language models: they consistently perform better with minimal or no reasoning, raising a fundamental question - can audio intelligence truly benefit from deliberate thinking? We introduce Step-Audio-R1, the first audio reasoning model that successfully unlocks reasoning capabilities in the audio domain. Through our proposed Modality-Grounded Reasoning Distillation (MGRD) framework, Step-Audio-R1 learns to generate audio-relevant reasoning chains that genuinely ground themselves in acoustic features rather than hallucinating disconnected deliberations. Our model exhibits strong audio reasoning capabilities, surpassing Gemini 2.5 Pro and achieving performance comparable to the state-of-the-art Gemini 3 Pro across comprehensive audio understanding and reasoning benchmarks spanning speech, environmental sounds, and music. These results demonstrate that reasoning is a transferable capability across modalities when appropriately anchored, transforming extended deliberation from a liability into a powerful asset for audio intelligence. By establishing the first successful audio reasoning model, Step-Audio-R1 opens new pathways toward building truly multimodal reasoning systems that think deeply across all sensory modalities.

</details>


### [40] [Decomposing Theory of Mind: How Emotional Processing Mediates ToM Abilities in LLMs](https://arxiv.org/abs/2511.15895)
*Ivan Chulo,Ananya Joshi*

Main category: cs.AI

TL;DR: The paper analyzes how activation steering improves Theory of Mind in LLMs, finding that enhanced belief attribution performance is mediated by increased emotional processing and suppressed analytical reasoning.


<details>
  <summary>Details</summary>
Motivation: To understand the internal mechanisms behind activation steering's improvement of Theory of Mind in language models, as previous work showed effectiveness but lacked clarity on what specific cognitive changes occur.

Method: Decomposed ToM by comparing steered vs baseline LLM activations using linear probes trained on 45 cognitive actions, applied Contrastive Activation Addition steering to Gemma-3-4B, and evaluated on 1,000 BigToM forward belief scenarios.

Result: Improved performance on belief attribution tasks (32.5% to 46.7% accuracy) was mediated by activations processing emotional content (emotion perception +2.23, emotion valuing +2.20) while suppressing analytical processes (questioning -0.78, convergent thinking -1.59).

Conclusion: Successful Theory of Mind abilities in LLMs are mediated by emotional understanding rather than analytical reasoning.

Abstract: Recent work shows activation steering substantially improves language models' Theory of Mind (ToM) (Bortoletto et al. 2024), yet the mechanisms of what changes occur internally that leads to different outputs remains unclear. We propose decomposing ToM in LLMs by comparing steered versus baseline LLMs' activations using linear probes trained on 45 cognitive actions. We applied Contrastive Activation Addition (CAA) steering to Gemma-3-4B and evaluated it on 1,000 BigToM forward belief scenarios (Gandhi et al. 2023), we find improved performance on belief attribution tasks (32.5\% to 46.7\% accuracy) is mediated by activations processing emotional content : emotion perception (+2.23), emotion valuing (+2.20), while suppressing analytical processes: questioning (-0.78), convergent thinking (-1.59). This suggests that successful ToM abilities in LLMs are mediated by emotional understanding, not analytical reasoning.

</details>


### [41] [Thinking, Faithful and Stable: Mitigating Hallucinations in LLMs](https://arxiv.org/abs/2511.15921)
*Chelsea Zou,Yiheng Yao,Basant Khalil*

Main category: cs.AI

TL;DR: A self-correcting framework for LLMs that uses fine-grained uncertainty signals (confidence alignment and token-level entropy) to detect and mitigate hallucinations during multi-step reasoning through reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To address hallucinations in LLMs during multi-step reasoning by going beyond final answer correctness and focusing on detecting unreliable reasoning in real-time through uncertainty signals.

Method: Uses fine-grained uncertainty signals (self-assessed confidence alignment and token-level entropy spikes) with a composite reward function that penalizes unjustified high confidence and entropy spikes. Applies reinforcement learning to shape generation behavior through confidence-aware reward feedback.

Result: Improves both final answer accuracy and reasoning calibration, with ablations confirming the individual contributions of each uncertainty signal.

Conclusion: The framework successfully makes LLMs more introspective and improves the coherence and faithfulness of intermediate reasoning steps through real-time uncertainty detection and reinforcement learning.

Abstract: This project develops a self correcting framework for large language models (LLMs) that detects and mitigates hallucinations during multi-step reasoning. Rather than relying solely on final answer correctness, our approach leverages fine grained uncertainty signals: 1) self-assessed confidence alignment, and 2) token-level entropy spikes to detect unreliable and unfaithful reasoning in real time. We design a composite reward function that penalizes unjustified high confidence and entropy spikes, while encouraging stable and accurate reasoning trajectories. These signals guide a reinforcement learning (RL) policy that makes the model more introspective and shapes the model's generation behavior through confidence-aware reward feedback, improving not just outcome correctness but the coherence and faithfulness of their intermediate reasoning steps. Experiments show that our method improves both final answer accuracy and reasoning calibration, with ablations validating the individual contribution of each signal.

</details>


### [42] [JudgeBoard: Benchmarking and Enhancing Small Language Models for Reasoning Evaluation](https://arxiv.org/abs/2511.15958)
*Zhenyu Bi,Gaurav Srivastava,Yang Li,Meng Lu,Swastik Roy,Morteza Ziyadi,Xuan Wang*

Main category: cs.AI

TL;DR: JudgeBoard is a novel evaluation pipeline that directly queries models to assess answer correctness without requiring answer comparisons. It introduces MAJ (Multi-Agent Judging), a multi-agent framework using multiple SLMs to achieve LLM-level judgment accuracy through collaborative deliberation.


<details>
  <summary>Details</summary>
Motivation: Current LLM-as-a-judge frameworks rely on indirect answer comparisons using predefined metrics, which are difficult to fully automate and offer limited support for fine-grained, scalable evaluation of reasoning outputs. There's a need for more direct and automated evaluation methods.

Method: Proposed JudgeBoard pipeline that directly queries models to assess correctness, focusing on mathematical and science/commonsense reasoning. Introduced MAJ framework using multiple interacting SLMs with distinct reasoning profiles to collaboratively approximate LLM-level judgment accuracy.

Result: Significant performance gap exists between SLMs and LLMs in isolated judging tasks. However, MAJ substantially improves SLM reliability and consistency. On MATH dataset, MAJ with smaller models performs comparably or better than larger models. Multi-agent SLM systems can potentially match or exceed LLM performance in judgment tasks.

Conclusion: Multi-agent SLM systems show promise for scalable and efficient assessment, potentially matching or exceeding LLM performance in judgment tasks through collaborative deliberation, with implications for more automated and fine-grained evaluation of reasoning outputs.

Abstract: While small language models (SLMs) have shown promise on various reasoning tasks, their ability to judge the correctness of answers remains unclear compared to large language models (LLMs). Prior work on LLM-as-a-judge frameworks typically relies on comparing candidate answers against ground-truth labels or other candidate answers using predefined metrics like entailment. However, this approach is inherently indirect and difficult to fully automate, offering limited support for fine-grained and scalable evaluation of reasoning outputs. In this work, we propose JudgeBoard, a novel evaluation pipeline that directly queries models to assess the correctness of candidate answers without requiring extra answer comparisons. We focus on two core reasoning domains: mathematical reasoning and science/commonsense reasoning, and construct task-specific evaluation leaderboards using both accuracy-based ranking and an Elo-based rating system across five benchmark datasets, enabling consistent model comparison as judges rather than comparators. To improve judgment performance in lightweight models, we propose MAJ (Multi-Agent Judging), a novel multi-agent evaluation framework that leverages multiple interacting SLMs with distinct reasoning profiles to approximate LLM-level judgment accuracy through collaborative deliberation. Experimental results reveal a significant performance gap between SLMs and LLMs in isolated judging tasks. However, our MAJ framework substantially improves the reliability and consistency of SLMs. On the MATH dataset, MAJ using smaller-sized models as backbones performs comparatively well or even better than their larger-sized counterparts. Our findings highlight that multi-agent SLM systems can potentially match or exceed LLM performance in judgment tasks, with implications for scalable and efficient assessment.

</details>


### [43] [KRAL: Knowledge and Reasoning Augmented Learning for LLM-assisted Clinical Antimicrobial Therapy](https://arxiv.org/abs/2511.15974)
*Zhe Li,Yehan Qiu,Yujie Chen,Xiang Zhou*

Main category: cs.AI

TL;DR: KRAL is a novel paradigm that enhances clinical LLMs through knowledge and reasoning augmentation, achieving superior performance over traditional methods at significantly lower costs while preserving privacy.


<details>
  <summary>Details</summary>
Motivation: Address limitations of LLMs in clinical decision-making including knowledge gaps, privacy concerns, high costs, and limited reasoning capabilities in complex antimicrobial therapy scenarios.

Method: Uses teacher-model reasoning for knowledge distillation via answer-to-question reverse generation, heuristic learning for semi-supervised data augmentation (80% reduction in manual annotation), and agentic reinforcement learning to jointly enhance knowledge and reasoning while optimizing efficiency.

Result: Significantly outperforms RAG and SFT: Accuracy@1 on MEDQA increased by 1.8% vs. SFT and 3.6% vs. RAG; Pass@1 on PUMCH Antimicrobial increased by 27% vs. SFT and 27.2% vs. RAG, achieved at ~20% of SFT's long-term training costs.

Conclusion: KRAL establishes an effective solution for enhancing local LLMs' clinical diagnostic capabilities, enabling low-cost, high-safety deployment in complex medical decision support systems.

Abstract: Clinical antimicrobial therapy requires the dynamic integration of pathogen profiles, host factors, pharmacological properties of antimicrobials, and the severity of infection.This complexity imposes fundamental limitations on the applicability of Large Language Models (LLMs) in high-stakes clinical decision-making including knowledge gaps, data privacy concerns, high deployment costs, and limited reasoning capabilities. To address these challenges, we propose KRAL (Knowledge and Reasoning Augmented Learning), a low-cost, scalable, privacy-preserving paradigm that leverages teacher-model reasoning to automatically distill knowledge and reasoning trajectories via answer-to-question reverse generation, employs heuristic learning for semi-supervised data augmentation (reducing manual annotation requirements by approximately 80%), and utilizes agentic reinforcement learning to jointly enhance medical knowledge and reasoning while optimizing computational and memory efficiency. A hierarchical evaluation employing diverse teacher-model proxies reduces assessment costs, while modular interface design facilitates seamless system updates. Experimental results demonstrate that KRAL significantly outperforms traditional Retrieval-Augmented Generation (RAG) and Supervised Fine-Tuning (SFT) methods. It improves knowledge question-answering capability (Accuracy@1 on the external open-source benchmark MEDQA increased by 1.8% vs. SFT and 3.6% vs. RAG) and reasoning capability (Pass@1 on the external benchmark PUMCH Antimicrobial increased by 27% vs. SFT and 27.2% vs. RAG), achieved at ~20% of SFT's long-term training costs. This establishes KRAL as an effective solution for enhancing local LLMs' clinical diagnostic capabilities, enabling low-cost, high-safety deployment in complex medical decision support.

</details>


### [44] [Detecting Sleeper Agents in Large Language Models via Semantic Drift Analysis](https://arxiv.org/abs/2511.15992)
*Shahin Zanbaghi,Ryan Rostampour,Farhan Abid,Salim Al Jarmakani*

Main category: cs.AI

TL;DR: A dual-method detection system combining semantic drift analysis and canary baseline comparison achieves 92.5% accuracy in identifying backdoored LLMs (sleeper agents) in real-time without model modifications.


<details>
  <summary>Details</summary>
Motivation: LLMs can be backdoored to exhibit malicious behavior under specific conditions while appearing safe during training, with existing backdoors persisting through safety training and no practical detection methods available.

Method: Combines semantic drift analysis using Sentence-BERT embeddings to measure deviation from safe baselines with canary questions that monitor response consistency, operating in real-time without model modification.

Result: Achieves 92.5% accuracy with 100% precision (zero false positives) and 85% recall on the Cadenza-Labs dolphin-llama3-8B sleeper agent model, with detection in <1s per query.

Conclusion: Provides the first practical solution to LLM backdoor detection, addressing a critical security gap in AI deployment through embedding-based detection that identifies deceptive behavior without sacrificing efficiency.

Abstract: Large Language Models (LLMs) can be backdoored to exhibit malicious behavior under specific deployment conditions while appearing safe during training a phenomenon known as "sleeper agents." Recent work by Hubinger et al. demonstrated that these backdoors persist through safety training, yet no practical detection methods exist. We present a novel dual-method detection system combining semantic drift analysis with canary baseline comparison to identify backdoored LLMs in real-time. Our approach uses Sentence-BERT embeddings to measure semantic deviation from safe baselines, complemented by injected canary questions that monitor response consistency. Evaluated on the official Cadenza-Labs dolphin-llama3-8B sleeper agent model, our system achieves 92.5% accuracy with 100% precision (zero false positives) and 85% recall. The combined detection method operates in real-time (<1s per query), requires no model modification, and provides the first practical solution to LLM backdoor detection. Our work addresses a critical security gap in AI deployment and demonstrates that embedding-based detection can effectively identify deceptive model behavior without sacrificing deployment efficiency.

</details>


### [45] [CARE-RAG - Clinical Assessment and Reasoning in RAG](https://arxiv.org/abs/2511.15994)
*Deepthi Potluri,Aby Mammen Mathew,Jeffrey B DeWitt,Alexander L. Rasgon,Yide Hao,Junyuan Hong,Ying Ding*

Main category: cs.AI

TL;DR: This paper examines the gap between retrieval and reasoning in LLMs, particularly in clinical settings, using Written Exposure Therapy guidelines as a testbed. It finds that providing authoritative evidence doesn't guarantee correct reasoning and proposes an evaluation framework measuring accuracy, consistency, and fidelity.


<details>
  <summary>Details</summary>
Motivation: To address the concerning gap between retrieval and reasoning in LLMs, especially in clinical settings where outputs must align with structured protocols, as access to right evidence doesn't ensure correct reasoning.

Method: Used Written Exposure Therapy (WET) guidelines as a testbed, evaluated model responses to curated clinician-vetted questions, and proposed an evaluation framework measuring accuracy, consistency, and fidelity of reasoning.

Result: Found that errors persist even when authoritative passages are provided, showing that retrieval-augmented generation (RAG) can constrain outputs but safe deployment requires rigorous reasoning assessment alongside retrieval.

Conclusion: Safe deployment of LLMs in clinical settings requires assessing reasoning as rigorously as retrieval, as RAG alone cannot guarantee correct reasoning even with authoritative evidence.

Abstract: Access to the right evidence does not guarantee that large language models (LLMs) will reason with it correctly. This gap between retrieval and reasoning is especially concerning in clinical settings, where outputs must align with structured protocols. We study this gap using Written Exposure Therapy (WET) guidelines as a testbed. In evaluating model responses to curated clinician-vetted questions, we find that errors persist even when authoritative passages are provided. To address this, we propose an evaluation framework that measures accuracy, consistency, and fidelity of reasoning. Our results highlight both the potential and the risks: retrieval-augmented generation (RAG) can constrain outputs, but safe deployment requires assessing reasoning as rigorously as retrieval.

</details>


### [46] [Sensorium Arc: AI Agent System for Oceanic Data Exploration and Interactive Eco-Art](https://arxiv.org/abs/2511.15997)
*Noah Bissell,Ethan Paley,Joshua Harrison,Juliano Calil,Myungin Lee*

Main category: cs.AI

TL;DR: Sensorium Arc is an interactive AI system that personifies the ocean as a poetic speaker, enabling natural conversations about marine data through a multi-agent framework that blends scientific insights with ecological poetics.


<details>
  <summary>Details</summary>
Motivation: To reimagine ocean data as a living narrative rather than abstract datasets, enabling affective and intuitive access to complex environmental information through conversational AI.

Method: Built on a modular multi-agent system and retrieval-augmented LLM framework, using keyword detection and semantic parsing to dynamically trigger data visualizations and audiovisual content based on dialogue cues.

Result: The system successfully creates immersive explorations of marine data through natural spoken conversations with AI agents that embody the ocean's perspective, blending scientific accuracy with poetic expression.

Conclusion: Sensorium Arc demonstrates the potential of conversational AI agents to mediate human understanding of environmental data and proposes a new paradigm for human-machine-ecosystem interaction.

Abstract: Sensorium Arc (AI reflects on climate) is a real-time multimodal interactive AI agent system that personifies the ocean as a poetic speaker and guides users through immersive explorations of complex marine data. Built on a modular multi-agent system and retrieval-augmented large language model (LLM) framework, Sensorium enables natural spoken conversations with AI agents that embodies the ocean's perspective, generating responses that blend scientific insight with ecological poetics. Through keyword detection and semantic parsing, the system dynamically triggers data visualizations and audiovisual playback based on time, location, and thematic cues drawn from the dialogue. Developed in collaboration with the Center for the Study of the Force Majeure and inspired by the eco-aesthetic philosophy of Newton Harrison, Sensorium Arc reimagines ocean data not as an abstract dataset but as a living narrative. The project demonstrates the potential of conversational AI agents to mediate affective, intuitive access to high-dimensional environmental data and proposes a new paradigm for human-machine-ecosystem.

</details>


### [47] [MUSEKG: A Knowledge Graph Over Museum Collections](https://arxiv.org/abs/2511.16014)
*Jinhao Li,Jianzhong Qi,Soyeon Caren Han,Eun-Jung Holden*

Main category: cs.AI

TL;DR: MuseKG is a knowledge graph framework that integrates structured and unstructured museum data using symbolic-neural methods, enabling natural language queries and outperforming LLM and SPARQL baselines.


<details>
  <summary>Details</summary>
Motivation: Digital transformation in cultural heritage has created fragmented collections of artefact data, and existing museum information systems struggle to integrate heterogeneous metadata, unstructured documents, and multimodal artefacts into a coherent queryable form.

Method: MuseKG constructs a typed property graph linking objects, people, organizations, and visual/textual labels through symbolic-neural integration, supporting natural language queries.

Result: Evaluations on real museum collections show robust performance across queries over attributes, relations, and related entities, surpassing large-language-model zero-shot, few-shot and SPARQL prompt baselines.

Conclusion: The framework demonstrates the importance of symbolic grounding for interpretable and scalable cultural heritage reasoning, enabling web-scale integration of digital heritage knowledge.

Abstract: Digital transformation in the cultural heritage sector has produced vast yet fragmented collections of artefact data. Existing frameworks for museum information systems struggle to integrate heterogeneous metadata, unstructured documents, and multimodal artefacts into a coherent and queryable form. We present MuseKG, an end-to-end knowledge-graph framework that unifies structured and unstructured museum data through symbolic-neural integration. MuseKG constructs a typed property graph linking objects, people, organisations, and visual or textual labels, and supports natural language queries. Evaluations on real museum collections demonstrate robust performance across queries over attributes, relations, and related entities, surpassing large-language-model zero-shot, few-shot and SPARQL prompt baselines. The results highlight the importance of symbolic grounding for interpretable and scalable cultural heritage reasoning, and pave the way for web-scale integration of digital heritage knowledge.

</details>


### [48] [SpellForger: Prompting Custom Spell Properties In-Game using BERT supervised-trained model](https://arxiv.org/abs/2511.16018)
*Emanuel C. Silva,Emily S. M. Salum,Gabriel M. Arantes,Matheus P. Pereira,Vinicius F. Oliveira,Alessandro L. Bicho*

Main category: cs.AI

TL;DR: SpellForger is a game where players create custom spells using natural language prompts, with AI interpreting descriptions to generate balanced spell parameters in real-time gameplay.


<details>
  <summary>Details</summary>
Motivation: To explore AI as a core gameplay co-creation tool rather than just for content generation, enabling unique player personalization and creativity through natural language spell creation.

Method: Uses a supervised-trained BERT model to interpret player prompts and map them to spell prefabs with balanced parameters (damage, cost, effects), developed in Unity with Python AI backend.

Result: Expected to deliver a functional prototype demonstrating real-time spell generation within an engaging gameplay loop where player creativity is central.

Conclusion: Validates the use of AI as a direct gameplay mechanic that enables player-driven content creation and personalized gaming experiences.

Abstract: Introduction: The application of Artificial Intelligence in games has evolved significantly, allowing for dynamic content generation. However, its use as a core gameplay co-creation tool remains underexplored. Objective: This paper proposes SpellForger, a game where players create custom spells by writing natural language prompts, aiming to provide a unique experience of personalization and creativity. Methodology: The system uses a supervisedtrained BERT model to interpret player prompts. This model maps textual descriptions to one of many spell prefabs and balances their parameters (damage, cost, effects) to ensure competitive integrity. The game is developed in the Unity Game Engine, and the AI backend is in Python. Expected Results: We expect to deliver a functional prototype that demonstrates the generation of spells in real time, applied to an engaging gameplay loop, where player creativity is central to the experience, validating the use of AI as a direct gameplay mechanic.

</details>


### [49] [An Aligned Constraint Programming Model For Serial Batch Scheduling With Minimum Batch Size](https://arxiv.org/abs/2511.16045)
*Jorge A. Huertas,Pascal Van Hentenryck*

Main category: cs.AI

TL;DR: A novel Constraint Programming model for serial batch scheduling with minimum batch sizes that eliminates the need for predefined virtual batches, using direct sequence reasoning and improved search strategies to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing CP models for serial batch scheduling with minimum batch sizes rely on predefined virtual batches that suffer from dimensionality issues and add complexity. This approach aims to create a more compact and efficient formulation.

Method: Proposes a CP model that uses key alignment parameters to reason directly on sequences of same-family jobs on machines, avoiding virtual batches. Enhanced with tailored search phases and strengthened constraint propagator inference.

Result: Extensive testing on 5000 instances shows superiority on small-to-medium instances (up to 100 jobs) and ability to find solutions up to 25% better than existing methods on large instances (up to 500 jobs, 10 families, 10 machines).

Conclusion: The proposed CP model provides a more efficient and compact formulation for serial batch scheduling with minimum batch sizes, significantly outperforming existing MIP, tabu search, and CP approaches across various instance sizes.

Abstract: In serial batch (s-batch) scheduling, jobs from similar families are grouped into batches and processed sequentially to avoid repetitive setups that are required when processing consecutive jobs of different families. Despite its large success in scheduling, only three Constraint Programming (CP) models have been proposed for this problem considering minimum batch sizes, which is a common requirement in many practical settings, including the ion implantation area in semiconductor manufacturing. These existing CP models rely on a predefined virtual set of possible batches that suffers from the curse of dimensionality and adds complexity to the problem. This paper proposes a novel CP model that does not rely on this virtual set. Instead, it uses key alignment parameters that allow it to reason directly on the sequences of same-family jobs scheduled on the machines, resulting in a more compact formulation. This new model is further improved by exploiting the problem's structure with tailored search phases and strengthened inference levels of the constraint propagators. The extensive computational experiments on nearly five thousand instances compare the proposed models against existing methods in the literature, including mixed-integer programming formulations, tabu search meta-heuristics, and CP approaches. The results demonstrate the superiority of the proposed models on small-to-medium instances with up to 100 jobs, and their ability to find solutions up to 25\% better than the ones produces by existing methods on large-scale instances with up to 500 jobs, 10 families, and 10 machines.

</details>


### [50] [Artificial Intelligence and Accounting Research: A Framework and Agenda](https://arxiv.org/abs/2511.16055)
*Theophanis C. Stratopoulos,Victor Xiaoqi Wang*

Main category: cs.AI

TL;DR: This paper proposes a framework for classifying AI-accounting research along research focus and methodological dimensions, analyzes how accounting researchers can strategically position themselves, and examines how GenAI transforms the research process while intensifying competition.


<details>
  <summary>Details</summary>
Motivation: Recent advances in generative AI and large language models are fundamentally transforming accounting research, creating both opportunities and competitive threats for scholars, necessitating a framework to understand and navigate these changes.

Method: The paper develops a classification framework with two dimensions: research focus (accounting-centric vs AI-centric) and methodological approach (AI-based vs traditional methods). It applies this framework to papers from IJAIS special issue and recent AI-accounting research in leading journals.

Result: The analysis reveals that while GenAI democratizes certain research capabilities, it intensifies competition by raising expectations for higher-order contributions where human judgment, creativity, and theoretical depth remain valuable.

Conclusion: The shifts in accounting research due to AI call for reforming doctoral education to cultivate comparative advantages while building AI fluency, emphasizing strategic positioning and collaboration for accounting scholars.

Abstract: Recent advances in artificial intelligence, particularly generative AI (GenAI) and large language models (LLMs), are fundamentally transforming accounting research, creating both opportunities and competitive threats for scholars. This paper proposes a framework that classifies AI-accounting research along two dimensions: research focus (accounting-centric versus AI-centric) and methodological approach (AI-based versus traditional methods). We apply this framework to papers from the IJAIS special issue and recent AI-accounting research published in leading accounting journals to map existing studies and identify research opportunities. Using this same framework, we analyze how accounting researchers can leverage their expertise through strategic positioning and collaboration, revealing where accounting scholars' strengths create the most value. We further examine how GenAI and LLMs transform the research process itself, comparing the capabilities of human researchers and AI agents across the entire research workflow. This analysis reveals that while GenAI democratizes certain research capabilities, it simultaneously intensifies competition by raising expectations for higher-order contributions where human judgment, creativity, and theoretical depth remain valuable. These shifts call for reforming doctoral education to cultivate comparative advantages while building AI fluency.

</details>


### [51] [OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe](https://arxiv.org/abs/2511.16334)
*Kaichen Zhang,Keming Wu,Zuhao Yang,Kairui Hu,Bin Wang,Ziwei Liu,Xingxuan Li,Lidong Bing*

Main category: cs.AI

TL;DR: OpenMMReasoner is a transparent two-stage training recipe for multimodal reasoning that uses supervised fine-tuning with 874K samples and reinforcement learning with 74K samples, achieving 11.6% improvement over baseline across nine benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of transparent and reproducible data curation and training strategies in multimodal reasoning research, which hinders scalable progress in the field.

Method: A two-stage approach: 1) Supervised fine-tuning with 874K-sample cold-start dataset featuring step-by-step validation, 2) Reinforcement learning with 74K-sample dataset across diverse domains to sharpen and stabilize reasoning abilities.

Result: Achieved 11.6% improvement over Qwen2.5-VL-7B-Instruct baseline across nine multimodal reasoning benchmarks, demonstrating superior performance and establishing solid empirical foundation.

Conclusion: The work highlights the critical importance of data quality and training design in multimodal reasoning, provides fully transparent and reproducible methodology, and opens up new possibilities for large-scale multimodal reasoning research.

Abstract: Recent advancements in large reasoning models have fueled growing interest in extending such capabilities to multimodal domains. However, despite notable progress in visual reasoning, the lack of transparent and reproducible data curation and training strategies remains a major barrier to scalable research. In this work, we introduce OpenMMReasoner, a fully transparent two-stage recipe for multimodal reasoning spanning supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct an 874K-sample cold-start dataset with rigorous step-by-step validation, providing a strong foundation for reasoning capabilities. The subsequent RL stage leverages a 74K-sample dataset across diverse domains to further sharpen and stabilize these abilities, resulting in a more robust and efficient learning process. Extensive evaluations demonstrate that our training recipe not only surpasses strong baselines but also highlights the critical role of data quality and training design in shaping multimodal reasoning performance. Notably, our method achieves a 11.6% improvement over the Qwen2.5-VL-7B-Instruct baseline across nine multimodal reasoning benchmarks, establishing a solid empirical foundation for future large-scale multimodal reasoning research. We open-sourced all our codes, pipeline, and data at https://github.com/EvolvingLMMs-Lab/OpenMMReasoner.

</details>


### [52] [A Hybrid Proactive And Predictive Framework For Edge Cloud Resource Management](https://arxiv.org/abs/2511.16075)
*Hrikshesh Kumar,Anika Garg,Anshul Gupta,Yashika Agarwal*

Main category: cs.AI

TL;DR: A proactive cloud-edge workload management framework that combines CNN-LSTM time series forecasting with multi-agent Deep Reinforcement Learning to optimize resource allocation by predicting future needs rather than reacting to current problems.


<details>
  <summary>Details</summary>
Motivation: Traditional reactive resource management using static thresholds leads to either overspending on unnecessary resources or performance degradation due to insufficient resources, highlighting the need for proactive solutions.

Method: Hybrid architecture embedding CNN-LSTM time series forecasting predictions directly into the state space of a multi-agent Deep Reinforcement Learning orchestrator, enabling the AI manager to anticipate future workload patterns.

Result: The system significantly outperforms traditional methods, effectively solving complex multi-objective optimization problems involving cost efficiency, performance, and reliability simultaneously.

Conclusion: By providing predictive capabilities that allow the AI manager to "see the future," the framework enables smooth, long-term planning and optimal resource allocation decisions rather than reactive problem-solving.

Abstract: Old cloud edge workload resource management is too reactive. The problem with relying on static thresholds is that we are either overspending for more resources than needed or have reduced performance because of their lack. This is why we work on proactive solutions. A framework developed for it stops reacting to the problems but starts expecting them. We design a hybrid architecture, combining two powerful tools: the CNN LSTM model for time series forecasting and an orchestrator based on multi agent Deep Reinforcement Learning In fact the novelty is in how we combine them as we embed the predictive forecast from the CNN LSTM directly into the DRL agent state space. That is what makes the AI manager smarter it sees the future, which allows it to make better decisions about a long term plan for where to run tasks That means finding that sweet spot between how much money is saved while keeping the system healthy and apps fast for users That is we have given it eyes in order to see down the road so that it does not have to lurch from one problem to another it finds a smooth path forward Our tests show our system easily beats the old methods It is great at solving tough problems like making complex decisions and juggling multiple goals at once like being cheap fast and reliable

</details>


### [53] [TOFA: Training-Free One-Shot Federated Adaptation for Vision-Language Models](https://arxiv.org/abs/2511.16423)
*Li Zhang,Zhongxuan Han,XiaoHua Feng,Jiaming Zhang,Yuyuan Li,Linbo Jiang,Jianan Lin,Chaochao Chen*

Main category: cs.AI

TL;DR: TOFA is a training-free one-shot federated adaptation framework for Vision-Language Models that addresses data heterogeneity without requiring additional client or server training resources.


<details>
  <summary>Details</summary>
Motivation: Existing VLM adaptation methods in federated learning incur high communication costs and security risks from iterative training, while current one-shot approaches fail to fully exploit multimodal information, handle data heterogeneity, and avoid additional training requirements.

Method: TOFA uses visual and textual pipelines: a hierarchical Bayesian model learns personalized class prototypes visually, while text prompts are evaluated and globally aligned. An adaptive weight calibration mechanism combines both modalities to balance personalization and robustness.

Result: Extensive experiments across 9 datasets in various federated settings demonstrate TOFA's effectiveness in adapting VLMs efficiently.

Conclusion: TOFA provides an effective training-free solution for one-shot federated VLM adaptation that fully leverages multimodal features while handling data heterogeneity without requiring additional training resources.

Abstract: Efficient and lightweight adaptation of pre-trained Vision-Language Models (VLMs) to downstream tasks through collaborative interactions between local clients and a central server is a rapidly emerging research topic in federated learning. Existing adaptation algorithms are typically trained iteratively, which incur significant communication costs and increase the susceptibility to potential attacks. Motivated by the one-shot federated training techniques that reduce client-server exchanges to a single round, developing a lightweight one-shot federated VLM adaptation method to alleviate these issues is particularly attractive. However, current one-shot approaches face certain challenges in adapting VLMs within federated settings: (1) insufficient exploitation of the rich multimodal information inherent in VLMs; (2) lack of specialized adaptation strategies to systematically handle the severe data heterogeneity; and (3) requiring additional training resource of clients or server. To bridge these gaps, we propose a novel Training-free One-shot Federated Adaptation framework for VLMs, named TOFA. To fully leverage the generalizable multimodal features in pre-trained VLMs, TOFA employs both visual and textual pipelines to extract task-relevant representations. In the visual pipeline, a hierarchical Bayesian model learns personalized, class-specific prototype distributions. For the textual pipeline, TOFA evaluates and globally aligns the generated local text prompts for robustness. An adaptive weight calibration mechanism is also introduced to combine predictions from both modalities, balancing personalization and robustness to handle data heterogeneity. Our method is training-free, not relying on additional training resources on either the client or server side. Extensive experiments across 9 datasets in various federated settings demonstrate the effectiveness of the proposed TOFA method.

</details>


### [54] [SkyRL-Agent: Efficient RL Training for Multi-turn LLM Agent](https://arxiv.org/abs/2511.16108)
*Shiyi Cao,Dacheng Li,Fangzhou Zhao,Shuo Yuan,Sumanth R. Hegde,Connor Chen,Charlie Ruan,Tyler Griggs,Shu Liu,Eric Tang,Richard Liaw,Philipp Moritz,Matei Zaharia,Joseph E. Gonzalez,Ion Stoica*

Main category: cs.AI

TL;DR: SkyRL-Agent is a framework for efficient multi-turn agent training with optimized asynchronous dispatching and tool integration, used to train SA-SWE-32B software engineering agent that achieves 39.4% Pass@1 on SWE-Bench with 2x cost reduction.


<details>
  <summary>Details</summary>
Motivation: To create an efficient framework for training and evaluating multi-turn, long-horizon agents with seamless integration to existing RL frameworks, addressing the need for cost-effective agent training.

Method: Uses optimized asynchronous pipeline dispatcher (1.55x speedup), AST-based search tool for code navigation, and tool-enhanced training recipe to improve rollout efficiency and training performance.

Result: SA-SWE-32B achieves 39.4% Pass@1 on SWE-Bench Verified with >2x cost reduction compared to prior models, and generalizes well to other agentic tasks like Terminal-Bench, BrowseComp-Plus, and WebArena.

Conclusion: SkyRL-Agent enables efficient agent training with significant performance improvements and cost reductions, demonstrating extensibility across various agent types and training backends.

Abstract: We introduce SkyRL-Agent, a framework for efficient, multi-turn, long-horizon agent training and evaluation. It provides efficient asynchronous dispatching, lightweight tool integration, and flexible backend interoperability, enabling seamless use with existing RL frameworks such as SkyRL-train, VeRL, and Tinker.
  Using SkyRL-Agent, we train SA-SWE-32B, a software engineering agent trained from Qwen3-32B (24.4% Pass@1) purely with reinforcement learning. We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and improve training efficiency. Together, these optimizations enable SA-SWE-32B to reach 39.4% Pass@1 on SWE-Bench Verified with more than 2x cost reduction compared to prior models reaching similar performance. Despite being trained solely on SWE tasks, SA-SWE-32B generalizes effectively to other agentic tasks, including Terminal-Bench, BrowseComp-Plus, and WebArena. We further demonstrate SkyRL-Agent's extensibility through case studies on deep research, computer use, and memory agents, each trained using a different training backend.

</details>


### [55] [D-GARA: A Dynamic Benchmarking Framework for GUI Agent Robustness in Real-World Anomalies](https://arxiv.org/abs/2511.16590)
*Sen Chen,Tong Zhao,Yi Bin,Fei Ma,Wenqi Shao,Zheng Wang*

Main category: cs.AI

TL;DR: D-GARA is a dynamic benchmarking framework that evaluates Android GUI agent robustness against real-world anomalies like permission dialogs and battery warnings, revealing significant performance degradation in current agents.


<details>
  <summary>Details</summary>
Motivation: Existing GUI agent datasets are static and idealized, failing to capture real-world complexity and anomalies, creating a gap in evaluating agent robustness.

Method: Proposed D-GARA framework with diverse real-world anomalies, constructed benchmark with annotated Android apps containing embedded anomalies, and conducted comprehensive experiments.

Result: State-of-the-art GUI agents showed substantial performance degradation when exposed to anomaly-rich environments, demonstrating the need for robustness-aware learning.

Conclusion: D-GARA provides a modular, extensible framework for evaluating GUI agent robustness against real-world anomalies, supporting integration of new tasks and anomaly types.

Abstract: Developing intelligent agents capable of operating a wide range of Graphical User Interfaces (GUIs) with human-level proficiency is a key milestone on the path toward Artificial General Intelligence. While most existing datasets and benchmarks for training and evaluating GUI agents are static and idealized, failing to reflect the complexity and unpredictability of real-world environments, particularly the presence of anomalies. To bridge this research gap, we propose D-GARA, a dynamic benchmarking framework, to evaluate Android GUI agent robustness in real-world anomalies. D-GARA introduces a diverse set of real-world anomalies that GUI agents commonly face in practice, including interruptions such as permission dialogs, battery warnings, and update prompts. Based on D-GARA framework, we construct and annotate a benchmark featuring commonly used Android applications with embedded anomalies to support broader community research. Comprehensive experiments and results demonstrate substantial performance degradation in state-of-the-art GUI agents when exposed to anomaly-rich environments, highlighting the need for robustness-aware learning. D-GARA is modular and extensible, supporting the seamless integration of new tasks, anomaly types, and interaction scenarios to meet specific evaluation goals.

</details>


### [56] [Multidimensional Rubric-oriented Reward Model Learning via Geometric Projection Reference Constraints](https://arxiv.org/abs/2511.16139)
*Yongnan Jin,Xurui Li,Feng Cao,Liucun Gao,Juanjuan Yao*

Main category: cs.AI

TL;DR: MR-RML is a novel alignment framework that addresses LLM limitations in medical practice through multidimensional rubric-oriented reward model learning with geometric projection constraints, achieving state-of-the-art performance on medical benchmarks.


<details>
  <summary>Details</summary>
Motivation: To overcome critical alignment challenges in medical LLMs including disconnect from clinical needs, difficulty adapting to evolving standards, and inability to capture nuanced medical quality criteria.

Method: Proposes MR-RML via GPRC framework with three innovations: (1) Dimensions-Scenarios-Disciplines medical standard system, (2) independent multi-dimensional reward model, (3) geometric projection reference constraints that transform medical logic into mathematical regularization.

Result: Achieved substantial performance gains over base Qwen-32B (45% on full subset, 85% on Hard subset) and SOTA among open-source LLMs with scores of 62.7 (full) and 44.7 (hard), outperforming most closed-source models.

Conclusion: MR-RML successfully integrates medical standards into LLM training pipeline and demonstrates significant improvements in clinical alignment and performance through structured multidimensional evaluation and geometric constraints.

Abstract: The integration of large language models (LLMs) into medical practice holds transformative potential, yet their real-world clinical utility remains limited by critical alignment challenges: (1) a disconnect between static evaluation benchmarks and dynamic clinical cognitive needs, (2) difficulties in adapting to evolving, multi-source medical standards, and (3) the inability of conventional reward models to capture nuanced, multi-dimensional medical quality criteria. To address these gaps, we propose MR-RML (Multidimensional Rubric-oriented Reward Model Learning) via GPRC (Geometric Projection Reference Constraints), a novel alignment framework that integrates medical standards into a structured "Dimensions-Scenarios-Disciplines" matrix to guide data generation and model optimization. MR-RML introduces three core innovations: (1) a "Dimensions-Scenarios-Disciplines" medical standard system that embeds domain standards into the full training pipeline; (2) an independent multi-dimensional reward model that decomposes evaluation criteria, shifting from real-time rubric-based scoring to internalized reward modeling for improved consistency and cost-efficiency; (3) geometric projection reference constraints that transform medical cognitive logic into mathematical regularization, aligning scoring gradients with clinical reasoning and enabling synthetic data-driven training. Through extensive evaluations on the authoritative medical benchmark Healthbench, our method yields substantial performance gains over the base LLM Qwen-32B (45% on the full subset and 85% on Hard subset, respectively). It achieves a SOTA among open-source LLMs with scores of 62.7 (full subset) and 44.7 (hard subset), while also outperforming the majority of closed-source models.

</details>


### [57] [FOOTPASS: A Multi-Modal Multi-Agent Tactical Context Dataset for Play-by-Play Action Spotting in Soccer Broadcast Videos](https://arxiv.org/abs/2511.16183)
*Jeremie Ochin,Raphael Chekroun,Bogdan Stanciulescu,Sotiris Manitsaris*

Main category: cs.AI

TL;DR: FOOTPASS is the first benchmark dataset for play-by-play action spotting in soccer that integrates computer vision outputs with tactical knowledge to generate reliable play-by-play data streams for sports analytics.


<details>
  <summary>Details</summary>
Motivation: Current action recognition methods are insufficient for reliable play-by-play data extraction in soccer, while tactical modeling research has advanced but requires better automated data extraction methods.

Method: Leveraging tactical knowledge as prior to support computer-vision-based predictions, integrating both STAD and MOT approaches in a multi-modal, multi-agent tactical context.

Result: Creation of FOOTPASS dataset enabling development of player-centric action spotting methods that exploit computer vision outputs and soccer tactical regularities.

Conclusion: FOOTPASS enables more automated and reliable extraction of play-by-play data essential for data-driven sports analytics by combining computer vision with tactical knowledge.

Abstract: Soccer video understanding has motivated the creation of datasets for tasks such as temporal action localization, spatiotemporal action detection (STAD), or multiobject tracking (MOT). The annotation of structured sequences of events (who does what, when, and where) used for soccer analytics requires a holistic approach that integrates both STAD and MOT. However, current action recognition methods remain insufficient for constructing reliable play-by-play data and are typically used to assist rather than fully automate annotation. Parallel research has advanced tactical modeling, trajectory forecasting, and performance analysis, all grounded in game-state and play-by-play data. This motivates leveraging tactical knowledge as a prior to support computer-vision-based predictions, enabling more automated and reliable extraction of play-by-play data. We introduce Footovision Play-by-Play Action Spotting in Soccer Dataset (FOOTPASS), the first benchmark for play-by-play action spotting over entire soccer matches in a multi-modal, multi-agent tactical context. It enables the development of methods for player-centric action spotting that exploit both outputs from computer-vision tasks (e.g., tracking, identification) and prior knowledge of soccer, including its tactical regularities over long time horizons, to generate reliable play-by-play data streams. These streams form an essential input for data-driven sports analytics.

</details>


### [58] [From Performance to Understanding: A Vision for Explainable Automated Algorithm Design](https://arxiv.org/abs/2511.16201)
*Niki van Stein,Anna V. Kononova,Thomas Bäck*

Main category: cs.AI

TL;DR: The paper proposes explainable automated algorithm design by combining LLM-driven discovery with systematic benchmarking and problem-class descriptors to create interpretable, class-specific algorithms.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based automated algorithm design focuses on performance but lacks explainability - it doesn't reveal why algorithms work, which components matter, or how design choices relate to problem structures.

Method: Three-pillar approach: (1) LLM-driven discovery of algorithmic variants, (2) explainable benchmarking that attributes performance to components and hyperparameters, (3) problem-class descriptors connecting algorithm behavior to landscape structure.

Result: Forms a closed knowledge loop where discovery, explanation, and generalization reinforce each other, shifting from blind search to interpretable algorithm design.

Conclusion: This integration will accelerate progress while producing reusable scientific insight into when and why optimization strategies succeed, moving beyond mere performance-driven automation.

Abstract: Automated algorithm design is entering a new phase: Large Language Models can now generate full optimisation (meta)heuristics, explore vast design spaces and adapt through iterative feedback. Yet this rapid progress is largely performance-driven and opaque. Current LLM-based approaches rarely reveal why a generated algorithm works, which components matter or how design choices relate to underlying problem structures. This paper argues that the next breakthrough will come not from more automation, but from coupling automation with understanding from systematic benchmarking. We outline a vision for explainable automated algorithm design, built on three pillars: (i) LLM-driven discovery of algorithmic variants, (ii) explainable benchmarking that attributes performance to components and hyperparameters and (iii) problem-class descriptors that connect algorithm behaviour to landscape structure. Together, these elements form a closed knowledge loop in which discovery, explanation and generalisation reinforce each other. We argue that this integration will shift the field from blind search to interpretable, class-specific algorithm design, accelerating progress while producing reusable scientific insight into when and why optimisation strategies succeed.

</details>


### [59] [Multi-Agent Collaborative Reward Design for Enhancing Reasoning in Reinforcement Learning](https://arxiv.org/abs/2511.16202)
*Pei Yang,Ke Zhang,Ji Wang,Xiao Chen,Yuxin Tang,Eric Yang,Lynn Ai,Bill Shi*

Main category: cs.AI

TL;DR: CRM is a multi-agent collaborative reward framework that replaces single reward models with specialized evaluators to improve robustness and interpretability in RLHF by decomposing preference evaluation across domain-specific agents.


<details>
  <summary>Details</summary>
Motivation: Conventional reward models struggle with optimizing multiple conflicting preference dimensions (factuality, helpfulness, safety) and lack transparency in scoring decisions.

Method: Decomposes preference evaluation into domain-specific agents producing partial signals, with global evaluators and a centralized aggregator that fuses signals balancing step-wise correctness, multi-agent agreement, and repetition penalties.

Result: Enables multi-perspective reward shaping without additional human annotations, yielding single training rewards compatible with standard RL pipelines with advantage-based updates.

Conclusion: CRM and rewardBench provide a practical, modular path to more transparent reward modeling and more stable optimization in RLHF.

Abstract: We present CRM (Multi-Agent Collaborative Reward Model), a framework that replaces a single black-box reward model with a coordinated team of specialist evaluators to improve robustness and interpretability in RLHF. Conventional reward models struggle to jointly optimize multiple, sometimes conflicting, preference dimensions (e.g., factuality, helpfulness, safety) and offer limited transparency into why a score is assigned. CRM addresses these issues by decomposing preference evaluation into domain-specific agents that each produce partial signals, alongside global evaluators such as ranker-based and embedding-similarity rewards. A centralized aggregator fuses these signals at each timestep, balancing factors like step-wise correctness, multi-agent agreement, and repetition penalties, yielding a single training reward compatible with standard RL pipelines. The policy is optimized with advantage-based updates (e.g., GAE), while a value model regresses to the aggregated reward, enabling multi-perspective reward shaping without requiring additional human annotations beyond those used to train the evaluators. To support training and assessment, we introduce rewardBench, a benchmark and training suite aligned with the collaborative structure of CRM. Together, CRM and rewardBench provide a practical, modular path to more transparent reward modeling and more stable optimization.

</details>


### [60] [ChemLabs on ChemO: A Multi-Agent System for Multimodal Reasoning on IChO 2025](https://arxiv.org/abs/2511.16205)
*Xu Qiang,Shengyuan Bai,Leqing Chen,Zijing Liu,Yu Li*

Main category: cs.AI

TL;DR: ChemO is a new benchmark for AI chemical reasoning based on International Chemistry Olympiad problems, featuring innovations for automated assessment and a multi-agent framework that achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Chemistry has remained an open challenge for AI reasoning due to its unique multimodal symbolic language, unlike mathematics and physics which already have established benchmarks.

Method: Uses Assessment-Equivalent Reformulation (AER) to convert visual output problems into tractable formats and Structured Visual Enhancement (SVE) to disentangle visual perception from chemical reasoning. Proposes ChemLabs, a hierarchical multi-agent framework with specialized agents for problem decomposition, perception, reasoning, and auditing.

Result: The top configuration achieves 93.6/100 score, surpassing estimated human gold medal threshold and establishing new state-of-the-art in automated chemical problem-solving.

Conclusion: ChemO provides a crucial testbed for advanced AI chemical reasoning, and the multi-agent approach with SVE yields dramatic performance improvements, demonstrating the framework's effectiveness.

Abstract: Olympiad-level benchmarks in mathematics and physics are crucial testbeds for advanced AI reasoning, but chemistry, with its unique multimodal symbolic language, has remained an open challenge. We introduce ChemO, a new benchmark built from the International Chemistry Olympiad (IChO) 2025. ChemO features two key innovations for automated assessment: Assessment-Equivalent Reformulation (AER), which converts problems requiring visual outputs (e.g., drawing molecules) into computationally tractable formats, and Structured Visual Enhancement (SVE), a diagnostic mechanism to disentangle a model's visual perception capabilities from its core chemical reasoning. To tackle this benchmark, we propose ChemLabs, a hierarchical multi-agent framework that mimics human expert collaboration through specialized agents for problem decomposition, perception, reasoning, and auditing. Experiments on state-of-the-art multimodal models demonstrate that combining SVE with our multi-agent system yields dramatic performance gains. Our top configuration achieves a score of 93.6 out of 100, surpassing an estimated human gold medal threshold and establishing a new state-of-the-art in automated chemical problem-solving. ChemO Dataset: https://huggingface.co/datasets/IDEA-AI4SCI/ChemO

</details>


### [61] [FlipVQA-Miner: Cross-Page Visual Question-Answer Mining from Textbooks](https://arxiv.org/abs/2511.16216)
*Zhen Hao Wong,Jingwen Deng,Hao Liang,Runming He,Chengyu Shen,Wentao Zhang*

Main category: cs.AI

TL;DR: An automated pipeline that extracts high-quality QA and VQA pairs from educational documents using layout-aware OCR and LLM-based semantic parsing, enabling scalable use of real-world educational content for LLM training.


<details>
  <summary>Details</summary>
Motivation: Existing instruction-tuning and RL datasets are costly to curate and often rely on synthetic samples that introduce hallucination and limited diversity, while abundant high-quality human-authored QA content in educational materials remains underexploited due to transformation difficulties.

Method: Combines layout-aware OCR with LLM-based semantic parsing to automatically extract well-formed QA and visual-QA pairs from educational documents, transforming raw PDFs into AI-ready supervision.

Result: Experiments across diverse document types show the method produces accurate, aligned, and low-noise QA/VQA pairs, enabling scalable use of real-world educational content.

Conclusion: This approach provides a practical alternative to synthetic data generation for improving reasoning-oriented LLM training and enables scalable use of real-world educational content.

Abstract: The development of Large Language Models (LLMs) increasingly depends on high-quality supervised data, yet existing instruction-tuning and RL datasets remain costly to curate and often rely on synthetic samples that introduce hallucination and limited diversity. At the same time, textbooks and exercise materials contain abundant, high-quality human-authored Question-Answer(QA) content that remains underexploited due to the difficulty of transforming raw PDFs into AI-ready supervision. Although modern OCR and vision-language models can accurately parse document structure, their outputs lack the semantic alignment required for training. We propose an automated pipeline that extracts well-formed QA and visual-QA (VQA) pairs from educational documents by combining layout-aware OCR with LLM-based semantic parsing. Experiments across diverse document types show that the method produces accurate, aligned, and low-noise QA/VQA pairs. This approach enables scalable use of real-world educational content and provides a practical alternative to synthetic data generation for improving reasoning-oriented LLM training. All code and data-processing pipelines are open-sourced at https://github.com/OpenDCAI/DataFlow.

</details>


### [62] [Revisiting Fairness-aware Interactive Recommendation: Item Lifecycle as a Control Knob](https://arxiv.org/abs/2511.16248)
*Yun Lu,Xiaoyu Shi,Hong Xie,Chongjun Xia,Zhenhui Gong,Mingsheng Shang*

Main category: cs.AI

TL;DR: This paper introduces LHRL, a lifecycle-aware hierarchical reinforcement learning framework for fair interactive recommendation that leverages item lifecycle patterns to dynamically balance fairness and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address fairness in interactive recommendation systems by incorporating item lifecycle patterns, which were found to follow a compressed three-phase pattern in short-video platforms rather than the classical four-stage model.

Method: LHRL framework with PhaseFormer (STL decomposition + attention for phase detection) and two-level HRL agent (high-level policy for phase-aware fairness constraints, low-level policy for user engagement optimization).

Result: Experiments on real-world datasets show LHRL significantly improves both fairness and user engagement, and lifecycle-aware rewards consistently boost performance in existing RL-based models.

Conclusion: The lifecycle-aware approach effectively reconciles long-term equity and short-term utility in recommendation systems, demonstrating generalizability and practical value.

Abstract: This paper revisits fairness-aware interactive recommendation (e.g., TikTok, KuaiShou) by introducing a novel control knob, i.e., the lifecycle of items. We make threefold contributions. First, we conduct a comprehensive empirical analysis and uncover that item lifecycles in short-video platforms follow a compressed three-phase pattern, i.e., rapid growth, transient stability, and sharp decay, which significantly deviates from the classical four-stage model (introduction, growth, maturity, decline). Second, we introduce LHRL, a lifecycle-aware hierarchical reinforcement learning framework that dynamically harmonizes fairness and accuracy by leveraging phase-specific exposure dynamics. LHRL consists of two key components: (1) PhaseFormer, a lightweight encoder combining STL decomposition and attention mechanisms for robust phase detection; (2) a two-level HRL agent, where the high-level policy imposes phase-aware fairness constraints, and the low-level policy optimizes immediate user engagement. This decoupled optimization allows for effective reconciliation between long-term equity and short-term utility. Third, experiments on multiple real-world interactive recommendation datasets demonstrate that LHRL significantly improves both fairness and user engagement. Furthermore, the integration of lifecycle-aware rewards into existing RL-based models consistently yields performance gains, highlighting the generalizability and practical value of our approach.

</details>


### [63] [MuISQA: Multi-Intent Retrieval-Augmented Generation for Scientific Question Answering](https://arxiv.org/abs/2511.16283)
*Zhiyuan Li,Haisheng Yu,Guangchuan Guo,Nan Zhou,Jiajun Zhang*

Main category: cs.AI

TL;DR: The paper introduces MuISQA benchmark for evaluating multi-intent scientific QA and proposes an intent-aware retrieval framework using LLMs to decompose questions, retrieve evidence for each intent, and aggregate results via RRF.


<details>
  <summary>Details</summary>
Motivation: Conventional RAG systems are single-intent oriented, leading to incomplete evidence coverage for complex scientific questions that require multi-hop reasoning across diverse sources.

Method: An intent-aware retrieval framework that uses LLMs to hypothesize answers, decompose questions into intent-specific queries, retrieve supporting passages for each intent, and aggregate results using Reciprocal Rank Fusion (RRF).

Result: Experiments on MuISQA benchmark and general RAG datasets show consistent outperformance over conventional approaches, particularly in retrieval accuracy and evidence coverage.

Conclusion: The proposed intent-aware retrieval framework effectively addresses the limitations of single-intent RAG systems by improving evidence coverage across multiple intents in complex scientific question answering.

Abstract: Complex scientific questions often entail multiple intents, such as identifying gene mutations and linking them to related diseases. These tasks require evidence from diverse sources and multi-hop reasoning, while conventional retrieval-augmented generation (RAG) systems are usually single-intent oriented, leading to incomplete evidence coverage. To assess this limitation, we introduce the Multi-Intent Scientific Question Answering (MuISQA) benchmark, which is designed to evaluate RAG systems on heterogeneous evidence coverage across sub-questions. In addition, we propose an intent-aware retrieval framework that leverages large language models (LLMs) to hypothesize potential answers, decompose them into intent-specific queries, and retrieve supporting passages for each underlying intent. The retrieved fragments are then aggregated and re-ranked via Reciprocal Rank Fusion (RRF) to balance coverage across diverse intents while reducing redundancy. Experiments on both MuISQA benchmark and other general RAG datasets demonstrate that our method consistently outperforms conventional approaches, particularly in retrieval accuracy and evidence coverage.

</details>


### [64] [Distributed Agent Reasoning Across Independent Systems With Strict Data Locality](https://arxiv.org/abs/2511.16292)
*Daniel Vaughan,Kateřina Vaughan*

Main category: cs.AI

TL;DR: Proof-of-concept demonstration of agent-to-agent communication across distributed systems using natural-language messages without shared identifiers or centralized data exchange, enabling secure cooperation between organizations via pseudonymised tokens and local data lookups.


<details>
  <summary>Details</summary>
Motivation: To explore how multiple organizations can cooperate securely while maintaining data privacy, using pseudonymised case tokens and controlled operational boundaries without requiring shared identifiers or centralized data exchange.

Method: Uses Orpius platform for multi-agent orchestration with OperationRelay calls for natural-language communication. Agents operate on local data (clinic records, insurance tables, clinical guidance) using HMAC-based pseudonymous tokens. Clinic computes tokens, Insurer evaluates coverage and consults Specialist, who returns recommendations.

Result: Successfully demonstrated feasibility of distributed agent communication using natural-language messages and pseudonymised tokens. System enabled secure cooperation between Clinic, Insurer, and Specialist Network while keeping patient data local and preventing identity reconstruction.

Conclusion: The prototype highlights architectural patterns for privacy-preserving distributed reasoning among specialized agents. Future work should focus on more rigorous evaluation and research in decentralized multi-agent systems, as this was a limited proof-of-concept without clinical validation.

Abstract: This paper presents a proof-of-concept demonstration of agent-to-agent communication across distributed systems, using only natural-language messages and without shared identifiers, structured schemas, or centralised data exchange. The prototype explores how multiple organisations (represented here as a Clinic, Insurer, and Specialist Network) can cooperate securely via pseudonymised case tokens, local data lookups, and controlled operational boundaries.
  The system uses Orpius as the underlying platform for multi-agent orchestration, tool execution, and privacy-preserving communication. All agents communicate through OperationRelay calls, exchanging concise natural-language summaries. Each agent operates on its own data (such as synthetic clinic records, insurance enrolment tables, and clinical guidance extracts), and none receives or reconstructs patient identity. The Clinic computes an HMAC-based pseudonymous token, the Insurer evaluates coverage rules and consults the Specialist agent, and the Specialist returns an appropriateness recommendation.
  The goal of this prototype is intentionally limited: to demonstrate feasibility, not to provide a clinically validated, production-ready system. No clinician review was conducted, and no evaluation beyond basic functional runs was performed. The work highlights architectural patterns, privacy considerations, and communication flows that enable distributed reasoning among specialised agents while keeping data local to each organisation. We conclude by outlining opportunities for more rigorous evaluation and future research in decentralised multi-agent systems.

</details>


### [65] [Reducing Instability in Synthetic Data Evaluation with a Super-Metric in MalDataGen](https://arxiv.org/abs/2511.16373)
*Anna Luiza Gomes da Silva,Diego Kreutz,Angelo Diniz,Rodrigo Mansilha,Celso Nobre da Fonseca*

Main category: cs.AI

TL;DR: This paper introduces a Super-Metric for evaluating synthetic Android malware data quality, aggregating eight metrics across four fidelity dimensions into a single weighted score that shows improved stability and correlation with classifier performance.


<details>
  <summary>Details</summary>
Motivation: Existing metrics for evaluating synthetic Android malware data suffer from instability and lack of standardization, making quality assessment challenging.

Method: Integrated a Super-Metric into MalDataGen that aggregates eight metrics across four fidelity dimensions to produce a single weighted score.

Result: Experiments with ten generative models and five balanced datasets showed the Super-Metric is more stable and consistent than traditional metrics, with stronger correlations to classifier performance.

Conclusion: The proposed Super-Metric provides a more reliable and standardized approach for evaluating synthetic Android malware data quality compared to existing metrics.

Abstract: Evaluating the quality of synthetic data remains a persistent challenge in the Android malware domain due to instability and the lack of standardization among existing metrics. This work integrates into MalDataGen a Super-Metric that aggregates eight metrics across four fidelity dimensions, producing a single weighted score. Experiments involving ten generative models and five balanced datasets demonstrate that the Super-Metric is more stable and consistent than traditional metrics, exhibiting stronger correlations with the actual performance of classifiers.

</details>


### [66] [An Agent-Based Framework for the Automatic Validation of Mathematical Optimization Models](https://arxiv.org/abs/2511.16383)
*Alexander Zadorojniy,Segev Wasserkrug,Eitan Farchi*

Main category: cs.AI

TL;DR: Proposes an agent-based method for automatic validation of optimization models generated from natural language descriptions, extending software testing techniques to optimization modeling.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of validating that optimization models generated by LLMs from natural language descriptions are correct and meet requirements, as current methods lack systematic validation approaches.

Method: Uses an ensemble of agents that: 1) generate a problem-level testing API, 2) create tests using this API, and 3) generate optimization-specific mutations to assess test suite quality.

Result: The framework demonstrates high-quality validation through experiments, measured by mutation coverage - a well-known software testing metric.

Conclusion: The proposed agent-based validation framework effectively addresses the validation challenge for LLM-generated optimization models by adapting software testing methodologies.

Abstract: Recently, using Large Language Models (LLMs) to generate optimization models from natural language descriptions has became increasingly popular. However, a major open question is how to validate that the generated models are correct and satisfy the requirements defined in the natural language description. In this work, we propose a novel agent-based method for automatic validation of optimization models that builds upon and extends methods from software testing to address optimization modeling . This method consists of several agents that initially generate a problem-level testing API, then generate tests utilizing this API, and, lastly, generate mutations specific to the optimization model (a well-known software testing technique assessing the fault detection power of the test suite). In this work, we detail this validation framework and show, through experiments, the high quality of validation provided by this agent ensemble in terms of the well-known software testing measure called mutation coverage.

</details>


### [67] [CorrectHDL: Agentic HDL Design with LLMs Leveraging High-Level Synthesis as Reference](https://arxiv.org/abs/2511.16395)
*Kangwei Xu,Grace Li Zhang,Ulf Schlichtmann,Bing Li*

Main category: cs.AI

TL;DR: CorrectHDL is a framework that uses HLS results as functional references to correct errors in LLM-generated HDL designs, achieving better area and power efficiency than conventional HLS while maintaining functional correctness.


<details>
  <summary>Details</summary>
Motivation: LLMs show potential in hardware design but suffer from hallucination that introduces functional errors in generated HDL designs, requiring a method to ensure correctness while leveraging LLM capabilities.

Method: Uses C/C++ program input to generate HDL via LLM, repairs syntax errors with RAG, then iteratively improves functional correctness by comparing simulated behavior with HLS reference design from conventional tools.

Result: Generated circuits achieve significantly better area and power efficiency than conventional HLS designs and approach human-engineered circuit quality while maintaining functional correctness.

Conclusion: The framework effectively combines LLM generative capabilities with traditional correctness-driven IC design flows, demonstrating the potential of agentic HDL design.

Abstract: Large Language Models (LLMs) have demonstrated remarkable potential in hardware front-end design using hardware description languages (HDLs). However, their inherent tendency toward hallucination often introduces functional errors into the generated HDL designs. To address this issue, we propose the framework CorrectHDL that leverages high-level synthesis (HLS) results as functional references to correct potential errors in LLM-generated HDL designs.The input to the proposed framework is a C/C++ program that specifies the target circuit's functionality. The program is provided to an LLM to directly generate an HDL design, whose syntax errors are repaired using a Retrieval-Augmented Generation (RAG) mechanism. The functional correctness of the LLM-generated circuit is iteratively improved by comparing its simulated behavior with an HLS reference design produced by conventional HLS tools, which ensures the functional correctness of the result but can lead to suboptimal area and power efficiency. Experimental results demonstrate that circuits generated by the proposed framework achieve significantly better area and power efficiency than conventional HLS designs and approach the quality of human-engineered circuits. Meanwhile, the correctness of the resulting HDL implementation is maintained, highlighting the effectiveness and potential of agentic HDL design leveraging the generative capabilities of LLMs and the rigor of traditional correctness-driven IC design flows.

</details>


### [68] [Trustworthy AI in the Agentic Lakehouse: from Concurrency to Governance](https://arxiv.org/abs/2511.16402)
*Jacopo Tagliabue,Federico Bianchi,Ciro Greco*

Main category: cs.AI

TL;DR: The paper proposes Bauplan, an agent-first lakehouse design that addresses infrastructure challenges to enable trustworthy agentic workflows through data and compute isolation.


<details>
  <summary>Details</summary>
Motivation: Current enterprises don't trust AI agents with production data because traditional lakehouses are unsuitable for agent access patterns, requiring a new infrastructure approach.

Method: The authors propose Bauplan, which reimplements data and compute isolation in the lakehouse by drawing operational analogies to MVCC in databases but adapting it for decoupled, multi-language settings.

Result: A reference implementation of a self-healing pipeline in Bauplan demonstrates seamless coupling of agent reasoning with guarantees for correctness and trust.

Conclusion: Building trustworthy agentic workflows requires solving the infrastructure problem first - designing lakehouses around transactions enables governance to follow naturally.

Abstract: Even as AI capabilities improve, most enterprises do not consider agents trustworthy enough to work on production data. In this paper, we argue that the path to trustworthy agentic workflows begins with solving the infrastructure problem first: traditional lakehouses are not suited for agent access patterns, but if we design one around transactions, governance follows. In particular, we draw an operational analogy to MVCC in databases and show why a direct transplant fails in a decoupled, multi-language setting. We then propose an agent-first design, Bauplan, that reimplements data and compute isolation in the lakehouse. We conclude by sharing a reference implementation of a self-healing pipeline in Bauplan, which seamlessly couples agent reasoning with all the desired guarantees for correctness and trust.

</details>


### [69] [Pharos-ESG: A Framework for Multimodal Parsing, Contextual Narration, and Hierarchical Labeling of ESG Report](https://arxiv.org/abs/2511.16417)
*Yan Chen,Yu Zou,Jialei Zeng,Haoran You,Xiaorui Zhou,Aixi Zhong*

Main category: cs.AI

TL;DR: Pharos-ESG is a unified framework that transforms unstructured ESG reports into structured representations using multimodal parsing, contextual narration, and hierarchical labeling to address challenges in understanding ESG reports due to irregular layouts and weak structure.


<details>
  <summary>Details</summary>
Motivation: ESG reports present significant challenges for large-scale understanding due to chaotic reading order from slide-like irregular layouts and implicit hierarchies from lengthy, weakly structured content, making it difficult to assess corporate ESG performance effectively.

Method: The framework integrates a reading-order modeling module based on layout flow, hierarchy-aware segmentation guided by table-of-contents anchors, and a multimodal aggregation pipeline that contextually transforms visual elements into coherent natural language. It further enriches outputs with ESG, GRI, and sentiment labels.

Result: Extensive experiments on annotated benchmarks demonstrate that Pharos-ESG consistently outperforms both dedicated document parsing systems and general-purpose multimodal models. The authors also release Aurora-ESG, the first large-scale public dataset of ESG reports spanning multiple markets.

Conclusion: Pharos-ESG effectively transforms ESG reports into structured representations with enriched annotations, supporting ESG integration in financial governance and decision-making through improved parsing performance and the creation of a comprehensive ESG report dataset.

Abstract: Environmental, Social, and Governance (ESG) principles are reshaping the foundations of global financial gover- nance, transforming capital allocation architectures, regu- latory frameworks, and systemic risk coordination mecha- nisms. However, as the core medium for assessing corpo- rate ESG performance, the ESG reports present significant challenges for large-scale understanding, due to chaotic read- ing order from slide-like irregular layouts and implicit hier- archies arising from lengthy, weakly structured content. To address these challenges, we propose Pharos-ESG, a uni- fied framework that transforms ESG reports into structured representations through multimodal parsing, contextual nar- ration, and hierarchical labeling. It integrates a reading-order modeling module based on layout flow, hierarchy-aware seg- mentation guided by table-of-contents anchors, and a multi- modal aggregation pipeline that contextually transforms vi- sual elements into coherent natural language. The framework further enriches its outputs with ESG, GRI, and sentiment labels, yielding annotations aligned with the analytical de- mands of financial research. Extensive experiments on anno- tated benchmarks demonstrate that Pharos-ESG consistently outperforms both dedicated document parsing systems and general-purpose multimodal models. In addition, we release Aurora-ESG, the first large-scale public dataset of ESG re- ports, spanning Mainland China, Hong Kong, and U.S. mar- kets, featuring unified structured representations of multi- modal content, enriched with fine-grained layout and seman- tic annotations to better support ESG integration in financial governance and decision-making.

</details>


### [70] [From generative AI to the brain: five takeaways](https://arxiv.org/abs/2511.16432)
*Claudius Gros*

Main category: cs.AI

TL;DR: The paper argues that neuroscience should study generative AI principles and ML characterizations that may be operative in the brain, discussing five specific examples from ML research.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between advances in machine learning/generative AI and neuroscience by investigating which generative principles and ML characterizations could be relevant for understanding brain function and cognitive neuroscience.

Method: The authors suggest a systematic investigation approach and discuss five concrete examples from ML research: shortcomings of world modelling, generation of thought processes, attention mechanisms, neural scaling laws, and quantization techniques.

Result: The paper identifies specific ML concepts and principles that could potentially inform neuroscience research, highlighting how much neuroscience could learn from recent ML advances.

Conclusion: It is imperative for neuroscience to thoroughly investigate generative AI principles and ML characterizations that may be operative in the brain, as these could provide valuable insights into neural information processing and cognitive functions.

Abstract: The big strides seen in generative AI are not based on somewhat obscure algorithms, but due to clearly defined generative principles. The resulting concrete implementations have proven themselves in large numbers of applications. We suggest that it is imperative to thoroughly investigate which of these generative principles may be operative also in the brain, and hence relevant for cognitive neuroscience. In addition, ML research led to a range of interesting characterizations of neural information processing systems. We discuss five examples, the shortcomings of world modelling, the generation of thought processes, attention, neural scaling laws, and quantization, that illustrate how much neuroscience could potentially learn from ML research.

</details>


### [71] [PersonaDrift: A Benchmark for Temporal Anomaly Detection in Language-Based Dementia Monitoring](https://arxiv.org/abs/2511.16445)
*Joy Lai,Alex Mihailidis*

Main category: cs.AI

TL;DR: PersonaDrift is a synthetic benchmark for evaluating methods to detect progressive communication changes in people with dementia, simulating 60-day interaction logs with caregiver-informed personas and testing various detection approaches.


<details>
  <summary>Details</summary>
Motivation: Current computational tools don't effectively track gradual communication changes in dementia patients, which caregivers notice informally but lack systematic monitoring tools.

Method: Created synthetic 60-day interaction logs based on caregiver interviews, simulating two key communication changes (flattened sentiment and off-topic replies) with progressive injection. Evaluated anomaly detection methods, statistical models, sequence models, and classifiers in generalized vs personalized settings.

Result: Flattened sentiment detectable with simple statistical models in low-variability users; semantic drift requires temporal modeling and personalized baselines. Personalized classifiers consistently outperform generalized ones across both tasks.

Conclusion: Individual behavioral context is crucial for detecting communication changes in dementia patients, with personalized approaches showing superior performance over generalized methods.

Abstract: People living with dementia (PLwD) often show gradual shifts in how they communicate, becoming less expressive, more repetitive, or drifting off-topic in subtle ways. While caregivers may notice these changes informally, most computational tools are not designed to track such behavioral drift over time. This paper introduces PersonaDrift, a synthetic benchmark designed to evaluate machine learning and statistical methods for detecting progressive changes in daily communication, focusing on user responses to a digital reminder system. PersonaDrift simulates 60-day interaction logs for synthetic users modeled after real PLwD, based on interviews with caregivers. These caregiver-informed personas vary in tone, modality, and communication habits, enabling realistic diversity in behavior. The benchmark focuses on two forms of longitudinal change that caregivers highlighted as particularly salient: flattened sentiment (reduced emotional tone and verbosity) and off-topic replies (semantic drift). These changes are injected progressively at different rates to emulate naturalistic cognitive trajectories, and the framework is designed to be extensible to additional behaviors in future use cases. To explore this novel application space, we evaluate several anomaly detection approaches, unsupervised statistical methods (CUSUM, EWMA, One-Class SVM), sequence models using contextual embeddings (GRU + BERT), and supervised classifiers in both generalized and personalized settings. Preliminary results show that flattened sentiment can often be detected with simple statistical models in users with low baseline variability, while detecting semantic drift requires temporal modeling and personalized baselines. Across both tasks, personalized classifiers consistently outperform generalized ones, highlighting the importance of individual behavioral context.

</details>


### [72] [Utilizing Large Language Models for Zero-Shot Medical Ontology Extension from Clinical Notes](https://arxiv.org/abs/2511.16548)
*Guanchen Wu,Yuzhang Xie,Huanwei Wu,Zhe He,Hui Shao,Xiao Hu,Carl Yang*

Main category: cs.AI

TL;DR: CLOZE is a zero-shot framework that uses LLMs to automatically extract medical entities from clinical notes and integrate them into hierarchical medical ontologies, requiring no training data while preserving patient privacy.


<details>
  <summary>Details</summary>
Motivation: Clinical notes contain valuable medical insights but are underutilized for ontology extension. There's a need for automated methods to extract and integrate medical concepts from unstructured clinical text into existing ontologies.

Method: Uses large language models (LLMs) to extract medical entities from clinical notes and integrate them into hierarchical medical ontologies in a zero-shot manner without additional training or labeled data, with automated PHI removal for privacy protection.

Result: Experimental results show CLOZE provides accurate, scalable, and privacy-preserving ontology extension, effectively identifying disease-related concepts and capturing complex hierarchical relationships.

Conclusion: CLOZE demonstrates strong potential as a cost-efficient solution for ontology extension that can support various downstream applications in biomedical research and clinical informatics.

Abstract: Integrating novel medical concepts and relationships into existing ontologies can significantly enhance their coverage and utility for both biomedical research and clinical applications. Clinical notes, as unstructured documents rich with detailed patient observations, offer valuable context-specific insights and represent a promising yet underutilized source for ontology extension. Despite this potential, directly leveraging clinical notes for ontology extension remains largely unexplored. To address this gap, we propose CLOZE, a novel framework that uses large language models (LLMs) to automatically extract medical entities from clinical notes and integrate them into hierarchical medical ontologies. By capitalizing on the strong language understanding and extensive biomedical knowledge of pre-trained LLMs, CLOZE effectively identifies disease-related concepts and captures complex hierarchical relationships. The zero-shot framework requires no additional training or labeled data, making it a cost-efficient solution. Furthermore, CLOZE ensures patient privacy through automated removal of protected health information (PHI). Experimental results demonstrate that CLOZE provides an accurate, scalable, and privacy-preserving ontology extension framework, with strong potential to support a wide range of downstream applications in biomedical research and clinical informatics.

</details>


### [73] [Consciousness in Artificial Intelligence? A Framework for Classifying Objections and Constraints](https://arxiv.org/abs/2511.16582)
*Andres Campero,Derek Shiller,Jaan Aru,Jonathan Simon*

Main category: cs.AI

TL;DR: A taxonomical framework for classifying challenges to digital AI consciousness, distinguishing between challenges to computational functionalism and digital consciousness, with three degrees of force from possibility to strict impossibility.


<details>
  <summary>Details</summary>
Motivation: To provide structure and tools for disambiguating between different types of challenges to the possibility of consciousness in digital AI systems, clarifying the debate without taking sides.

Method: Developed a taxonomical framework based on Marr's levels of granularity and three degrees of force, then applied it to analyze 14 prominent examples from scientific and philosophical literature.

Result: Created a systematic classification system that distinguishes challenges by their level of granularity and degree of force, providing clarity in the digital consciousness debate.

Conclusion: The framework offers a valuable tool for structuring the debate about digital AI consciousness by clearly distinguishing between different types of challenges and their implications.

Abstract: We develop a taxonomical framework for classifying challenges to the possibility of consciousness in digital artificial intelligence systems. This framework allows us to identify the level of granularity at which a given challenge is intended (the levels we propose correspond to Marr's levels) and to disambiguate its degree of force: is it a challenge to computational functionalism that leaves the possibility of digital consciousness open (degree 1), a practical challenge to digital consciousness that suggests improbability without claiming impossibility (degree 2), or an argument claiming that digital consciousness is strictly impossible (degree 3)? We apply this framework to 14 prominent examples from the scientific and philosophical literature. Our aim is not to take a side in the debate, but to provide structure and a tool for disambiguating between challenges to computational functionalism and challenges to digital consciousness, as well as between different ways of parsing such challenges.

</details>


### [74] [Formal Abductive Latent Explanations for Prototype-Based Networks](https://arxiv.org/abs/2511.16588)
*Jules Soria,Zakaria Chihani,Julien Girard-Satabin,Alban Grastien,Romain Xu-Darme,Daniela Cancila*

Main category: cs.AI

TL;DR: This paper introduces Abductive Latent Explanations (ALEs), a formal approach to address misleading explanations in case-based reasoning networks by providing sufficient conditions on latent representations that guarantee predictions.


<details>
  <summary>Details</summary>
Motivation: Case-based reasoning models provide explanations by pointing to prototypes, but these explanations can be misleading as different instances with the same explanation may lead to different predictions, which limits their usefulness in safety-critical applications.

Method: The authors propose ALEs, which combine case-based reasoning interpretability with formal XAI guarantees. They develop a solver-free, scalable algorithm using three distinct paradigms to generate sufficient conditions on latent representations that imply predictions.

Result: The approach demonstrates feasibility on diverse datasets for both standard and fine-grained image classification, showing that ALEs provide more reliable explanations than traditional prototype-based explanations.

Conclusion: ALE formalism successfully bridges the gap between inherent interpretability of case-based reasoning models and formal guarantees, offering more trustworthy explanations for safety-critical applications while maintaining scalability.

Abstract: Case-based reasoning networks are machine-learning models that make predictions based on similarity between the input and prototypical parts of training samples, called prototypes. Such models are able to explain each decision by pointing to the prototypes that contributed the most to the final outcome. As the explanation is a core part of the prediction, they are often qualified as ``interpretable by design". While promising, we show that such explanations are sometimes misleading, which hampers their usefulness in safety-critical contexts. In particular, several instances may lead to different predictions and yet have the same explanation. Drawing inspiration from the field of formal eXplainable AI (FXAI), we propose Abductive Latent Explanations (ALEs), a formalism to express sufficient conditions on the intermediate (latent) representation of the instance that imply the prediction. Our approach combines the inherent interpretability of case-based reasoning models and the guarantees provided by formal XAI. We propose a solver-free and scalable algorithm for generating ALEs based on three distinct paradigms, compare them, and present the feasibility of our approach on diverse datasets for both standard and fine-grained image classification. The associated code can be found at https://github.com/julsoria/ale

</details>


### [75] [You Only Forward Once: An Efficient Compositional Judging Paradigm](https://arxiv.org/abs/2511.16600)
*Tianlong Zhang,Hongwei Xue,Shilin Yan,Di Wu,Chen Xu,Yunyun Yang*

Main category: cs.AI

TL;DR: YOFO is a template-conditioned method that enables multimodal large language models to judge all requirements in a single forward pass, achieving orders-of-magnitude speedups while maintaining interpretability and state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM judging approaches face a fundamental trade-off: single-score adaptation misaligns with generative nature and limits fine-grained understanding, while autoregressive generation is too slow for high-throughput settings.

Method: YOFO uses a template-conditioned approach where an autoregressive model accepts structured requirement templates and produces binary yes/no decisions for each requirement in a single forward pass by reading the logits of the final token associated with each requirement.

Result: Extensive experiments show YOFO achieves state-of-the-art results on standard recommendation datasets, supports dependency-aware analysis, and benefits from post-hoc Chain of Thought reasoning.

Conclusion: YOFO successfully resolves the speed-accuracy trade-off in MLLM judging by enabling efficient single-pass requirement verification while preserving interpretability and supporting advanced analytical features.

Abstract: Multimodal large language models (MLLMs) show strong potential as judges. However, existing approaches face a fundamental trade-off: adapting MLLMs to output a single score misaligns with the generative nature of MLLMs and limits fine-grained requirement understanding, whereas autoregressively generating judging analyses is prohibitively slow in high-throughput settings. Observing that judgment reduces to verifying whether inputs satisfy a set of structured requirements, we propose YOFO, a template-conditioned method that judges all requirements in a single forward pass. Built on an autoregressive model, YOFO accepts a structured requirement template and, in one inference step, produces a binary yes/no decision for each requirement by reading the logits of the final token associated with that requirement. This design yields orders-of-magnitude speedups while preserving interpretability. Extensive experiments show that YOFO not only achieves state-of-the-art results on standard recommendation datasets, but also supports dependency-aware analysis-where subsequent judgments are conditioned on previous ones-and further benefits from post-hoc CoT.

</details>


### [76] [Bridging VLMs and Embodied Intelligence with Deliberate Practice Policy Optimization](https://arxiv.org/abs/2511.16602)
*Yi Zhang,Che Liu,Xiancong Ren,Hanchu Ni,Yingji Zhang,Shuai Zhang,Zeyuan Ding,Jiayu Hu,Haozhe Shan,Junbo Qi,Yan Bai,Dengjie Li,Jiachen Luo,Yidong Wang,Yong Dai,Zenglin Xu,Bin Shen,Qifan Wang,Jian Tang,Xiaozhu Ju*

Main category: cs.AI

TL;DR: DPPO is a metacognitive training framework that alternates between supervised fine-tuning and reinforcement learning to maximize learning efficiency from sparse data, achieving significant performance improvements in embodied intelligence systems.


<details>
  <summary>Details</summary>
Motivation: To overcome the embodied data bottleneck (scarce real-world data) and algorithmic inefficiency of existing methods that are resource-prohibitive for developing universal embodied intelligence systems.

Method: Deliberate Practice Policy Optimization (DPPO) - a metacognitive "Metaloop" framework that dynamically alternates between supervised fine-tuning (competence expansion) and reinforcement learning (skill refinement) to automatically identify weaknesses and allocate resources efficiently.

Result: Training Pelican-VL 1.0 with DPPO achieved 20.3% performance improvement over base model and surpassed open-source models at 100B-parameter scale by 10.6%. Models and code are open-sourced.

Conclusion: DPPO provides the first systematic framework that alleviates data and resource bottlenecks, enabling efficient development of versatile embodied agents and advancing embodied intelligence research.

Abstract: Developing a universal and versatile embodied intelligence system presents two primary challenges: the critical embodied data bottleneck, where real-world data is scarce and expensive, and the algorithmic inefficiency of existing methods, which are resource-prohibitive. To address these limitations, we introduce Deliberate Practice Policy Optimization (DPPO), a metacognitive ``Metaloop'' training framework that dynamically alternates between supervised fine-tuning (competence expansion) and reinforcement learning (skill refinement). This enables automatic weakness identification and targeted resource allocation, specifically designed to maximize learning efficiency from sparse, finite data. Theoretically, DPPO can be formalised as a unified preference-learning framework. Empirically, training a vision-language embodied model with DPPO, referred to as Pelican-VL 1.0, yields a 20.3% performance improvement over the base model and surpasses open-source models at the 100B-parameter scale by 10.6%. We are open-sourcing both the models and code, providing the first systematic framework that alleviates the data and resource bottleneck and enables the community to build versatile embodied agents efficiently.

</details>


### [77] [MedBayes-Lite: Bayesian Uncertainty Quantification for Safe Clinical Decision Support](https://arxiv.org/abs/2511.16625)
*Elias Hossain,Md Mehedi Hasan Nipu,Maleeha Sheikh,Rajib Rana,Subash Neupane,Niloofar Yousefi*

Main category: cs.AI

TL;DR: MedBayes-Lite is a lightweight Bayesian enhancement for clinical transformers that adds uncertainty quantification without retraining, improving calibration and reducing overconfidence by 32-48% while preventing 41% of diagnostic errors.


<details>
  <summary>Details</summary>
Motivation: Transformers show strong potential for clinical decision support but remain prone to overconfidence, especially in ambiguous medical cases where calibrated uncertainty is critical for reliable predictions.

Method: The framework integrates three components: Bayesian Embedding Calibration using Monte Carlo dropout for epistemic uncertainty, Uncertainty-Weighted Attention that marginalizes over token reliability, and Confidence-Guided Decision Shaping inspired by clinical risk minimization.

Result: Across biomedical QA and clinical prediction benchmarks (MedQA, PubMedQA, MIMIC-III), MedBayes-Lite consistently improves calibration and trustworthiness, reducing overconfidence by 32 to 48 percent. In simulated clinical settings, it can prevent up to 41 percent of diagnostic errors by flagging uncertain predictions for human review.

Conclusion: MedBayes-Lite effectively enables reliable uncertainty propagation and improves interpretability in medical AI systems without requiring retraining or significant architectural changes.

Abstract: We propose MedBayes-Lite, a lightweight Bayesian enhancement for transformer-based clinical language models designed to produce reliable, uncertainty-aware predictions. Although transformers show strong potential for clinical decision support, they remain prone to overconfidence, especially in ambiguous medical cases where calibrated uncertainty is critical. MedBayes-Lite embeds uncertainty quantification directly into existing transformer pipelines without any retraining or architectural rewiring, adding no new trainable layers and keeping parameter overhead under 3 percent. The framework integrates three components: (i) Bayesian Embedding Calibration using Monte Carlo dropout for epistemic uncertainty, (ii) Uncertainty-Weighted Attention that marginalizes over token reliability, and (iii) Confidence-Guided Decision Shaping inspired by clinical risk minimization. Across biomedical QA and clinical prediction benchmarks (MedQA, PubMedQA, MIMIC-III), MedBayes-Lite consistently improves calibration and trustworthiness, reducing overconfidence by 32 to 48 percent. In simulated clinical settings, it can prevent up to 41 percent of diagnostic errors by flagging uncertain predictions for human review. These results demonstrate its effectiveness in enabling reliable uncertainty propagation and improving interpretability in medical AI systems.

</details>


### [78] [Enhancing Forex Forecasting Accuracy: The Impact of Hybrid Variable Sets in Cognitive Algorithmic Trading Systems](https://arxiv.org/abs/2511.16657)
*Juan C. King,Jose M. Amigo*

Main category: cs.AI

TL;DR: Implementation of an AI-based algorithmic trading system for EUR-USD Forex trading using both fundamental and technical analysis features, with comparative evaluation of which feature type provides better predictive capacity.


<details>
  <summary>Details</summary>
Motivation: To develop an advanced AI trading system for high-frequency Forex trading that integrates both fundamental macroeconomic variables and technical indicators to generate profitable trading signals for the EUR-USD pair.

Method: Integration of fundamental macroeconomic variables (GDP, Unemployment Rate from Euro Zone and US) with technical indicators (oscillators, Fibonacci levels, price divergences) in an AI algorithmic trading system, evaluated using machine learning metrics and backtesting simulations on historical data.

Result: The system's performance was evaluated using standard machine learning metrics for predictive accuracy and backtesting simulations to assess trading profitability and risk.

Conclusion: Comparative analysis determines which class of input features (fundamental or technical) provides greater and more reliable predictive capacity for generating profitable trading signals in the EUR-USD Forex market.

Abstract: This paper presents the implementation of an advanced artificial intelligence-based algorithmic trading system specifically designed for the EUR-USD pair within the high-frequency environment of the Forex market. The methodological approach centers on integrating a holistic set of input features: key fundamental macroeconomic variables (for example, Gross Domestic Product and Unemployment Rate) collected from both the Euro Zone and the United States, alongside a comprehensive suite of technical variables (including indicators, oscillators, Fibonacci levels, and price divergences). The performance of the resulting algorithm is evaluated using standard machine learning metrics to quantify predictive accuracy and backtesting simulations across historical data to assess trading profitability and risk. The study concludes with a comparative analysis to determine which class of input features, fundamental or technical, provides greater and more reliable predictive capacity for generating profitable trading signals.

</details>


### [79] [Cognitive Foundations for Reasoning and Their Manifestation in LLMs](https://arxiv.org/abs/2511.16660)
*Priyanka Kargupta,Shuyue Stella Li,Haocheng Wang,Jinu Lee,Shan Chen,Orevaoghene Ahia,Dean Light,Thomas L. Griffiths,Max Kleiman-Weiner,Jiawei Han,Asli Celikyilmaz,Yulia Tsvetkov*

Main category: cs.AI

TL;DR: This paper analyzes differences between human and AI reasoning by creating a cognitive taxonomy, evaluating 170K reasoning traces across 17 models, and revealing systematic structural differences. Humans use hierarchical nesting and meta-cognition while models rely on shallow forward chaining. The research develops test-time reasoning guidance that improves performance by up to 60% on complex problems.


<details>
  <summary>Details</summary>
Motivation: To understand why large language models solve complex problems but fail on simpler variants, suggesting they use different mechanisms than human reasoning. The goal is to bridge cognitive science and LLM research to develop models that reason through principled cognitive mechanisms rather than brittle shortcuts.

Method: Created a taxonomy of 28 cognitive elements spanning computational constraints, meta-cognitive controls, knowledge representations, and transformation operations. Analyzed 170K reasoning traces from 17 models across text, vision, and audio modalities, plus 54 human think-aloud traces. Conducted meta-analysis of 1,598 LLM reasoning papers.

Result: Revealed systematic structural differences: humans use hierarchical nesting and meta-cognitive monitoring while models rely on shallow forward chaining, especially on ill-structured problems. Research community focuses on easily quantifiable behaviors while neglecting meta-cognitive controls that correlate with success. Models have behavioral repertoires for success but don't deploy them spontaneously.

Conclusion: Developed test-time reasoning guidance that automatically scaffolds successful structures, improving performance by up to 60% on complex problems. Established a foundation for developing models that reason through principled cognitive mechanisms rather than brittle spurious reasoning shortcuts, opening new directions for improving model capabilities and testing human cognition theories at scale.

Abstract: Large language models solve complex problems yet fail on simpler variants, suggesting they achieve correct outputs through mechanisms fundamentally different from human reasoning. We synthesize cognitive science research into a taxonomy of 28 cognitive elements spanning computational constraints, meta-cognitive controls, knowledge representations, and transformation operations, then analyze their behavioral manifestations in reasoning traces. We propose a fine-grained cognitive evaluation framework and conduct the first large-scale analysis of 170K traces from 17 models across text, vision, and audio modalities, alongside 54 human think-aloud traces, which we make publicly available. Our analysis reveals systematic structural differences: humans employ hierarchical nesting and meta-cognitive monitoring while models rely on shallow forward chaining, with divergence most pronounced on ill-structured problems. Meta-analysis of 1,598 LLM reasoning papers reveals the research community concentrates on easily quantifiable behaviors (sequential organization: 55%, decomposition: 60%) while neglecting meta-cognitive controls (self-awareness: 16%, evaluation: 8%) that correlate with success. Models possess behavioral repertoires associated with success but fail to deploy them spontaneously. Leveraging these patterns, we develop test-time reasoning guidance that automatically scaffold successful structures, improving performance by up to 60% on complex problems. By bridging cognitive science and LLM research, we establish a foundation for developing models that reason through principled cognitive mechanisms rather than brittle spurious reasoning shortcuts or memorization, opening new directions for both improving model capabilities and testing theories of human cognition at scale.

</details>
