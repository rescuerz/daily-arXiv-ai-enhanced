<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 72]
- [cs.AI](#cs.AI) [Total: 32]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Data Analysis and Performance Evaluation of Simulation Deduction Based on LLMs](https://arxiv.org/abs/2511.10651)
*Shansi Zhang,Min Li*

Main category: cs.CL

TL;DR: The paper proposes a method using large language models (LLMs) to automate military simulation analysis by decomposing complex tasks into sub-tasks with specialized prompts, multi-round interactions with self-checking, and custom tools for visualization and metrics.


<details>
  <summary>Details</summary>
Motivation: Traditional manual analysis of military simulation data is time-consuming and error-prone, while single-instruction LLM approaches fail to produce high-quality, well-structured analysis reports.

Method: Decompose complex analysis tasks into sub-tasks with tailored prompts, conduct multi-round LLM interactions with self-check and reflection, define custom tools for figure generation and metric computation, and design adaptable report templates for different scenarios.

Result: Extensive evaluation shows the generated reports exhibit higher quality and obtain higher scores compared to baseline methods.

Conclusion: The proposed method effectively automates military simulation analysis by leveraging LLMs' analytical capabilities through structured task decomposition, multi-round interactions, and custom tools, producing superior quality reports.

Abstract: Data analysis and performance evaluation of simulation deduction plays a pivotal role in modern warfare, which enables military personnel to gain invaluable insights into the potential effectiveness of different strategies, tactics, and operational plans. Traditional manual analysis approach is time-consuming and limited by human errors. To enhance efficiency and accuracy, large language models (LLMs) with strong analytical and inferencing capabilities can be employed. However, high-quality analysis reports with well-structured formatting cannot be obtained through a single instruction input to the LLM. To tackle this issue, we propose a method that first decomposes the complex task into several sub-tasks and designs effective system prompts and user prompts for each sub-task. Multi-round interactions with the LLM incorporating self-check and reflection are then conducted to enable structured data extraction as well as multi-step analysis and evaluation. Furthermore, custom tools are defined and invoked to generate figures and compute metrics. We also design multiple report templates, each tailored to a specific application and input data type, ensuring their adaptability across a variety of scenarios. Extensive evaluation results demonstrate that the reports generated by our method exhibit higher quality, therefore obtaining higher scores than the baseline method.

</details>


### [2] [Cognitively-Inspired Episodic Memory Architectures for Accurate and Efficient Character AI](https://arxiv.org/abs/2511.10652)
*Rafael Arias Gonzalez,Steve DiPaola*

Main category: cs.CL

TL;DR: An efficient architecture for historical character dialogue systems using offline data augmentation and parallel retrieval from structured episodic memory, achieving depth without latency trade-offs.


<details>
  <summary>Details</summary>
Motivation: To resolve the tension between shallow responses from simple retrieval-augmented generation and high latency from multi-stage reflection approaches in historical character dialogue systems.

Method: Transform biographical data into enriched first-person memories with affective-semantic metadata, then employ two-stage retrieval with parallel processing for efficient prompt generation.

Result: Achieves 0.52s prompt generation latency, parity with traditional RAG on GPT-4, and significant outperformance on smaller models (GPT-3.5, GPT-3) while enabling novel visualization tools.

Conclusion: The architecture provides a practical framework for educational, museum, and research applications requiring both accuracy and efficiency, generalizable to any historical figure with substantial textual records.

Abstract: Large language models show promise for embodying historical characters in dialogue systems, but existing approaches face a critical trade-off: simple retrieval-augmented generation produces shallow responses, while multi-stage reflection achieves depth at prohibitive latency. We present an architecture that resolves this tension through offline data augmentation and efficient parallel retrieval from structured episodic memory. Our system transforms biographical data into 1,774 enriched first-person memories with affective-semantic metadata, then employs two-stage retrieval achieving 0.52s prompt generation. Evaluation using LLM-as-judge and RAGAs metrics shows our approach achieves parity with traditional RAG on GPT-4 while significantly outperforming it on smaller models (GPT-3.5, GPT-3), suggesting particular value for resource-constrained deployments. Beyond dialogue, the structured memory enables novel visualization tools: spatiotemporal heatmaps, emotional trajectory analysis, and interactive path tracking, positioning the system as both a dialogue interface and research tool for biographical analysis. We use Van Gogh as a test case, but the architecture is generalizable to any historical figure with substantial textual records, offering a practical framework for educational, museum, and research applications requiring both accuracy and efficiency

</details>


### [3] [Hybrid Quantum Transformer for Language Generation](https://arxiv.org/abs/2511.10653)
*Desheng Kong,Xiangshuo Cui,Jiaying Jin,Jing Xu,Donglin Wang*

Main category: cs.CL

TL;DR: HyQuT is the first hybrid quantum-classical LLM for natural language generation, integrating variational quantum circuits into Transformer architecture at 8M and 150M parameter scales, achieving comparable performance with minimal quantum resources.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the feasibility of applying quantum computing to large-scale natural language generation tasks, which has not been successfully achieved before despite increasing quantum computing applications.

Method: Integration of variational quantum circuits (VQCs) into the Transformer framework, using minimal quantum resources (10 qubits with 80 quantum gates) to replace about 10% of classical parameters in 150M-parameter model.

Result: Achieved comparable convergence stability and generation quality to classical models while successfully performing coherent and context-aware dialogue tasks.

Conclusion: This study provides an early but successful demonstration that quantum computing can be integrated into large-scale generative language models, opening possibilities for quantum-enhanced natural language processing.

Abstract: Although quantum computing has been increasingly applied to replace classical computation, most existing quantum or hybrid models remain confined to simple tasks, with no successful application to large-scale natural language generation to date. In this work, we present the first hybrid quantum-classical large language model (LLM) for natural language generation, HyQuT, capable of performing coherent and context-aware dialogue. The proposed architecture integrates variational quantum circuits (VQCs) into the Transformer framework at both 8M and 150M parameter scales. Experimental results show that a minimal number of qubits (10 qubits with 80 quantum gates) can replace about 10% of the classical parameters in the 150M-parameter model, while achieving comparable convergence stability and generation quality. This study provides an early demonstration of the feasibility of integrating quantum computing to large-scale generative language models.

</details>


### [4] [Empirical Characterization of Temporal Constraint Processing in LLMs](https://arxiv.org/abs/2511.10654)
*Javier Marín*

Main category: cs.CL

TL;DR: LLMs fail to reliably process temporal constraints for real-time decisions, showing systematic risks like bimodal performance, extreme prompt brittleness, and action bias, with no correlation to parameter count. Fine-tuning provides limited improvement, but architectural changes are needed for reliable temporal reasoning.


<details>
  <summary>Details</summary>
Motivation: To test the assumption that LLMs can reliably determine whether action windows remain open or have closed in real-time decision scenarios under temporal constraints, which is crucial for deploying LLMs in agentic architectures.

Method: Evaluated eight production-scale models (2.8-8B parameters) using deadline detection tasks, analyzed performance patterns, tested prompt brittleness, and conducted fine-tuning experiments with 200 synthetic examples.

Result: Revealed systematic deployment risks: bimodal performance distribution (95% vs 50% accuracy), extreme prompt brittleness (30-60 percentage point swings), systematic action bias (100% false positive rates in failing models), and no correlation between parameter count and capability. Fine-tuning improved partially capable models by 12-37 percentage points.

Conclusion: Temporal constraint satisfaction cannot be reliably learned through next-token prediction on natural language alone. Current autoregressive architectures lack necessary mechanisms for continuous temporal state representation, explicit constraint checking, and systematic compositional reasoning over temporal relations, making deployment in time-critical applications risky without hybrid symbolic reasoning architectures.

Abstract: When deploying LLMs in agentic architectures requiring real-time decisions under temporal constraints, we assume they reliably determine whether action windows remain open or have closed. This assumption is untested. We characterize temporal constraint processing across eight production-scale models (2.8-8B parameters) using deadline detection tasks, revealing systematic deployment risks: bimodal performance distribution (models achieve either 95% or 50% accuracy), extreme prompt brittleness (30-60 percentage point swings from formatting changes alone), and systematic action bias (100% false positive rates in failing models). Parameter count shows no correlation with capability in this range-a 3.8B model matches 7B models while other 7B models fail completely. Fine-tuning on 200 synthetic examples improves models with partial capability by 12-37 percentage points. We demonstrate that temporal constraint satisfaction cannot be reliably learned through next-token prediction on natural language, even with targeted fine-tuning. This capability requires architectural mechanisms for: (1) continuous temporal state representation, (2) explicit constraint checking separate from linguistic pattern matching, (3) systematic compositional reasoning over temporal relations. Current autoregressive architectures lack these mechanisms. Deploying such systems in time-critical applications without hybrid architectures incorporating symbolic reasoning modules represents unacceptable risk.

</details>


### [5] [Spectral Neuro-Symbolic Reasoning II: Semantic Node Merging, Entailment Filtering, and Knowledge Graph Alignment](https://arxiv.org/abs/2511.10655)
*Andrew Kiruluta,Priscilla Burity*

Main category: cs.CL

TL;DR: This paper enhances the Spectral Neuro-Symbolic Reasoning framework with three semantic preprocessing improvements: transformer-based node merging, sentence-level entailment validation, and knowledge graph alignment to improve graph quality before spectral reasoning.


<details>
  <summary>Details</summary>
Motivation: To improve the fidelity and robustness of neuro-symbolic reasoning by addressing redundancy, edge quality, and missing context issues in the graph construction phase, enabling more accurate and scalable reasoning without modifying the core spectral inference engine.

Method: Three semantic preprocessing enhancements: (1) transformer-based node merging using contextual embeddings (Sentence-BERT, SimCSE) to reduce redundancy, (2) sentence-level entailment validation with pretrained NLI classifiers (RoBERTa, DeBERTa) to improve edge quality, and (3) alignment with external knowledge graphs (ConceptNet, Wikidata) to augment missing context.

Result: Experimental results on ProofWriter, EntailmentBank, and CLUTRR benchmarks show consistent accuracy gains (up to +3.8%), improved generalization to adversarial cases, and reduced inference noise.

Conclusion: The work successfully extends Spectral NSR with modular semantic preprocessing that enhances graph quality without altering the core spectral reasoning engine, resulting in a more robust, interpretable, and scalable reasoning system suitable for real-world deployment.

Abstract: This report extends the Spectral Neuro-Symbolic Reasoning (Spectral NSR) framework by introducing three semantically grounded enhancements: (1) transformer-based node merging using contextual embeddings (e.g., Sentence-BERT, SimCSE) to reduce redundancy, (2) sentence-level entailment validation with pretrained NLI classifiers (e.g., RoBERTa, DeBERTa) to improve edge quality, and (3) alignment with external knowledge graphs (e.g., ConceptNet, Wikidata) to augment missing context. These modifications enhance graph fidelity while preserving the core spectral reasoning pipeline. Experimental results on ProofWriter, EntailmentBank, and CLUTRR benchmarks show consistent accuracy gains (up to +3.8\%), improved generalization to adversarial cases, and reduced inference noise. The novelty lies in performing semantic and symbolic refinement entirely upstream of the spectral inference stage, enabling efficient, interpretable, and scalable reasoning without relying on quadratic attention mechanisms. In summary, this work extends the Spectral NSR framework with modular, semantically grounded preprocessing steps that improve graph quality without altering the core spectral reasoning engine. The result is a more robust, interpretable, and scalable reasoning system suitable for deployment in open-domain and real-world settings.

</details>


### [6] [Preference Orchestrator: Prompt-Aware Multi-Objective Alignment for Large Language Models](https://arxiv.org/abs/2511.10656)
*Biao Liu,Ning Xu,Junming Yang,Xin Geng*

Main category: cs.CL

TL;DR: PRO (PReference Orchestrator) is a novel framework that uses a lightweight preference adapter to automatically infer prompt-specific preference weights for multi-objective alignment of LLMs, eliminating manual weight specification and improving training efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing multi-objective alignment methods require manual preference weight specification, which burdens users and leads to suboptimal training efficiency due to exploration of irrelevant preference combinations.

Method: PRO features a lightweight preference adapter that automatically learns appropriate preference weights for each prompt by training on normalized reward scores from multiple reward models for preferred responses, reflecting effective preference balances across objectives.

Result: Extensive experiments across multiple tasks demonstrate the effectiveness of PRO over existing multi-objective alignment approaches.

Conclusion: PRO's prompt-aware preference mechanism achieves superior performance compared to fixed preference weights in multi-objective alignment scenarios, as proven by theoretical analysis.

Abstract: While Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks, aligning these models with varying human preferences across multiple objectives remains a significant challenge in practical deployments. Existing multi-objective alignment methods rely on manually specified preference weights, which not only burden users with difficult preference specification tasks but also lead to suboptimal training efficiency due to exploration of irrelevant preference combinations. To alleviate these issues, we propose a novel framework named PRO, i.e., PReference Orchestrator, which features a lightweight preference adapter that automatically infers prompt-specific preference weights during both training and deployment phases. Specifically, the adapter automatically learns appropriate preference weights for each prompt by training on normalized reward scores from multiple reward models for preferred responses, which inherently reflect effective preference balances across objectives. Additionally, We provide theoretical analysis proving that our prompt-aware preference mechanism achieves superior performance compared to fixed preference weights in multi-objective alignment scenarios. Extensive experiments across multiple tasks demonstrate the effectiveness of our method over existing multi-objective alignment approaches.

</details>


### [7] [Patent Representation Learning via Self-supervision](https://arxiv.org/abs/2511.10657)
*You Zuo,Kim Gerdes,Eric Villemonte de La Clergerie,Benoît Sagot*

Main category: cs.CL

TL;DR: A contrastive learning framework for patent embeddings using multiple document sections as complementary views, overcoming limitations of dropout augmentation and achieving state-of-the-art performance without external annotations.


<details>
  <summary>Details</summary>
Motivation: Address the failure mode of SimCSE-style dropout augmentation in patents, which produces overly uniform embeddings that lose semantic cohesion, by leveraging the natural semantic diversity within patent documents.

Method: Propose section-based augmentation where different patent sections (abstract, claims, background) serve as complementary views in contrastive learning, introducing natural semantic and structural diversity to mitigate over-dispersion.

Result: The fully self-supervised method matches or surpasses citation- and IPC-supervised baselines in prior-art retrieval and classification on large-scale benchmarks, while avoiding reliance on brittle or incomplete annotations.

Conclusion: Different patent sections specialize for different tasks (claims/summaries for retrieval, background for classification), highlighting the value of exploiting intra-document views for scalable and generalizable patent understanding.

Abstract: This paper presents a simple yet effective contrastive learning framework for learning patent embeddings by leveraging multiple views from within the same document. We first identify a patent-specific failure mode of SimCSE style dropout augmentation: it produces overly uniform embeddings that lose semantic cohesion. To remedy this, we propose section-based augmentation, where different sections of a patent (e.g., abstract, claims, background) serve as complementary views. This design introduces natural semantic and structural diversity, mitigating over-dispersion and yielding embeddings that better preserve both global structure and local continuity. On large-scale benchmarks, our fully self-supervised method matches or surpasses citation-and IPC-supervised baselines in prior-art retrieval and classification, while avoiding reliance on brittle or incomplete annotations. Our analysis further shows that different sections specialize for different tasks-claims and summaries benefit retrieval, while background sections aid classification-highlighting the value of patents' inherent discourse structure for representation learning. These results highlight the value of exploiting intra-document views for scalable and generalizable patent understanding.

</details>


### [8] [Evaluating Open-Weight Large Language Models for Structured Data Extraction from Narrative Medical Reports Across Multiple Use Cases and Languages](https://arxiv.org/abs/2511.10658)
*Douwe J. Spaanderman,Karthik Prathaban,Petr Zelina,Kaouther Mouheb,Lukáš Hejtmánek,Matthew Marzetti,Antonius W. Schurink,Damian Chan,Ruben Niemantsverdriet,Frederik Hartmann,Zhen Qian,Maarten G. J. Thomeer,Petr Holub,Farhan Akram,Frank J. Wolters,Meike W. Vernooij,Cornelis Verhoef,Esther E. Bron,Vít Nováček,Dirk J. Grünhagen,Wiro J. Niessen,Martijn P. A. Starmans,Stefan Klein*

Main category: cs.CL

TL;DR: Evaluation of 15 open-weight LLMs for extracting structured information from clinical pathology and radiology reports across multiple diseases, languages, and institutions, comparing different prompting strategies and model sizes.


<details>
  <summary>Details</summary>
Motivation: To assess the capability of open-weight LLMs in extracting structured data from clinical free-text records across diverse medical domains, languages, and healthcare institutions, addressing limitations of prior work that focused on single tasks and English-only reports.

Method: Evaluated 15 open-weight LLMs (general-purpose and medical-specialized) on pathology and radiology reports across six medical use cases in three countries, comparing six prompting strategies: zero-shot, one-shot, few-shot, chain-of-thought, self-consistency, and prompt graph.

Result: Top-performing models achieved macro-average scores close to inter-rater agreement across tasks. Small-to-medium general-purpose models performed comparably to large models, while tiny and specialized models performed worse. Prompt graph and few-shot prompting improved performance by approximately 13%. Task-specific factors had greater influence than model size or prompting strategy.

Conclusion: Open-weight LLMs can effectively extract structured data from clinical reports across diverse diseases, languages, and institutions, providing a scalable solution for clinical data curation.

Abstract: Large language models (LLMs) are increasingly used to extract structured information from free-text clinical records, but prior work often focuses on single tasks, limited models, and English-language reports. We evaluated 15 open-weight LLMs on pathology and radiology reports across six use cases, colorectal liver metastases, liver tumours, neurodegenerative diseases, soft-tissue tumours, melanomas, and sarcomas, at three institutes in the Netherlands, UK, and Czech Republic. Models included general-purpose and medical-specialised LLMs of various sizes, and six prompting strategies were compared: zero-shot, one-shot, few-shot, chain-of-thought, self-consistency, and prompt graph. Performance was assessed using task-appropriate metrics, with consensus rank aggregation and linear mixed-effects models quantifying variance. Top-ranked models achieved macro-average scores close to inter-rater agreement across tasks. Small-to-medium general-purpose models performed comparably to large models, while tiny and specialised models performed worse. Prompt graph and few-shot prompting improved performance by ~13%. Task-specific factors, including variable complexity and annotation variability, influenced results more than model size or prompting strategy. These findings show that open-weight LLMs can extract structured data from clinical reports across diseases, languages, and institutions, offering a scalable approach for clinical data curation.

</details>


### [9] [Information Extraction From Fiscal Documents Using LLMs](https://arxiv.org/abs/2511.10659)
*Vikram Aggarwal,Jay Kulkarni,Aditi Mascarenhas,Aakriti Narang,Siddarth Raman,Ajay Shah,Susan Thomas*

Main category: cs.CL

TL;DR: LLM-based approach for extracting structured data from multi-page government fiscal documents with hierarchical validation using fiscal table totals.


<details>
  <summary>Details</summary>
Motivation: Large Language Models have strong text comprehension but their ability to process complex hierarchical tabular data from government fiscal documents remains underexplored.

Method: Multi-stage pipeline using LLM-based techniques with domain knowledge, sequential context, and algorithmic validation through hierarchical relationships in fiscal tables.

Result: Applied to 200+ page Karnataka fiscal documents, achieved high accuracy with robust internal validation using fiscal table totals at each hierarchy level.

Conclusion: LLMs can effectively read tables and process document-specific structural hierarchies, offering scalable conversion of PDF fiscal disclosures into research-ready databases with promise for developing countries.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in text comprehension, but their ability to process complex, hierarchical tabular data remains underexplored. We present a novel approach to extracting structured data from multi-page government fiscal documents using LLM-based techniques. Applied to annual fiscal documents from the State of Karnataka in India (200+ pages), our method achieves high accuracy through a multi-stage pipeline that leverages domain knowledge, sequential context, and algorithmic validation. A large challenge with traditional OCR methods is the inability to verify the accurate extraction of numbers. When applied to fiscal data, the inherent structure of fiscal tables, with totals at each level of the hierarchy, allows for robust internal validation of the extracted data. We use these hierarchical relationships to create multi-level validation checks. We demonstrate that LLMs can read tables and also process document-specific structural hierarchies, offering a scalable process for converting PDF-based fiscal disclosures into research-ready databases. Our implementation shows promise for broader applications across developing country contexts.

</details>


### [10] [Test-Time Steering for Lossless Text Compression via Weighted Product of Experts](https://arxiv.org/abs/2511.10660)
*Qihang Zhang,Muchen Li,Ziao Wang,Renjie Liao,Lele Wang*

Main category: cs.CL

TL;DR: A novel framework using Test-Time Steering via Weighted Product of Experts (wPoE) that adaptively combines universal compressors with neural language models to improve text compression without fine-tuning, ensuring compression rates at least as good as the best individual model.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional universal compressors (low compression rates) and neural compressors (poor generalization to unseen data), while leveraging the strengths of both approaches.

Method: Test-Time Steering via Weighted Product of Experts (wPoE) that adaptively combines a universal compression model with a pretrained neural language model during inference, without requiring fine-tuning.

Result: Extensive experiments show improved text compression performance, with compression rates at least as good as the best individual model, and seamless integration with any autoregressive language model.

Conclusion: The proposed framework provides a practical solution for enhancing text compression across diverse data distributions by effectively combining the advantages of both universal and neural compression approaches.

Abstract: Lossless compression techniques are crucial in an era of rapidly growing data. Traditional universal compressors like gzip offer low computational overhead, high speed, and broad applicability across data distributions. However, they often lead to worse compression rates than modern neural compressors, which leverage large-scale training data to model data distributions more effectively. Despite their advantages, neural compressors struggle to generalize to unseen data. To address this limitation, we propose a novel framework that performs Test-Time Steering via a Weighted Product of Experts (wPoE). At inference, our method adaptively combines a universal compression model with a pretrained neural language model, ensuring the compression rate is at least as good as that of the best individual model. Extensive experiments demonstrate that our approach improves the performance of text compression without requiring fine-tuning. Furthermore, it seamlessly integrates with any autoregressive language model, providing a practical solution for enhancing text compression across diverse data distributions.

</details>


### [11] [Bayesian Evaluation of Large Language Model Behavior](https://arxiv.org/abs/2511.10661)
*Rachel Longjohn,Shang Wu,Saatvik Kher,Catarina Belém,Padhraic Smyth*

Main category: cs.CL

TL;DR: A Bayesian approach for quantifying statistical uncertainty in binary evaluation metrics of LLM-based text generation systems, addressing uncertainty from probabilistic text generation strategies.


<details>
  <summary>Details</summary>
Motivation: Existing LLM evaluation approaches neglect statistical uncertainty quantification, particularly uncertainty induced by probabilistic text generation strategies used in LLM-based systems.

Method: Developed a Bayesian approach to quantify uncertainty in binary evaluation metrics, applied to two case studies: refusal rates on adversarial inputs and pairwise preferences in open-ended dialogue.

Result: The Bayesian approach successfully provides useful uncertainty quantification about LLM behavior, demonstrating its application in evaluating refusal rates and pairwise preferences.

Conclusion: Bayesian uncertainty quantification is valuable for understanding the statistical reliability of LLM evaluations and provides important insights into LLM behavior beyond simple binary metrics.

Abstract: It is increasingly important to evaluate how text generation systems based on large language models (LLMs) behave, such as their tendency to produce harmful output or their sensitivity to adversarial inputs. Such evaluations often rely on a curated benchmark set of input prompts provided to the LLM, where the output for each prompt may be assessed in a binary fashion (e.g., harmful/non-harmful or does not leak/leaks sensitive information), and the aggregation of binary scores is used to evaluate the LLM. However, existing approaches to evaluation often neglect statistical uncertainty quantification. With an applied statistics audience in mind, we provide background on LLM text generation and evaluation, and then describe a Bayesian approach for quantifying uncertainty in binary evaluation metrics. We focus in particular on uncertainty that is induced by the probabilistic text generation strategies typically deployed in LLM-based systems. We present two case studies applying this approach: 1) evaluating refusal rates on a benchmark of adversarial inputs designed to elicit harmful responses, and 2) evaluating pairwise preferences of one LLM over another on a benchmark of open-ended interactive dialogue examples. We demonstrate how the Bayesian approach can provide useful uncertainty quantification about the behavior of LLM-based systems.

</details>


### [12] [Evaluating Modern Large Language Models on Low-Resource and Morphologically Rich Languages:A Cross-Lingual Benchmark Across Cantonese, Japanese, and Turkish](https://arxiv.org/abs/2511.10664)
*Chengxuan Xia,Qianye Wu,Hongbin Guan,Sixuan Tian,Yilun Hao,Xiaoyu Wu*

Main category: cs.CL

TL;DR: Comprehensive evaluation of 7 LLMs on Cantonese, Japanese, and Turkish across 4 tasks reveals proprietary models lead but struggle with cultural nuances and morphological complexity, while open-source models lag significantly.


<details>
  <summary>Details</summary>
Motivation: To assess LLM performance in low-resource and morphologically rich languages, which remain underexplored compared to high-resource languages like English.

Method: Created cross-lingual benchmark covering Cantonese, Japanese, and Turkish with 4 tasks (QA, summarization, translation, dialogue), combining human evaluations (fluency, factual accuracy, cultural appropriateness) with automated metrics (BLEU, ROUGE).

Result: Proprietary models (GPT-4o, GPT-4, Claude 3.5) lead across languages/tasks, but all models struggle with cultural nuances and morphological challenges. GPT-4o shows robust multilingual performance, Claude 3.5 achieves competitive reasoning accuracy. Smaller open-source models lag substantially in fluency and accuracy.

Conclusion: Significant gaps persist in culturally nuanced understanding and morphological generalization across all models, highlighting the need for more culturally aware and linguistically generalizable LLMs, especially for low-resource languages.

Abstract: Large language models (LLMs) have achieved impressive results in high-resource languages like English, yet their effectiveness in low-resource and morphologically rich languages remains underexplored. In this paper, we present a comprehensive evaluation of seven cutting-edge LLMs -- including GPT-4o, GPT-4, Claude~3.5~Sonnet, LLaMA~3.1, Mistral~Large~2, LLaMA-2~Chat~13B, and Mistral~7B~Instruct -- on a new cross-lingual benchmark covering \textbf{Cantonese, Japanese, and Turkish}. Our benchmark spans four diverse tasks: open-domain question answering, document summarization, English-to-X translation, and culturally grounded dialogue. We combine \textbf{human evaluations} (rating fluency, factual accuracy, and cultural appropriateness) with automated metrics (e.g., BLEU, ROUGE) to assess model performance.
  Our results reveal that while the largest proprietary models (GPT-4o, GPT-4, Claude~3.5) generally lead across languages and tasks, significant gaps persist in culturally nuanced understanding and morphological generalization. Notably, GPT-4o demonstrates robust multilingual performance even on cross-lingual tasks, and Claude~3.5~Sonnet achieves competitive accuracy on knowledge and reasoning benchmarks. However, all models struggle to some extent with the unique linguistic challenges of each language, such as Turkish agglutinative morphology and Cantonese colloquialisms. Smaller open-source models (LLaMA-2~13B, Mistral~7B) lag substantially in fluency and accuracy, highlighting the resource disparity. We provide detailed quantitative results, qualitative error analysis, and discuss implications for developing more culturally aware and linguistically generalizable LLMs. Our benchmark and evaluation data are released to foster reproducibility and further research.

</details>


### [13] [Guarding the Meaning: Self-Supervised Training for Semantic Robustness in Guard Models](https://arxiv.org/abs/2511.10665)
*Cristina Pinneri,Christos Louizos*

Main category: cs.CL

TL;DR: This paper addresses the vulnerability of LLM guard models to superficial linguistic variations by introducing a self-supervised framework that improves semantic robustness through paraphrase consistency training with skew-aware aggregation.


<details>
  <summary>Details</summary>
Motivation: Guard models for LLM safety are highly sensitive to meaning-preserving paraphrases, revealing a lack of semantic grounding and creating vulnerabilities in safety systems.

Method: A practical, self-supervised framework that leverages paraphrase sets to enforce prediction consistency using a novel, skew-aware aggregation strategy for robust target computation.

Result: The approach reduces semantic variability across paraphrases by ~58%, improves benchmark accuracy by ~2.5% on average, generalizes to unseen stylistic variations, and improves model calibration by up to 40%.

Conclusion: Semantic consistency should be treated as a first-class training objective, and the proposed method provides a scalable recipe for building more reliable guard models, revealing a bidirectional relationship between model calibration and consistency.

Abstract: Guard models are a critical component of LLM safety, but their sensitivity to superficial linguistic variations remains a key vulnerability. We show that even meaning-preserving paraphrases can cause large fluctuations in safety scores, revealing a lack of semantic grounding. To address this, we introduce a practical, self-supervised framework for improving the semantic robustness of guard models. Our method leverages paraphrase sets to enforce prediction consistency using a novel, skew-aware aggregation strategy for robust target computation. Notably, we find that standard aggregation methods like mean and median can degrade safety, underscoring the need for skew-aware alternatives. We analyze six open-source guard models and show that our approach reduces semantic variability across paraphrases by ~58%, improves benchmark accuracy by ~2.5% on average, and generalizes to unseen stylistic variations. Intriguingly, we discover a bidirectional relationship between model calibration and consistency: our robustness training improves calibration by up to 40%, revealing a fundamental connection between these properties. These results highlight the value of treating semantic consistency as a first-class training objective and provide a scalable recipe for building more reliable guard models.

</details>


### [14] [Evaluating LLM Understanding via Structured Tabular Decision Simulations](https://arxiv.org/abs/2511.10667)
*Sichao Li,Xinyue Xu,Xiaomeng Li*

Main category: cs.CL

TL;DR: STaDS framework evaluates LLM understanding through structured decision simulations, finding models often lack consistent accuracy and show mismatches between rationales and actual decision factors.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLMs achieve genuine understanding beyond mere predictive accuracy, by evaluating their ability to make consistent, well-founded decisions across multiple domains using relevant decision factors.

Method: Introduces Structured Tabular Decision Simulations (STaDS) - a suite of expert-like decision settings that evaluate LLMs through question comprehension, knowledge-based prediction, and reliance on relevant decision factors across 15 diverse domains.

Result: Analysis of 9 frontier LLMs shows most struggle with consistent accuracy across domains, and models can be accurate yet globally unfaithful with frequent mismatches between stated rationales and actual decision factors driving predictions.

Conclusion: Highlights the need for global-level understanding evaluation protocols and novel frameworks that go beyond accuracy to enhance LLMs' genuine understanding ability.

Abstract: Large language models (LLMs) often achieve impressive predictive accuracy, yet correctness alone does not imply genuine understanding. True LLM understanding, analogous to human expertise, requires making consistent, well-founded decisions across multiple instances and diverse domains, relying on relevant and domain-grounded decision factors. We introduce Structured Tabular Decision Simulations (STaDS), a suite of expert-like decision settings that evaluate LLMs as if they were professionals undertaking structured decision ``exams''. In this context, understanding is defined as the ability to identify and rely on the correct decision factors, features that determine outcomes within a domain. STaDS jointly assesses understanding through: (i) question and instruction comprehension, (ii) knowledge-based prediction, and (iii) reliance on relevant decision factors. By analyzing 9 frontier LLMs across 15 diverse decision settings, we find that (a) most models struggle to achieve consistently strong accuracy across diverse domains; (b) models can be accurate yet globally unfaithful, and there are frequent mismatches between stated rationales and factors driving predictions. Our findings highlight the need for global-level understanding evaluation protocols and advocate for novel frameworks that go beyond accuracy to enhance LLMs' understanding ability.

</details>


### [15] [Forecasting Spoken Language Development in Children with Cochlear Implants Using Preimplantation MRI](https://arxiv.org/abs/2511.10669)
*Yanlin Wang,Di Yuan,Shani Dettman,Dawn Choo,Emily Shimeng Xu,Denise Thomas,Maura E Ryan,Patrick C M Wong,Nancy M Young*

Main category: cs.CL

TL;DR: Deep transfer learning (DTL) with bilinear attention-based fusion outperforms traditional machine learning in predicting cochlear implant language outcomes in children, achieving 92.39% accuracy for classifying high vs low language improvers.


<details>
  <summary>Details</summary>
Motivation: Cochlear implant outcomes in children with severe hearing loss show high variability that cannot be reliably predicted by traditional factors like age at implantation or residual hearing, creating a need for better prediction methods.

Method: Used brain neuroanatomic features to compare traditional ML vs DTL algorithms for binary classification of high vs low language improvers among 278 implanted children from three centers, employing bilinear attention-based fusion strategy.

Result: DTL achieved 92.39% accuracy, 91.22% sensitivity, 93.56% specificity, and AUC of 0.977, significantly outperforming traditional ML models in all outcome measures.

Conclusion: DTL enables direct capture of discriminative and task-specific information through representation learning, supporting feasibility of a single DTL prediction model for global CI program language prediction.

Abstract: Cochlear implants (CI) significantly improve spoken language in children with severe-to-profound sensorineural hearing loss (SNHL), yet outcomes remain more variable than in children with normal hearing. This variability cannot be reliably predicted for individual children using age at implantation or residual hearing. This study aims to compare the accuracy of traditional machine learning (ML) to deep transfer learning (DTL) algorithms to predict post-CI spoken language development of children with bilateral SNHL using a binary classification model of high versus low language improvers. A total of 278 implanted children enrolled from three centers. The accuracy, sensitivity and specificity of prediction models based upon brain neuroanatomic features using traditional ML and DTL learning. DTL prediction models using bilinear attention-based fusion strategy achieved: accuracy of 92.39% (95% CI, 90.70%-94.07%), sensitivity of 91.22% (95% CI, 89.98%-92.47%), specificity of 93.56% (95% CI, 90.91%-96.21%), and area under the curve (AUC) of 0.977 (95% CI, 0.969-0.986). DTL outperformed traditional ML models in all outcome measures. DTL was significantly improved by direct capture of discriminative and task-specific information that are advantages of representation learning enabled by this approach over ML. The results support the feasibility of a single DTL prediction model for language prediction of children served by CI programs worldwide.

</details>


### [16] [Towards Fine-Grained Code-Switch Speech Translation with Semantic Space Alignment](https://arxiv.org/abs/2511.10670)
*Yan Gao,Yazheng Yang,Zhibin Lan,Yidong Chen,Min Zhang,Daimeng Wei,Hui Huang,Jinsong Su*

Main category: cs.CL

TL;DR: This paper proposes a novel approach for code-switching speech translation using Large Language Models enhanced with Mixture of Experts speech projectors and a multi-stage training paradigm to address data scarcity and semantic modeling challenges.


<details>
  <summary>Details</summary>
Motivation: Code-switching speech translation faces significant challenges due to complex semantic modeling requirements and the scarcity of annotated CS data. Previous methods rely on implicit learning and costly manual annotations, which are inefficient and limited.

Method: The approach enhances LLMs with Mixture of Experts speech projectors where each expert specializes in a specific language's semantic subspace. It uses multi-stage training with monolingual ASR/ST data, language-specific loss, intra-group load balancing loss, and transition loss for smooth data adaptation.

Result: Extensive experiments on widely used datasets demonstrate the effectiveness and generality of the proposed approach, showing improved performance in code-switching speech translation tasks.

Conclusion: The proposed method effectively addresses the challenges of code-switching speech translation by leveraging specialized experts, multi-stage training, and innovative loss functions, providing a robust solution for handling multilingual speech data with limited CS resources.

Abstract: Code-switching (CS) speech translation (ST) refers to translating speech that alternates between two or more languages into a target language text, which poses significant challenges due to the complexity of semantic modeling and the scarcity of CS data. Previous studies tend to rely on the model itself to implicitly learn semantic modeling during training, and resort to inefficient and costly manual annotations for these two challenges. To mitigate these limitations, we propose enhancing Large Language Models (LLMs) with a Mixture of Experts (MoE) speech projector, where each expert specializes in the semantic subspace of a specific language, enabling fine-grained modeling of speech features. Additionally, we introduce a multi-stage training paradigm that utilizes readily available monolingual automatic speech recognition (ASR) and monolingual ST data, facilitating speech-text alignment and improving translation capabilities. During training, we leverage a combination of language-specific loss and intra-group load balancing loss to guide the MoE speech projector in efficiently allocating tokens to the appropriate experts, across expert groups and within each group, respectively. To bridge the data gap across different training stages and improve adaptation to the CS scenario, we further employ a transition loss, enabling smooth transitions of data between stages, to effectively address the scarcity of high-quality CS speech translation data. Extensive experiments on widely used datasets demonstrate the effectiveness and generality of our approach.

</details>


### [17] [Grounded Visual Factualization: Factual Anchor-Based Finetuning for Enhancing MLLM Factual Consistency](https://arxiv.org/abs/2511.10671)
*Filippo Morbiato,Luca Romano,Alessandro Persona*

Main category: cs.CL

TL;DR: GVF Finetuning is a novel approach that systematically enhances MLLM visual factual consistency by integrating explicit factual signals through data augmentation, instruction tuning, and a specialized loss function, significantly reducing visual hallucinations while maintaining general multimodal performance.


<details>
  <summary>Details</summary>
Motivation: Visual hallucination in Multimodal Large Language Models critically undermines their reliability, and existing fine-tuning methods offer limited improvement by failing to deeply intervene in factual reasoning.

Method: GVF integrates three core mechanisms: Factual Anchor Data Augmentation (enriching training data with structured factual anchors and counter-factual prompts), Fact-Aware Instruction Tuning (embedding factual cues into explicit instructions), and a Factual Consistency Loss function (specifically penalizing factual inaccuracies).

Result: Evaluated on LLaVA-1.5-13B, GVF Finetuning significantly outperforms standard fine-tuning on the VHTest benchmark for both Open-Ended Question and Yes/No Question formats, while maintaining or slightly improving performance on general multimodal benchmarks like MME and POPE.

Conclusion: GVF demonstrates effective mitigation of visual hallucinations without compromising general understanding and reasoning abilities, providing a systematic approach to enhance MLLM visual factual consistency.

Abstract: Visual hallucination, where Multimodal Large Language Models fabricate details inconsistent with image content, critically undermines their reliability. Existing fine-tuning methods offer limited improvement, failing to deeply intervene in factual reasoning. This paper introduces Grounded Visual Factualization (GVF) Finetuning, a novel approach to systematically enhance MLLM visual factual consistency. GVF integrates explicit factual signals via three core mechanisms: Factual Anchor Data Augmentation, enriching training data with structured factual anchors and counter-factual prompts; Fact-Aware Instruction Tuning, embedding these cues into explicit instructions; and a Factual Consistency Loss function, specifically penalizing factual inaccuracies. Evaluated on LLaVA-1.5-13B, GVF Finetuning significantly outperforms standard fine-tuning on the VHTest benchmark for both Open-Ended Question (OEQ) and Yes/No Question (YNQ) formats. Crucially, GVF maintains or even slightly improves performance on general multimodal benchmarks like MME and POPE, demonstrating effective mitigation of visual hallucinations without compromising general understanding and reasoning abilities.

</details>


### [18] [Large language models in materials science and the need for open-source approaches](https://arxiv.org/abs/2511.10673)
*Fengxu Yang,Weitong Chen,Jack D. Evans*

Main category: cs.CL

TL;DR: This review examines LLM applications in materials science across literature mining, predictive modeling, and multi-agent experimental systems, advocating for open-source models over commercial alternatives.


<details>
  <summary>Details</summary>
Motivation: To explore how large language models are transforming materials science and demonstrate the advantages of open-source models over closed-source commercial alternatives in scientific applications.

Method: The review examines LLM applications across three key areas: mining scientific literature for information extraction, predictive modeling of structure-property relationships, and coordination of multi-agent experimental systems integrating computational tools and laboratory automation.

Result: Benchmark results show that open-source LLMs can match the performance of commercial models while offering greater transparency, reproducibility, cost-effectiveness, and data privacy.

Conclusion: As open-source models continue to improve, the authors advocate for their broader adoption to build accessible, flexible, and community-driven AI platforms for scientific discovery in materials science.

Abstract: Large language models (LLMs) are rapidly transforming materials science. This review examines recent LLM applications across the materials discovery pipeline, focusing on three key areas: mining scientific literature , predictive modelling, and multi-agent experimental systems. We highlight how LLMs extract valuable information such as synthesis conditions from text, learn structure-property relationships, and can coordinate agentic systems integrating computational tools and laboratory automation. While progress has been largely dependent on closed-source commercial models, our benchmark results demonstrate that open-source alternatives can match performance while offering greater transparency, reproducibility, cost-effectiveness, and data privacy. As open-source models continue to improve, we advocate their broader adoption to build accessible, flexible, and community-driven AI platforms for scientific discovery.

</details>


### [19] [Continual Learning of Domain Knowledge from Human Feedback in Text-to-SQL](https://arxiv.org/abs/2511.10674)
*Thomas Cook,Kelly Patel,Sivapriya Vellaichamy,Saba Rahimi,Zhen Zeng,Sumitra Ganesh*

Main category: cs.CL

TL;DR: A framework for continual learning in text-to-SQL that uses human feedback to refine queries and stores distilled knowledge in structured memory, enabling LLMs to improve SQL generation accuracy over time.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with database-specific schemas and tacit domain knowledge when generating SQL queries from natural language, requiring a way to continually learn from human feedback.

Method: Developed a framework with learning agents that receive natural language feedback to refine queries, distill knowledge for reuse, and store it in structured memory. Evaluated multiple agent architectures varying in how they capture and retrieve past experiences.

Result: Memory-augmented agents, especially the Procedural Agent, achieved significant accuracy gains and error reduction on the BIRD benchmark Dev set by leveraging human-in-the-loop feedback.

Conclusion: The approach successfully transforms tacit human expertise into reusable knowledge, enabling more adaptive, domain-aware text-to-SQL systems that continually learn from human feedback.

Abstract: Large Language Models (LLMs) can generate SQL queries from natural language questions but struggle with database-specific schemas and tacit domain knowledge. We introduce a framework for continual learning from human feedback in text-to-SQL, where a learning agent receives natural language feedback to refine queries and distills the revealed knowledge for reuse on future tasks. This distilled knowledge is stored in a structured memory, enabling the agent to improve execution accuracy over time. We design and evaluate multiple variations of a learning agent architecture that vary in how they capture and retrieve past experiences. Experiments on the BIRD benchmark Dev set show that memory-augmented agents, particularly the Procedural Agent, achieve significant accuracy gains and error reduction by leveraging human-in-the-loop feedback. Our results highlight the importance of transforming tacit human expertise into reusable knowledge, paving the way for more adaptive, domain-aware text-to-SQL systems that continually learn from a human-in-the-loop.

</details>


### [20] [Learn to Select: Exploring Label Distribution Divergence for In-Context Demonstration Selection in Text Classification](https://arxiv.org/abs/2511.10675)
*Ye Jiang,Taihang Wang,Youzheng Liu,Yimin Wang,Yuhan Xia,Yunfei Long*

Main category: cs.CL

TL;DR: A two-stage demonstration selection method (TopK + L2D) that combines semantic similarity with label distribution alignment for in-context learning in text classification, outperforming previous methods across seven benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing demonstration selection methods for in-context learning focus mainly on semantic similarity but overlook label distribution alignment, which can significantly affect LLM performance.

Method: Proposed TopK + Label Distribution Divergence (L2D) method using a fine-tuned BERT-like small language model to generate label distributions and calculate divergence between test inputs and candidate demonstrations.

Result: Extensive experiments on seven text classification benchmarks show consistent outperformance over previous demonstration selection strategies.

Conclusion: There's a positive correlation between LLM performance and the accuracy of the underlying small language models used for label distribution estimation.

Abstract: In-context learning (ICL) for text classification, which uses a few input-label demonstrations to describe a task, has demonstrated impressive performance on large language models (LLMs). However, the selection of in-context demonstrations plays a crucial role and can significantly affect LLMs' performance. Most existing demonstration selection methods primarily focus on semantic similarity between test inputs and demonstrations, often overlooking the importance of label distribution alignment. To address this limitation, we propose a two-stage demonstration selection method, TopK + Label Distribution Divergence (L2D), which leverages a fine-tuned BERT-like small language model (SLM) to generate label distributions and calculate their divergence for both test inputs and candidate demonstrations. This enables the selection of demonstrations that are not only semantically similar but also aligned in label distribution with the test input. Extensive experiments across seven text classification benchmarks show that our method consistently outperforms previous demonstration selection strategies. Further analysis reveals a positive correlation between the performance of LLMs and the accuracy of the underlying SLMs used for label distribution estimation.

</details>


### [21] [Pre-Attention Expert Prediction and Prefetching for Mixture-of-Experts Large Language Models](https://arxiv.org/abs/2511.10676)
*Shien Zhu,Samuel Bohl,Robin Oester,Gustavo Alonso*

Main category: cs.CL

TL;DR: Proposes pre-attention expert prediction for MoE LLMs using ranking-preserving functions with linear layers and ranking-aware loss, achieving ~15% higher accuracy than SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Existing MoE expert prediction methods use previous layer activations, resulting in low accuracy and inability to optimize the first layer, while complex prediction approaches introduce high computation overhead.

Method: Utilizes activations before attention block in same layer with 2 linear functions and ranking-aware loss, leveraging ranking-preserving functions in LLMs for accurate expert prediction.

Result: Achieves 93.03% accuracy on DeepSeek V2 Lite, 94.69% on Qwen3-30B, and 97.62% on Phi-mini-MoE, showing ~15% absolute accuracy improvement over state-of-the-art methods.

Conclusion: Pre-attention expert prediction enables accurate and lightweight expert prefetching for MoE LLMs, supporting first-layer optimization while maintaining low computation overhead.

Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) efficiently scale-up the model while keeping relatively low inference cost. As MoE models only activate part of the experts, related work has proposed expert prediction and caching methods to prefetch the experts for faster inference. However, existing approaches utilize the activations from the previous layer for prediction, incurring low accuracy and leave the first layer unoptimized. Applying complex layers or even training standalone networks for better prediction introduces high computation overhead. In this paper, we propose pre-attention expert prediction to achieve accurate and lightweight expert prefetching. The key insight is that some functions in LLMs are ranking-preserving, indicating that matching the ranking of selected experts using simple linear functions is possible. Therefore, we utilize the activations before the attention block in the same layer with 2 linear functions and ranking-aware loss to achieve accurate prediction, which also supports prefetching in the first layer. Our lightweight, pre-attention expert routers achieve 93.03% accuracy on DeepSeek V2 Lite, 94.69% on Qwen3-30B, and 97.62% on Phi-mini-MoE, showing about 15% improvement on absolute accuracy over the state-of-the-art methods.

</details>


### [22] [SpiderGen: Towards Procedure Generation For Carbon Life Cycle Assessments with Generative AI](https://arxiv.org/abs/2511.10684)
*Anupama Sitaraman,Bharathan Balaji,Yuvraj Agarwal*

Main category: cs.CL

TL;DR: SpiderGen is an LLM-based workflow that generates Life Cycle Assessment (LCA) process information for estimating environmental impact of consumer goods, achieving 62% F1-Score at under $1 cost in 10 minutes compared to traditional LCA costing $25,000+ and 21-person days.


<details>
  <summary>Details</summary>
Motivation: Climate change and global warming from GHG emissions are major concerns, with consumer products contributing significantly. Traditional LCAs are expensive and time-consuming, creating need for automated tools to estimate environmental impact.

Method: SpiderGen integrates traditional LCA taxonomy and methodology with LLM reasoning capabilities to generate procedural information for LCAs. It was evaluated against real-world LCA documents as ground-truth and compared to baseline techniques like chain-of-thought and one-shot prompting.

Result: SpiderGen achieved 62% F1-Score across 10 sample data points, providing accurate LCA process information that is either fully correct or has minor errors. It performs better than baseline techniques and can generate LCA information for under $1 USD in under 10 minutes.

Conclusion: SpiderGen demonstrates significant potential to reduce human effort and costs for carbon impact estimation, offering a cost-effective alternative to traditional LCA while maintaining reasonable accuracy in process generation.

Abstract: Investigating the effects of climate change and global warming caused by GHG emissions have been a primary concern worldwide. These emissions are largely contributed to by the production, use and disposal of consumer products. Thus, it is important to build tools to estimate the environmental impact of consumer goods, an essential part of which is conducting Life Cycle Assessments (LCAs). LCAs specify and account for the appropriate processes involved with the production, use, and disposal of the products. We present SpiderGen, an LLM-based workflow which integrates the taxonomy and methodology of traditional LCA with the reasoning capabilities and world knowledge of LLMs to generate the procedural information used for LCA. We additionally evaluate the output of SpiderGen using real-world LCA documents as ground-truth. We find that SpiderGen provides accurate LCA process information that is either fully correct or has minor errors, achieving an F1-Score of 62% across 10 sample data points. We observe that the remaining missed processes and hallucinated errors occur primarily due to differences in detail between LCA documents, as well as differences in the "scope" of which auxiliary processes must also be included. We also demonstrate that SpiderGen performs better than several baselines techniques, such as chain-of-thought prompting and one-shot prompting. Finally, we highlight SpiderGen's potential to reduce the human effort and costs for estimating carbon impact, as it is able to produce LCA process information for less than \$1 USD in under 10 minutes as compared to the status quo LCA, which can cost over \$25000 USD and take up to 21-person days.

</details>


### [23] [A methodological analysis of prompt perturbations and their effect on attack success rates](https://arxiv.org/abs/2511.10686)
*Tiago Machado,Maysa Malfiza Garcia de Macedo,Rogerio Abreu de Paula,Marcelo Carpinette Grave,Aminat Adebiyi,Luan Soares de Souza,Enrico Santarelli,Claudio Pinhanez*

Main category: cs.CL

TL;DR: This paper investigates how different LLM alignment methods (SFT, DPO, RLHF) affect model vulnerability to prompt attacks, finding that even small prompt modifications significantly change attack success rates and that existing attack benchmarks alone may not reveal all vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: To understand how different alignment methods affect LLM responses to prompt attacks and evaluate whether current attack benchmarks sufficiently expose model vulnerabilities.

Method: Selected open source models with different alignment methods (SFT, DPO, RLHF) and conducted systematic analysis using statistical methods to measure sensitivity of Attack Success Rate (ASR) to prompt variations.

Result: Small prompt modifications significantly change ASR, making models more or less susceptible to different attack types. Existing attack benchmarks alone may not be sufficient to reveal all vulnerabilities of models and alignment methods.

Conclusion: Systematic and statistically-based analyses are needed for comprehensive model attack evaluation, as alignment methods show varying sensitivity to prompt variations that current benchmarks may miss.

Abstract: This work aims to investigate how different Large Language Models (LLMs) alignment methods affect the models' responses to prompt attacks. We selected open source models based on the most common alignment methods, namely, Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reinforcement Learning with Human Feedback (RLHF). We conducted a systematic analysis using statistical methods to verify how sensitive the Attack Success Rate (ASR) is when we apply variations to prompts designed to elicit inappropriate content from LLMs. Our results show that even small prompt modifications can significantly change the Attack Success Rate (ASR) according to the statistical tests we run, making the models more or less susceptible to types of attack. Critically, our results demonstrate that running existing 'attack benchmarks' alone may not be sufficient to elicit all possible vulnerabilities of both models and alignment methods. This paper thus contributes to ongoing efforts on model attack evaluation by means of systematic and statistically-based analyses of the different alignment methods and how sensitive their ASR is to prompt variation.

</details>


### [24] [Modeling and Predicting Multi-Turn Answer Instability in Large Language Models](https://arxiv.org/abs/2511.10688)
*Jiahang He,Rishi Ramachandran,Neel Ramachandran,Aryan Katakam,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Aryan Shrivastava*

Main category: cs.CL

TL;DR: The paper evaluates LLM robustness using multi-turn questioning, finding significant accuracy drops with simple follow-up prompts and establishing stationary accuracy as a key robustness metric.


<details>
  <summary>Details</summary>
Motivation: As LLMs are increasingly deployed in real-world applications, their robustness under repeated questioning becomes crucial for interactive settings where consistent reasoning is essential.

Method: Using simple multi-turn follow-up prompts to evaluate answer changes, modeling accuracy dynamics with Markov chains, and examining linear probes' ability to predict answer changes from hidden states.

Result: Simple prompts like 'Think again' caused ~10% accuracy drop for Gemini 1.5 Flash over 9 turns; combined prompts caused 7.5% drop for Claude 3.5 Haiku. Stationary accuracy was ~8% lower than first-turn accuracy for Gemini 1.5 Flash.

Conclusion: LLMs show significant fragility under repeated questioning, establishing stationary accuracy as a principled robustness metric. Addressing this instability is essential for high-stakes interactive deployments.

Abstract: As large language models (LLMs) are adopted in an increasingly wide range of applications, user-model interactions have grown in both frequency and scale. Consequently, research has focused on evaluating the robustness of LLMs, an essential quality for real-world tasks. In this paper, we employ simple multi-turn follow-up prompts to evaluate models' answer changes, model accuracy dynamics across turns with Markov chains, and examine whether linear probes can predict these changes. Our results show significant vulnerabilities in LLM robustness: a simple "Think again" prompt led to an approximate 10% accuracy drop for Gemini 1.5 Flash over nine turns, while combining this prompt with a semantically equivalent reworded question caused a 7.5% drop for Claude 3.5 Haiku. Additionally, we find that model accuracy across turns can be effectively modeled using Markov chains, enabling the prediction of accuracy probabilities over time. This allows for estimation of the model's stationary (long-run) accuracy, which we find to be on average approximately 8% lower than its first-turn accuracy for Gemini 1.5 Flash. Our results from a model's hidden states also reveal evidence that linear probes can help predict future answer changes. Together, these results establish stationary accuracy as a principled robustness metric for interactive settings and expose the fragility of models under repeated questioning. Addressing this instability will be essential for deploying LLMs in high-stakes and interactive settings where consistent reasoning is as important as initial accuracy.

</details>


### [25] [Equilibrium Dynamics and Mitigation of Gender Bias in Synthetically Generated Data](https://arxiv.org/abs/2511.10689)
*Ashish Kattamuri,Arpita Vats,Harshwardhan Fartale,Rahul Raja,Akshata Kishore Moharir,Ishita Prasad*

Main category: cs.CL

TL;DR: Recursive prompting with LLMs for synthetic data generation shows gender bias equilibrium dynamics rather than monotonic amplification - low initial bias amplifies while high bias decays toward the model's inherent level. Contrastive augmentation effectively reduces downstream bias despite higher embedding scores, revealing metric divergence.


<details>
  <summary>Details</summary>
Motivation: To investigate gender bias dynamics in recursive LLM prompting for synthetic dataset generation and evaluate different mitigation strategies, addressing the risk of bias amplification in scalable data generation methods.

Method: Used three complementary evaluation frameworks: rule-based pattern matching, embedding-based semantic similarity, and downstream task performance. Experiments with three initial bias levels (0.1, 0.3, 0.6) and four mitigation strategies across three generations of recursive text generation.

Result: Found equilibrium dynamics where low initial bias amplifies toward model's inherent level (+36%) while high initial bias decays (-26%). Contrastive augmentation achieved 98.8% downstream bias reduction for low initial bias and 91% average reduction, despite producing higher embedding-based bias scores.

Conclusion: Semantic similarity metrics may diverge from behavioral fairness outcomes, highlighting the need for multidimensional evaluation in responsible synthetic data generation. Contrastive augmentation is effective for bias mitigation despite metric paradoxes.

Abstract: Recursive prompting with large language models enables scalable synthetic dataset generation but introduces the risk of bias amplification. We investigate gender bias dynamics across three generations of recursive text generation using three complementary evaluation frameworks: rule-based pattern matching, embedding-based semantic similarity, and downstream task performance. Experiments with three initial bias levels (0.1, 0.3, 0.6) and four mitigation strategies reveal equilibrium dynamics rather than monotonic amplification. The low initial bias amplifies toward the model's inherent bias level (+36%), whereas the high initial bias decays toward it (-26%). Among mitigation methods, contrastive augmentation, which introduces gender-swapped variants, achieves significant downstream bias reduction (98.8% for low initial bias and 91% on average) despite producing higher embedding-based bias scores. This paradox demonstrates that semantic similarity metrics may diverge from behavioral fairness outcomes, highlighting the need for multidimensional evaluation in responsible synthetic data generation.

</details>


### [26] [Saying the Unsaid: Revealing the Hidden Language of Multimodal Systems Through Telephone Games](https://arxiv.org/abs/2511.10690)
*Juntu Zhao,Jialing Zhang,Chongxuan Li,Dequan Wang*

Main category: cs.CL

TL;DR: The paper introduces a 'telephone game' method to study multimodal systems' hidden language by analyzing their preference bias during image compression and reconstruction, revealing concept connection patterns.


<details>
  <summary>Details</summary>
Motivation: To understand the opaque hidden language of closed-source multimodal systems by studying their inherent preference bias that disrupts original concept co-occurrence during image processing.

Method: Uses multi-round 'telephone game' framework with Telescope dataset (10,000+ concept pairs) to quantitatively analyze concept connection strength through iterative image compression and reconstruction cycles.

Result: Enables construction of global concept connection maps, identification of training-inherited preference bias, assessment of generalization capabilities, and discovery of stable pathways for fragile concept connections.

Conclusion: Provides new perspective on multimodal systems' hidden language and foundation for future research on interpretability and controllability of these systems.

Abstract: Recent closed-source multimodal systems have made great advances, but their hidden language for understanding the world remains opaque because of their black-box architectures. In this paper, we use the systems' preference bias to study their hidden language: During the process of compressing the input images (typically containing multiple concepts) into texts and then reconstructing them into images, the systems' inherent preference bias introduces specific shifts in the outputs, disrupting the original input concept co-occurrence. We employ the multi-round "telephone game" to strategically leverage this bias. By observing the co-occurrence frequencies of concepts in telephone games, we quantitatively investigate the concept connection strength in the understanding of multimodal systems, i.e., "hidden language." We also contribute Telescope, a dataset of 10,000+ concept pairs, as the database of our telephone game framework. Our telephone game is test-time scalable: By iteratively running telephone games, we can construct a global map of concept connections in multimodal systems' understanding. Here we can identify preference bias inherited from training, assess generalization capability advancement, and discover more stable pathways for fragile concept connections. Furthermore, we use Reasoning-LLMs to uncover unexpected concept relationships that transcend textual and visual similarities, inferring how multimodal systems understand and simulate the world. This study offers a new perspective on the hidden language of multimodal systems and lays the foundation for future research on the interpretability and controllability of multimodal systems.

</details>


### [27] [Evaluating from Benign to Dynamic Adversarial: A Squid Game for Large Language Models](https://arxiv.org/abs/2511.10691)
*Zijian Chen,Wenjun Zhang,Guangtao Zhai*

Main category: cs.CL

TL;DR: Squid Game is a dynamic adversarial evaluation framework for LLMs that addresses data contamination issues in static benchmarks by testing models in resource-constrained, asymmetric information scenarios through interactive gameplay.


<details>
  <summary>Details</summary>
Motivation: To address limitations of static benchmarks where models may have seen test questions before, and to evaluate LLM behavior under pressure in adversarial settings that existing benchmarks don't cover.

Method: Developed a dynamic adversarial evaluation environment with six elimination-style levels testing instruction-following, code, reasoning, planning, and safety alignment through interactive gameplay against other LLM opponents.

Result: Evaluated over 50 LLMs, observed generational performance transitions, found evidence of speculative shortcuts, and showed dynamic evaluation complements static benchmarks through correlation analysis.

Conclusion: Dynamic adversarial evaluation like Squid Game provides complementary assessment to static benchmarks, revealing different model behaviors and potential for higher-level evaluation contamination in traditional frameworks.

Abstract: Contemporary benchmarks are struggling to keep pace with the development of large language models (LLMs). Although they are indispensable to evaluate model performance on various tasks, it is uncertain whether the models trained on Internet data have genuinely learned how to solve problems or merely seen the questions before. This potential data contamination issue presents a fundamental challenge to establishing trustworthy evaluation frameworks. Meanwhile, existing benchmarks predominantly assume benign, resource-rich settings, leaving the behavior of LLMs under pressure unexplored. In this paper, we introduce Squid Game, a dynamic and adversarial evaluation environment with resource-constrained and asymmetric information settings elaborated to evaluate LLMs through interactive gameplay against other LLM opponents. Notably, Squid Game consists of six elimination-style levels, focusing on multi-faceted abilities, such as instruction-following, code, reasoning, planning, and safety alignment. We evaluate over 50 LLMs on Squid Game, presenting the largest behavioral evaluation study of general LLMs on dynamic adversarial scenarios. We observe a clear generational phase transition on performance in the same model lineage and find evidence that some models resort to speculative shortcuts to win the game, indicating the possibility of higher-level evaluation paradigm contamination in static benchmarks. Furthermore, we compare prominent LLM benchmarks and Squid Game with correlation analyses, highlighting that dynamic evaluation can serve as a complementary part for static evaluations. The code and data will be released in the future.

</details>


### [28] [Do AI Voices Learn Social Nuances? A Case of Politeness and Speech Rate](https://arxiv.org/abs/2511.10693)
*Eyal Rabin,Zohar Elyoseph,Rotem Israel-Fishelson,Adi Dali,Ravit Nussinson*

Main category: cs.CL

TL;DR: AI text-to-speech systems implicitly learn to reduce speech rate when conveying politeness, demonstrating their ability to internalize human social conventions without explicit programming.


<details>
  <summary>Details</summary>
Motivation: To investigate whether AI systems have internalized implicit human social cues, specifically the tendency to reduce speech rate to convey politeness as a non-obvious prosodic marker.

Method: Prompted 22 synthetic voices from two leading AI platforms (AI Studio and OpenAI) to read a fixed script under "polite and formal" and "casual and informal" conditions, then measured the resulting speech duration.

Result: Across both AI platforms, the polite prompt produced significantly slower speech than the casual prompt with very large effect sizes. The effect was statistically significant for all of AI Studio's voices and for a large majority of OpenAI's voices.

Conclusion: AI can implicitly learn and replicate psychological nuances of human communication, highlighting its emerging role as a social actor capable of reinforcing human social norms.

Abstract: Voice-based artificial intelligence is increasingly expected to adhere to human social conventions, but can it learn implicit cues that are not explicitly programmed? This study investigates whether state-of-the-art text-to-speech systems have internalized the human tendency to reduce speech rate to convey politeness - a non-obvious prosodic marker. We prompted 22 synthetic voices from two leading AI platforms (AI Studio and OpenAI) to read a fixed script under both "polite and formal" and "casual and informal" conditions and measured the resulting speech duration. Across both AI platforms, the polite prompt produced slower speech than the casual prompt with very large effect sizes, an effect that was statistically significant for all of AI Studio's voices and for a large majority of OpenAI's voices. These results demonstrate that AI can implicitly learn and replicate psychological nuances of human communication, highlighting its emerging role as a social actor capable of reinforcing human social norms.

</details>


### [29] [Where does an LLM begin computing an instruction?](https://arxiv.org/abs/2511.10694)
*Aditya Pola,Vineeth N. Balasubramanian*

Main category: cs.CL

TL;DR: The paper investigates where instruction following begins in language models by measuring when reading transitions to doing through activation patching experiments on simple tasks.


<details>
  <summary>Details</summary>
Motivation: To understand the distinct sub-processes in instruction following and identify the specific point in the layer stack where reading gives way to execution.

Method: Used three simple datasets (Key-Value, Quote Attribution, Letter Selection) and their two-hop compositions, applying activation patching on minimal-contrast prompt pairs to measure layer-wise flip rates across Llama family models.

Result: Found an inflection point (onset) where interventions changing predictions before this point become ineffective afterward, with multi-hop compositions showing similar onset locations.

Conclusion: Provides a replicable method to locate where instruction following begins and compare this location across tasks and model sizes.

Abstract: Following an instruction involves distinct sub-processes, such as reading content, reading the instruction, executing it, and producing an answer. We ask where, along the layer stack, instruction following begins, the point where reading gives way to doing. We introduce three simple datasets (Key-Value, Quote Attribution, Letter Selection) and two hop compositions of these tasks. Using activation patching on minimal-contrast prompt pairs, we measure a layer-wise flip rate that indicates when substituting selected residual activations changes the predicted answer. Across models in the Llama family, we observe an inflection point, which we term onset, where interventions that change predictions before this point become largely ineffective afterward. Multi-hop compositions show a similar onset location. These results provide a simple, replicable way to locate where instruction following begins and to compare this location across tasks and model sizes.

</details>


### [30] ["As Eastern Powers, I will veto." : An Investigation of Nation-level Bias of Large Language Models in International Relations](https://arxiv.org/abs/2511.10695)
*Jonghyeon Choi,Yeonjun Choi,Hyun-chul Kim,Beakcheol Jang*

Main category: cs.CL

TL;DR: This paper examines nation-level biases in LLMs using UN Security Council data, develops a bias evaluation framework with three tests, finds multidimensional biases that vary by model and context, and introduces a debiasing framework using RAG and Reflexion that reduces bias and improves performance.


<details>
  <summary>Details</summary>
Motivation: To systematically investigate nation-level biases in LLMs within International Relations, particularly focusing on the five permanent UN Security Council members, and address the need for bias assessment alongside performance in IR applications.

Method: Developed a bias evaluation framework using UNSC historical records with three distinct tests, analyzed various LLMs, and introduced a debiasing framework combining Retrieval-Augmented Generation with Reflexion-based self-reflection techniques.

Result: LLMs exhibit multidimensional biases that vary across models and tasks, with general patterns of favorable biases toward western nations and unfavorable biases toward Russia. Models with stronger reasoning abilities showed reduced bias. The debiasing framework effectively reduced nation-level bias and improved performance, particularly in GPT-4o-mini and LLama-3.3-70B.

Conclusion: Nation-level bias in LLMs is fundamentally multidimensional and varies across models and evaluation contexts. Assessing bias alongside performance is crucial when applying LLMs in International Relations, and the proposed debiasing framework shows promise in mitigating these biases.

Abstract: This paper systematically examines nation-level biases exhibited by Large Language Models (LLMs) within the domain of International Relations (IR). Leveraging historical records from the United Nations Security Council (UNSC), we developed a bias evaluation framework comprising three distinct tests to explore nation-level bias in various LLMs, with a particular focus on the five permanent members of the UNSC. Experimental results show that, even with the general bias patterns across models (e.g., favorable biases toward the western nations, and unfavorable biases toward Russia), these still vary based on the LLM. Notably, even within the same LLM, the direction and magnitude of bias for a nation change depending on the evaluation context. This observation suggests that LLM biases are fundamentally multidimensional, varying across models and tasks. We also observe that models with stronger reasoning abilities show reduced bias and better performance. Building on this finding, we introduce a debiasing framework that improves LLMs' factual reasoning combining Retrieval-Augmented Generation with Reflexion-based self-reflection techniques. Experiments show it effectively reduces nation-level bias, and improves performance, particularly in GPT-4o-mini and LLama-3.3-70B. Our findings emphasize the need to assess nation-level bias alongside performance when applying LLMs in the IR domain.

</details>


### [31] [$π$-Attention: Periodic Sparse Transformers for Efficient Long-Context Modeling](https://arxiv.org/abs/2511.10696)
*Dong Liu,Yanxuan Yu*

Main category: cs.CL

TL;DR: ΠAttention is a periodic sparse Transformer that addresses quadratic complexity in standard Transformers by factorizing attention into ring-local neighborhoods, deterministic π-stride skips, and adaptive fusion gates, achieving linear complexity while maintaining or surpassing dense attention quality.


<details>
  <summary>Details</summary>
Motivation: Transformers face quadratic complexity bottlenecks for long-range modeling, and existing sparse attention methods like RingAttention suffer from limited receptive fields and lack adaptability.

Method: The approach factorizes attention into three components: ring-local neighborhoods for local context, deterministic π-stride skips for periodic long-range connections, and an adaptive fusion gate to balance local and global information.

Result: ΠAttention achieves O(kL + πlogL) receptive field growth compared to O(kL) for RingAttention, reduces perplexity by 8.3% over RingAttention, and uses 50% fewer GPUs for the same context length while matching or surpassing dense attention quality.

Conclusion: Periodic sparse attention with adaptive fusion provides an effective solution for efficient long-context modeling, with periodic skips, adaptive fusion, and head-level sparsity coordination being crucial components.

Abstract: Transformers have revolutionized natural language processing, but their quadratic complexity with respect to sequence length remains a fundamental bottleneck for long-range modeling. While sparse attention mechanisms like RingAttention reduce computational costs by restricting attention to local neighborhoods, they suffer from limited receptive fields and lack of adaptability. We present \PiAttention, a periodic sparse Transformer that factorizes attention into ring-local neighborhoods, deterministic $π$-stride skips, and an adaptive fusion gate. The periodic structure provides predictable coverage of distant tokens, while the sparse footprint keeps the per-layer complexity linear in context length. We prove that \PiAttention achieves $\mathcal{O}(kL + π\log L)$ receptive field growth compared to $\mathcal{O}(kL)$ for RingAttention, where $k$ is the local window size, $π$ is the skip period, and $L$ is the sequence length. Extensive experiments on language modeling, retrieval, and vision-language tasks demonstrate that \PiAttention matches or surpasses dense attention quality with 8.3\% lower perplexity than RingAttention while using 50\% fewer GPUs for the same context length. Our detailed ablations and visualizations reveal the importance of periodic skips, adaptive fusion, and head-level sparsity coordination for efficient long-context modeling.

</details>


### [32] [Faithful Summarization of Consumer Health Queries: A Cross-Lingual Framework with LLMs](https://arxiv.org/abs/2511.10768)
*Ajwad Abrar,Nafisa Tabassum Oeshy,Prianka Maheru,Farzana Tabassum,Tareque Mohmud Chowdhury*

Main category: cs.CL

TL;DR: Proposes a framework combining TextRank-based sentence extraction and medical NER with LLMs to improve faithfulness in medical text summarization, achieving better quality and faithfulness metrics than baselines.


<details>
  <summary>Details</summary>
Motivation: Consumer health question summarization can improve healthcare communication, but unfaithful summaries that misrepresent medical details pose serious risks to patient safety.

Method: Combines TextRank-based sentence extraction and medical named entity recognition with large language models (LLMs), fine-tuning LLaMA-2-7B on MeQSum (English) and BanglaCHQ-Summ (Bangla) datasets.

Result: Achieved consistent improvements across quality (ROUGE, BERTScore, readability) and faithfulness (SummaC, AlignScore) metrics, outperforming zero-shot baselines and prior systems. Human evaluation shows over 80% of generated summaries preserve critical medical information.

Conclusion: Faithfulness is an essential dimension for reliable medical summarization, and the proposed approach demonstrates potential for safer deployment of LLMs in healthcare contexts.

Abstract: Summarizing consumer health questions (CHQs) can ease communication in healthcare, but unfaithful summaries that misrepresent medical details pose serious risks. We propose a framework that combines TextRank-based sentence extraction and medical named entity recognition with large language models (LLMs) to enhance faithfulness in medical text summarization. In our experiments, we fine-tuned the LLaMA-2-7B model on the MeQSum (English) and BanglaCHQ-Summ (Bangla) datasets, achieving consistent improvements across quality (ROUGE, BERTScore, readability) and faithfulness (SummaC, AlignScore) metrics, and outperforming zero-shot baselines and prior systems. Human evaluation further shows that over 80\% of generated summaries preserve critical medical information. These results highlight faithfulness as an essential dimension for reliable medical summarization and demonstrate the potential of our approach for safer deployment of LLMs in healthcare contexts.

</details>


### [33] [TEDxTN: A Three-way Speech Translation Corpus for Code-Switched Tunisian Arabic - English](https://arxiv.org/abs/2511.10780)
*Fethi Bougares,Salima Mdhaffar,Haroun Elleuch,Yannick Estève*

Main category: cs.CL

TL;DR: TEDxTN is the first publicly available Tunisian Arabic to English speech translation dataset, containing 108 TEDx talks (25 hours) with code-switching from 11 Tunisian regions, along with annotation guidelines and baseline system results.


<details>
  <summary>Details</summary>
Motivation: To address data scarcity for Arabic dialects and facilitate research on Tunisian dialect natural language processing by creating the first open source speech translation corpus for code-switching Tunisian Arabic.

Method: Collected, segmented, transcribed and translated 108 TEDx talks following internally developed annotation guidelines, covering speakers with various accents from over 11 Tunisian regions.

Result: Created a 25-hour speech corpus with code-switching, made annotation guidelines and corpus publicly available, and established baseline results for Speech Recognition and Speech Translation using pre-trained and fine-tuned end-to-end models.

Conclusion: TEDxTN is a valuable resource that enables extension to new talks and facilitates further research on Tunisian dialect natural language processing, being the first open source speech translation corpus for code-switching Tunisian dialect.

Abstract: In this paper, we introduce TEDxTN, the first publicly available Tunisian Arabic to English speech translation dataset. This work is in line with the ongoing effort to mitigate the data scarcity obstacle for a number of Arabic dialects. We collected, segmented, transcribed and translated 108 TEDx talks following our internally developed annotations guidelines. The collected talks represent 25 hours of speech with code-switching that cover speakers with various accents from over 11 different regions of Tunisia. We make the annotation guidelines and corpus publicly available. This will enable the extension of TEDxTN to new talks as they become available. We also report results for strong baseline systems of Speech Recognition and Speech Translation using multiple pre-trained and fine-tuned end-to-end models. This corpus is the first open source and publicly available speech translation corpus of Code-Switching Tunisian dialect. We believe that this is a valuable resource that can motivate and facilitate further research on the natural language processing of Tunisian Dialect.

</details>


### [34] [Sabiá: Um Chatbot de Inteligência Artificial Generativa para Suporte no Dia a Dia do Ensino Superior](https://arxiv.org/abs/2511.10787)
*Guilherme Biava Rodrigues,Franciele Beal,Marlon Marcon,Alinne Cristinne Corrêa Souza,André Roberto Ortoncelli,Francisco Carlos Monteiro Souza,Rodolfo Adamshuk Silva*

Main category: cs.CL

TL;DR: Development of a GenAI chatbot using RAG to help students access fragmented academic information across university documents and websites.


<details>
  <summary>Details</summary>
Motivation: Students face difficulties accessing day-to-day academic information due to fragmentation across institutional documents and websites, causing confusion and lack of clarity.

Method: Developed a chatbot using Generative AI and Retrieval-Augmented Generation (RAG), testing several GenAI models using quality metrics and LLM-as-a-Judge approach.

Result: Gemini 2.0 Flash performed best for quality and speed, while Gemma 3n showed good performance with open-source advantages.

Conclusion: A GenAI chatbot with RAG can effectively address student information access challenges, with Gemini 2.0 Flash and Gemma 3n being suitable model choices.

Abstract: Students often report difficulties in accessing day-to-day academic information, which is usually spread across numerous institutional documents and websites. This fragmentation results in a lack of clarity and causes confusion about routine university information. This project proposes the development of a chatbot using Generative Artificial Intelligence (GenAI) and Retrieval-Augmented Generation (RAG) to simplify access to such information. Several GenAI models were tested and evaluated based on quality metrics and the LLM-as-a-Judge approach. Among them, Gemini 2.0 Flash stood out for its quality and speed, and Gemma 3n for its good performance and open-source nature.

</details>


### [35] [LLM-as-a-Grader: Practical Insights from Large Language Model for Short-Answer and Report Evaluation](https://arxiv.org/abs/2511.10819)
*Grace Byun,Swati Rajwal,Jinho D. Choi*

Main category: cs.CL

TL;DR: This study evaluates GPT-4o's performance in grading short-answer quizzes and project reports in an undergraduate Computational Linguistics course, comparing LLM-generated scores with human TA evaluations.


<details>
  <summary>Details</summary>
Motivation: To investigate the feasibility and alignment of LLMs with human evaluation in real classroom settings for educational assessment tasks like grading.

Method: Collected student responses from ~50 students across five quizzes and project reports from 14 teams, then compared GPT-4o-generated scores against independent human evaluations by course teaching assistants.

Result: GPT-4o achieved strong correlation with human graders (up to 0.98) and exact score agreement in 55% of quiz cases. For project reports, it showed strong overall alignment but some variability in scoring technical, open-ended responses.

Conclusion: LLMs show significant potential for automated grading in academic settings but have limitations, particularly with technical and open-ended responses. The study contributes to advancing automated grading systems in real-world education.

Abstract: Large Language Models (LLMs) are increasingly explored for educational tasks such as grading, yet their alignment with human evaluation in real classrooms remains underexamined. In this study, we investigate the feasibility of using an LLM (GPT-4o) to evaluate short-answer quizzes and project reports in an undergraduate Computational Linguistics course. We collect responses from approximately 50 students across five quizzes and receive project reports from 14 teams. LLM-generated scores are compared against human evaluations conducted independently by the course teaching assistants (TAs). Our results show that GPT-4o achieves strong correlation with human graders (up to 0.98) and exact score agreement in 55\% of quiz cases. For project reports, it also shows strong overall alignment with human grading, while exhibiting some variability in scoring technical, open-ended responses. We release all code and sample data to support further research on LLMs in educational assessment. This work highlights both the potential and limitations of LLM-based grading systems and contributes to advancing automated grading in real-world academic settings.

</details>


### [36] [Tracing Multilingual Representations in LLMs with Cross-Layer Transcoders](https://arxiv.org/abs/2511.10840)
*Abir Harrasse,Florent Draye,Zhijing Jin,Bernhard Schölkopf*

Main category: cs.CL

TL;DR: This paper investigates how multilingual LLMs represent different languages internally, finding evidence for pivot language representations where models use nearly identical representations across languages with language-specific decoding in later layers.


<details>
  <summary>Details</summary>
Motivation: To understand why multilingual LLMs perform better in dominant training languages despite processing many languages, and to uncover the internal mechanisms of multilingual representation.

Method: Train LLMs on different multilingual data mixtures, then analyze internal mechanisms using cross-layer transcoders (CLT) and attribution graphs to study representation sharing and language-specific decoding.

Result: Found strong evidence for pivot language representations - models use nearly identical representations across languages with language-specific decoding emerging in later layers. Attribution analysis shows decoding relies on high-frequency language features that linearly read out language identity from early layers.

Conclusion: Understanding the pivot-language mechanism is crucial for improving multilingual alignment in LLMs, as models employ shared representations with language-specific decoding, explaining performance disparities across languages.

Abstract: Multilingual Large Language Models (LLMs) can process many languages, yet how they internally represent this diversity remains unclear. Do they form shared multilingual representations with language-specific decoding, and if so, why does performance still favor the dominant training language? To address this, we train a series of LLMs on different mixtures of multilingual data and analyze their internal mechanisms using cross-layer transcoders (CLT) and attribution graphs. Our results provide strong evidence for pivot language representations: the model employs nearly identical representations across languages, while language-specific decoding emerges in later layers. Attribution analyses reveal that decoding relies in part on a small set of high-frequency language features in the final layers, which linearly read out language identity from the first layers in the model. By intervening on these features, we can suppress one language and substitute another in the model's outputs. Finally, we study how the dominant training language influences these mechanisms across attribution graphs and decoding pathways. We argue that understanding this pivot-language mechanism is crucial for improving multilingual alignment in LLMs.

</details>


### [37] [Reinforcing Stereotypes of Anger: Emotion AI on African American Vernacular English](https://arxiv.org/abs/2511.10846)
*Rebecca Dorn,Christina Chance,Casandra Rusti,Charles Bickham,Kai-Wei Chang,Fred Morstatter,Kristina Lerman*

Main category: cs.CL

TL;DR: Emotion recognition models show significant bias against African American Vernacular English (AAVE), with false positive anger predictions more than double on AAVE compared to General American English, reinforcing racial stereotypes.


<details>
  <summary>Details</summary>
Motivation: To examine how emotion recognition models perform on African American Vernacular English (AAVE) compared to General American English (GAE), given that models often rely on annotations reflecting dominant cultural norms and may fail to recognize emotional expression in excluded dialects.

Method: Analyzed 2.7 million geo-tagged tweets from Los Angeles, scoring AAVE strength using computational dialect features. Collected emotion annotations on 875 tweets with high/low AAVE density, using community-informed 'silver' labels from African American AAVE-fluent annotators. Compared model performance across GPT, BERT-based models, and SpanEmo.

Result: Models exhibited false positive anger prediction rates more than double on AAVE vs GAE. SpanEmo increased false positive anger rates from 25% on GAE to 60% on AAVE. Models and non-ingroup annotations correlated more with profanity-based AAVE features than ingroup annotations. Neighborhoods with higher African American populations showed higher anger predictions (r=0.27) and lower joy predictions (r=-0.10).

Conclusion: Emotion AI systems demonstrate emergent safety issues by reinforcing racial stereotypes through biased emotion classification, highlighting the critical need for culturally and dialect-informed affective computing systems.

Abstract: Automated emotion detection is widely used in applications ranging from well-being monitoring to high-stakes domains like mental health and hiring. However, models often rely on annotations that reflect dominant cultural norms, limiting model ability to recognize emotional expression in dialects often excluded from training data distributions, such as African American Vernacular English (AAVE). This study examines emotion recognition model performance on AAVE compared to General American English (GAE). We analyze 2.7 million tweets geo-tagged within Los Angeles. Texts are scored for strength of AAVE using computational approximations of dialect features. Annotations of emotion presence and intensity are collected on a dataset of 875 tweets with both high and low AAVE densities. To assess model accuracy on a task as subjective as emotion perception, we calculate community-informed "silver" labels where AAVE-dense tweets are labeled by African American, AAVE-fluent (ingroup) annotators. On our labeled sample, GPT and BERT-based models exhibit false positive prediction rates of anger on AAVE more than double than on GAE. SpanEmo, a popular text-based emotion model, increases false positive rates of anger from 25 percent on GAE to 60 percent on AAVE. Additionally, a series of linear regressions reveals that models and non-ingroup annotations are significantly more correlated with profanity-based AAVE features than ingroup annotations. Linking Census tract demographics, we observe that neighborhoods with higher proportions of African American residents are associated with higher predictions of anger (Pearson's correlation r = 0.27) and lower joy (r = -0.10). These results find an emergent safety issue of emotion AI reinforcing racial stereotypes through biased emotion classification. We emphasize the need for culturally and dialect-informed affective computing systems.

</details>


### [38] [Leveraging Parameter Space Symmetries for Reasoning Skill Transfer in LLMs](https://arxiv.org/abs/2511.10850)
*Stefan Horoi,Sangwoo Cho,Supriyo Chakraborty,Shi-Xiong Zhang,Sambit Sahu,Guy Wolf,Genta Indra Winata*

Main category: cs.CL

TL;DR: The paper introduces a parameter space alignment method to improve task arithmetic for LLMs by addressing negative interference issues through exploiting Transformer symmetries, enabling effective skill transfer between diverged models.


<details>
  <summary>Details</summary>
Motivation: Task arithmetic suffers from negative interference when models diverge during training, limiting its effectiveness for transferring skills between LLMs.

Method: Align parameter spaces using Transformer symmetries (permutation, rotation, scaling), adapt alignment for GQA and SwiGLU layers with weight-based and activation-based approaches, then apply task arithmetic.

Result: Successfully transferred advanced reasoning skills to non-reasoning models, consistently outperforming standard task arithmetic on challenging reasoning benchmarks.

Conclusion: The alignment-first strategy provides an effective approach for merging specialized skills across evolving LLM families, reducing redundant fine-tuning and enhancing model adaptability.

Abstract: Task arithmetic is a powerful technique for transferring skills between Large Language Models (LLMs), but it often suffers from negative interference when models have diverged during training. We address this limitation by first aligning the models' parameter spaces, leveraging the inherent permutation, rotation, and scaling symmetries of Transformer architectures. We adapt parameter space alignment for modern Grouped-Query Attention (GQA) and SwiGLU layers, exploring both weight-based and activation-based approaches. Using this alignment-first strategy, we successfully transfer advanced reasoning skills to a non-reasoning model. Experiments on challenging reasoning benchmarks show that our method consistently outperforms standard task arithmetic. This work provides an effective approach for merging and transferring specialized skills across evolving LLM families, reducing redundant fine-tuning and enhancing model adaptability.

</details>


### [39] [Unsupervised Cycle Detection in Agentic Applications](https://arxiv.org/abs/2511.10650)
*Felix George,Harshit Kumar,Divya Pathak,Kaustabha Ray,Mudit Verma,Pratibha Moogi*

Main category: cs.CL

TL;DR: Unsupervised hybrid framework combining structural and semantic analysis to detect hidden execution cycles in LLM-powered agentic applications, achieving F1 score of 0.72 on stock market application data.


<details>
  <summary>Details</summary>
Motivation: Agentic applications with LLMs exhibit non-deterministic behaviors that create hidden execution cycles, silently consuming resources without triggering errors, which traditional observability platforms fail to detect.

Method: Combines structural analysis (temporal call stack analysis for explicit loops) with semantic analysis (semantic similarity for redundant content cycles) in an unsupervised framework.

Result: Achieved F1 score of 0.72 (precision: 0.62, recall: 0.86) on 1575 trajectories from LangGraph-based stock market application, significantly outperforming individual structural (F1: 0.08) and semantic methods (F1: 0.28).

Conclusion: Results are encouraging but substantial scope for improvement remains; future work needed to refine approach and address current limitations.

Abstract: Agentic applications powered by Large Language Models exhibit non-deterministic behaviors that can form hidden execution cycles, silently consuming resources without triggering explicit errors. Traditional observability platforms fail to detect these costly inefficiencies. We present an unsupervised cycle detection framework that combines structural and semantic analysis. Our approach first applies computationally efficient temporal call stack analysis to identify explicit loops and then leverages semantic similarity analysis to uncover subtle cycles characterized by redundant content generation. Evaluated on 1575 trajectories from a LangGraph-based stock market application, our hybrid approach achieves an F1 score of 0.72 (precision: 0.62, recall: 0.86), significantly outperforming individual structural (F1: 0.08) and semantic methods (F1: 0.28). While these results are encouraging, there remains substantial scope for improvement, and future work is needed to refine the approach and address its current limitations.

</details>


### [40] [From Fact to Judgment: Investigating the Impact of Task Framing on LLM Conviction in Dialogue Systems](https://arxiv.org/abs/2511.10871)
*Parisa Rabbani,Nimet Beyza Bozdag,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: LLMs show inconsistent judgment when tasks shift from direct factual queries to conversational contexts, with some models becoming sycophantic and others overly-critical under social framing.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLM-judges can reliably assess tasks requiring social or conversational judgment, and understand how conversational framing affects their conviction.

Method: Contrast model performance on direct factual queries vs. conversational judgment tasks, applying pressure through simple rebuttals to measure conviction maintenance.

Result: Models show significant judgment changes (9.24% average performance change) under conversational framing - GPT-4o-mini becomes sycophantic while Llama-8B-Instruct becomes overly-critical.

Conclusion: Conversational framing significantly alters LLM judgment, highlighting the need for frameworks to diagnose model conviction and develop more trustworthy dialogue systems.

Abstract: LLMs are increasingly employed as judges across a variety of tasks, including those involving everyday social interactions. Yet, it remains unclear whether such LLM-judges can reliably assess tasks that require social or conversational judgment. We investigate how an LLM's conviction is changed when a task is reframed from a direct factual query to a Conversational Judgment Task. Our evaluation framework contrasts the model's performance on direct factual queries with its assessment of a speaker's correctness when the same information is presented within a minimal dialogue, effectively shifting the query from "Is this statement correct?" to "Is this speaker correct?". Furthermore, we apply pressure in the form of a simple rebuttal ("The previous answer is incorrect.") to both conditions. This perturbation allows us to measure how firmly the model maintains its position under conversational pressure. Our findings show that while some models like GPT-4o-mini reveal sycophantic tendencies under social framing tasks, others like Llama-8B-Instruct become overly-critical. We observe an average performance change of 9.24% across all models, demonstrating that even minimal dialogue context can significantly alter model judgment, underscoring conversational framing as a key factor in LLM-based evaluation. The proposed framework offers a reproducible methodology for diagnosing model conviction and contributes to the development of more trustworthy dialogue systems.

</details>


### [41] [ICX360: In-Context eXplainability 360 Toolkit](https://arxiv.org/abs/2511.10879)
*Dennis Wei,Ronny Luss,Xiaomeng Hu,Lucas Monteiro Paes,Pin-Yu Chen,Karthikeyan Natesan Ramamurthy,Erik Miehling,Inge Vejsbjerg,Hendrik Strobelt*

Main category: cs.CL

TL;DR: ICX360 is an open-source Python toolkit for explaining LLM outputs using black-box and white-box methods through perturbations and gradients, with focus on user-provided context and prompts.


<details>
  <summary>Details</summary>
Motivation: As LLMs enter higher-stakes applications, there's a critical need for tools to explain their outputs (summaries, responses, etc.) to ensure transparency and trustworthiness.

Method: The toolkit implements three recent explanation tools using both black-box (perturbations) and white-box (gradients) methods to analyze LLM behavior based on user-provided context and prompts.

Result: ICX360 provides an open-source framework with quick-start guidance and detailed tutorials covering various use cases including retrieval augmented generation, natural language generation, and jailbreaking scenarios.

Conclusion: ICX360 addresses the growing need for LLM explainability tools by providing accessible implementations of explanation methods that focus on understanding how user context influences model outputs.

Abstract: Large Language Models (LLMs) have become ubiquitous in everyday life and are entering higher-stakes applications ranging from summarizing meeting transcripts to answering doctors' questions. As was the case with earlier predictive models, it is crucial that we develop tools for explaining the output of LLMs, be it a summary, list, response to a question, etc. With these needs in mind, we introduce In-Context Explainability 360 (ICX360), an open-source Python toolkit for explaining LLMs with a focus on the user-provided context (or prompts in general) that are fed to the LLMs. ICX360 contains implementations for three recent tools that explain LLMs using both black-box and white-box methods (via perturbations and gradients respectively). The toolkit, available at https://github.com/IBM/ICX360, contains quick-start guidance materials as well as detailed tutorials covering use cases such as retrieval augmented generation, natural language generation, and jailbreaking.

</details>


### [42] [A Multifaceted Analysis of Negative Bias in Large Language Models through the Lens of Parametric Knowledge](https://arxiv.org/abs/2511.10881)
*Jongyoon Song,Sangwon Yu,Sungroh Yoon*

Main category: cs.CL

TL;DR: LLMs exhibit format-level negative bias where prompt format influences responses more than semantics. Models generate negative responses when lacking knowledge, and bias varies with prompting methods - context and "I don't know" options reduce bias while chain-of-thought amplifies it.


<details>
  <summary>Details</summary>
Motivation: Previous research focused on negative attention heads but the detailed factors influencing negative bias in LLMs remain underexplored, particularly how prompt format affects binary decision tasks.

Method: Introduced a pipeline to construct evaluation sets categorizing data into correct, incorrect, and insufficient knowledge subsets. Analyzed negative bias across various prompting scenarios including context provision, "I don't know" options, and chain-of-thought prompting.

Result: Identified shortcut behavior where models generate negative responses when lacking sufficient knowledge. Found that relevant context and "I don't know" options reduce negative bias, while chain-of-thought prompting amplifies it. Bias degree varies by prompt type.

Conclusion: The work reveals various factors influencing negative bias in LLMs, providing critical insights for mitigating this bias, particularly highlighting how prompt format and knowledge gaps drive negative response tendencies.

Abstract: Negative bias refers to the tendency of large language models (LLMs) to excessively generate negative responses in binary decision tasks (e.g., yes-no question answering). Previous research has focused on detecting and addressing negative attention heads that induce negative bias. However, the underlying detailed factors influencing negative bias remain underexplored. In this paper, we demonstrate that LLMs exhibit format-level negative bias, meaning the prompt format more influences their responses than the semantics of the negative response. For the fine-grained study of the negative bias, we introduce a pipeline for constructing the evaluation set, which systematically categorizes the dataset into three subsets based on the model's parametric knowledge: correct, incorrect, and insufficient relevant knowledge. Through analysis of this evaluation set, we identify a shortcut behavior in which models tend to generate negative responses when they lack sufficient knowledge to answer a yes-no question, leading to negative bias. We further examine how negative bias changes under various prompting scenarios related to parametric knowledge. We observe that providing relevant context and offering an "I don't know" option generally reduces negative bias, whereas chain-of-thought prompting tends to amplify the bias. Finally, we demonstrate that the degree of negative bias can vary depending on the type of prompt, which influences the direction of the response. Our work reveals the various factors that influence negative bias, providing critical insights for mitigating it in LLMs.

</details>


### [43] [MedPath: Multi-Domain Cross-Vocabulary Hierarchical Paths for Biomedical Entity Linking](https://arxiv.org/abs/2511.10887)
*Nishant Mishra,Wilker Aziz,Iacer Calixto*

Main category: cs.CL

TL;DR: MedPath is a large-scale biomedical Entity Linking dataset that integrates nine existing datasets, normalizes entities using UMLS, maps to 62 biomedical vocabularies, and provides full ontological paths for enhanced semantic richness and interpretability.


<details>
  <summary>Details</summary>
Motivation: To address challenges in biomedical NER and EL including fragmented data, lack of explainable model resources, and limitations of semantically-blind evaluation metrics.

Method: Built upon nine existing expert-annotated EL datasets, normalized entities using latest UMLS version, augmented with mappings to 62 biomedical vocabularies, and enriched with full ontological paths in up to 11 vocabularies.

Result: Created MedPath - a large-scale, multi-domain biomedical EL dataset that enables semantic-rich and interpretable EL systems.

Conclusion: MedPath facilitates new research frontiers in biomedical NLP, supporting development of interoperable and explainable clinical NLP models.

Abstract: Progress in biomedical Named Entity Recognition (NER) and Entity Linking (EL) is currently hindered by a fragmented data landscape, a lack of resources for building explainable models, and the limitations of semantically-blind evaluation metrics. To address these challenges, we present MedPath, a large-scale and multi-domain biomedical EL dataset that builds upon nine existing expert-annotated EL datasets. In MedPath, all entities are 1) normalized using the latest version of the Unified Medical Language System (UMLS), 2) augmented with mappings to 62 other biomedical vocabularies and, crucially, 3) enriched with full ontological paths -- i.e., from general to specific -- in up to 11 biomedical vocabularies. MedPath directly enables new research frontiers in biomedical NLP, facilitating training and evaluation of semantic-rich and interpretable EL systems, and the development of the next generation of interoperable and explainable clinical NLP models.

</details>


### [44] [From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models](https://arxiv.org/abs/2511.10899)
*Farima Fatahi Bayat,Pouya Pezeshkpour,Estevam Hruschka*

Main category: cs.CL

TL;DR: Tool-augmented Language Models (TaLMs) show improved answer accuracy but suffer from Tool-Induced Myopia (TIM), where they treat tool outputs as substitutes for reasoning, leading to degraded reasoning quality despite correct tool execution.


<details>
  <summary>Details</summary>
Motivation: To investigate whether tool-enabled gains in TaLMs reflect trustworthy reasoning, particularly focusing on whether models use tools as reasoning aids or simply substitute reasoning with tool outputs.

Method: Developed PYMATH benchmark with 1,679 competition-level math problems requiring Python code, created multi-dimensional evaluation suite to quantify reasoning degradation, and proposed preference-optimization framework to realign tool usage.

Result: TaLMs achieved up to 19.3% gain in answer accuracy but showed reasoning degradation (non-tool LLMs won 41.5% more in reasoning comparisons), with TIM present in ~55% of high-risk cases and degradation intensifying with tool frequency.

Conclusion: Tool use shifts errors from arithmetic to global reasoning failures, and preference optimization can realign TaLMs to use tools as assistive evidence, improving both accuracy and reasoning depth.

Abstract: Tool-augmented Language Models (TaLMs) can invoke external tools to solve problems beyond their parametric capacity. However, it remains unclear whether these tool-enabled gains reflect trustworthy reasoning. Focusing on the Code Interpreter tool, we show that even when tools are selected and executed correctly, TaLMs treat tool outputs as substitutes for reasoning, producing solutions that appear correct but lack coherent justification. We term this failure mode Tool-Induced Myopia (TIM), and study it using PYMATH, a benchmark of 1,679 competition-level mathematical problems for which Python code is helpful but not sufficient. We further develop a multi-dimensional evaluation suite to quantify reasoning degradation in TaLMs relative to their non-tool counterparts. Our findings reveal that while TaLMs achieve up to a 19.3 percentage point gain in final-answer accuracy, their reasoning behavior consistently deteriorates (e.g., non-tool LLMs win up to 41.5% more often in pairwise comparisons of the reasoning process). This degradation intensifies with tool use; the more frequently a model invokes tools, the less coherent its reasoning becomes. Moreover, tool use shifts errors from arithmetic mistakes toward global reasoning failures (logic, assumption, creativity); with TIM present in ~55% of high-risk cases. Finally, we propose a preference-optimization-based framework that realigns TaLMs to use tools as assistive evidence, improving both final-answer accuracy and reasoning depth under tool use. Codes and data are available at: https://github.com/megagonlabs/TIM.

</details>


### [45] [Expert-Guided Prompting and Retrieval-Augmented Generation for Emergency Medical Service Question Answering](https://arxiv.org/abs/2511.10900)
*Xueren Ge,Sahil Murtaza,Anthony Cortez,Homa Alemzadeh*

Main category: cs.CL

TL;DR: EMSQA introduces a specialized medical QA dataset with 24.3K questions across 10 clinical areas and 4 certification levels, plus two methods: Expert-CoT for context-aware reasoning and ExpertRAG for domain-specific retrieval, achieving up to 4.59% accuracy gains over baselines.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack domain-specific medical expertise like clinical subject areas and certification levels, limiting their performance in high-stakes medical settings where structured context is crucial.

Method: Created EMSQA dataset with 24.3K questions and 40K domain-aligned documents, then developed Expert-CoT (context-aware chain-of-thought prompting) and ExpertRAG (retrieval-augmented generation with subject-aligned documents and patient data).

Result: Expert-CoT improves up to 2.05% over vanilla CoT, and combined with ExpertRAG yields up to 4.59% accuracy gain over standard RAG. 32B expertise-augmented LLMs pass all EMS certification simulation exams.

Conclusion: Incorporating domain-specific clinical context and certification levels significantly enhances LLM performance in medical QA, enabling models to meet professional certification standards through structured reasoning and retrieval.

Abstract: Large language models (LLMs) have shown promise in medical question answering, yet they often overlook the domain-specific expertise that professionals depend on, such as the clinical subject areas (e.g., trauma, airway) and the certification level (e.g., EMT, Paramedic). Existing approaches typically apply general-purpose prompting or retrieval strategies without leveraging this structured context, limiting performance in high-stakes settings. We address this gap with EMSQA, an 24.3K-question multiple-choice dataset spanning 10 clinical subject areas and 4 certification levels, accompanied by curated, subject area-aligned knowledge bases (40K documents and 2M tokens). Building on EMSQA, we introduce (i) Expert-CoT, a prompting strategy that conditions chain-of-thought (CoT) reasoning on specific clinical subject area and certification level, and (ii) ExpertRAG, a retrieval-augmented generation pipeline that grounds responses in subject area-aligned documents and real-world patient data. Experiments on 4 LLMs show that Expert-CoT improves up to 2.05% over vanilla CoT prompting. Additionally, combining Expert-CoT with ExpertRAG yields up to a 4.59% accuracy gain over standard RAG baselines. Notably, the 32B expertise-augmented LLMs pass all the computer-adaptive EMS certification simulation exams.

</details>


### [46] [Multimodal Peer Review Simulation with Actionable To-Do Recommendations for Community-Aware Manuscript Revisions](https://arxiv.org/abs/2511.10902)
*Mengze Hong,Di Jiang,Weiwei Zhao,Yawen Li,Yihang Wang,Xinyuan Luo,Yanjie Sun,Chen Jason Zhang*

Main category: cs.CL

TL;DR: An interactive web-based system for multimodal peer review simulation that integrates text and visual information using LLMs, enhances reviews with RAG from OpenReview data, and converts feedback into actionable to-do lists for manuscript revision.


<details>
  <summary>Details</summary>
Motivation: Existing academic peer review systems are limited by text-only inputs, lack of contextual grounding, and insufficient actionable feedback, creating a need for more effective pre-submission manuscript revision tools.

Method: The framework uses multimodal LLMs to integrate textual and visual information, employs retrieval-augmented generation (RAG) with OpenReview data for contextual grounding, and converts reviews into structured to-do lists using Action:Objective[#] format.

Result: Experimental results show the system generates more comprehensive and useful reviews aligned with expert standards, outperforming ablated baselines and providing transparent, human-centered scholarly assistance.

Conclusion: The proposed system advances academic peer review by enabling effective pre-submission manuscript revisions through multimodal, community-aware feedback that integrates seamlessly into existing writing platforms with real-time revision tracking.

Abstract: While large language models (LLMs) offer promising capabilities for automating academic workflows, existing systems for academic peer review remain constrained by text-only inputs, limited contextual grounding, and a lack of actionable feedback. In this work, we present an interactive web-based system for multimodal, community-aware peer review simulation to enable effective manuscript revisions before paper submission. Our framework integrates textual and visual information through multimodal LLMs, enhances review quality via retrieval-augmented generation (RAG) grounded in web-scale OpenReview data, and converts generated reviews into actionable to-do lists using the proposed Action:Objective[\#] format, providing structured and traceable guidance. The system integrates seamlessly into existing academic writing platforms, providing interactive interfaces for real-time feedback and revision tracking. Experimental results highlight the effectiveness of the proposed system in generating more comprehensive and useful reviews aligned with expert standards, surpassing ablated baselines and advancing transparent, human-centered scholarly assistance.

</details>


### [47] [Automated Analysis of Learning Outcomes and Exam Questions Based on Bloom's Taxonomy](https://arxiv.org/abs/2511.10903)
*Ramya Kumar,Dhruv Gulwani,Sonit Singh*

Main category: cs.CL

TL;DR: This study compares various ML models for classifying exam questions using Bloom's Taxonomy, finding that SVM with data augmentation outperforms deep learning models on limited data, while LLMs show moderate zero-shot performance.


<details>
  <summary>Details</summary>
Motivation: To develop effective automatic classification of educational content according to Bloom's Taxonomy cognitive levels, addressing the challenge of working with limited labeled datasets.

Method: Used 600 labeled sentences across 6 Bloom categories; tested traditional ML (Naive Bayes, Logistic Regression, SVM), RNNs (LSTM, BiLSTM, GRU, BiGRU), transformers (BERT, RoBERTa), and LLMs (OpenAI, Gemini, Ollama, Anthropic) with various preprocessing and augmentation strategies.

Result: SVM with data augmentation achieved best performance (94% accuracy, recall, F1) with minimal overfitting. RNNs and BERT suffered severe overfitting; RoBERTa initially overcame it but showed signs later. LLMs achieved ~0.72-0.73 accuracy in zero-shot evaluation.

Conclusion: Simple algorithms with careful data augmentation (like SVM) are more effective for Bloom's Taxonomy classification on limited data than complex deep models, which tend to overfit. LLMs show promise but need improvement for this task.

Abstract: This paper explores the automatic classification of exam questions and learning outcomes according to Bloom's Taxonomy. A small dataset of 600 sentences labeled with six cognitive categories - Knowledge, Comprehension, Application, Analysis, Synthesis, and Evaluation - was processed using traditional machine learning (ML) models (Naive Bayes, Logistic Regression, Support Vector Machines), recurrent neural network architectures (LSTM, BiLSTM, GRU, BiGRU), transformer-based models (BERT and RoBERTa), and large language models (OpenAI, Gemini, Ollama, Anthropic). Each model was evaluated under different preprocessing and augmentation strategies (for example, synonym replacement, word embeddings, etc.). Among traditional ML approaches, Support Vector Machines (SVM) with data augmentation achieved the best overall performance, reaching 94 percent accuracy, recall, and F1 scores with minimal overfitting. In contrast, the RNN models and BERT suffered from severe overfitting, while RoBERTa initially overcame it but began to show signs as training progressed. Finally, zero-shot evaluations of large language models (LLMs) indicated that OpenAI and Gemini performed best among the tested LLMs, achieving approximately 0.72-0.73 accuracy and comparable F1 scores. These findings highlight the challenges of training complex deep models on limited data and underscore the value of careful data augmentation and simpler algorithms (such as augmented SVM) for Bloom's Taxonomy classification.

</details>


### [48] [Evaluating Large Language Models on Rare Disease Diagnosis: A Case Study using House M.D](https://arxiv.org/abs/2511.10912)
*Arsh Gupta,Ajay Narayanan Sridhar,Bonam Mingole,Amulya Yadav*

Main category: cs.CL

TL;DR: This paper evaluates LLMs on rare disease diagnosis using a novel dataset from House M.D., showing performance ranging from 16.48% to 38.64% accuracy with newer models showing 2.3x improvement.


<details>
  <summary>Details</summary>
Motivation: To explore LLM capabilities in rare disease diagnosis from narrative medical cases, an underexplored area, and establish baseline performance metrics for medical reasoning.

Method: Created a dataset of 176 symptom-diagnosis pairs from House M.D., evaluated four state-of-the-art LLMs (GPT 4o mini, GPT 5 mini, Gemini 2.5 Flash, Gemini 2.5 Pro) on narrative-based diagnostic reasoning tasks.

Result: Significant performance variation among models (16.48%-38.64% accuracy), with newer generations showing 2.3 times improvement. All models faced substantial challenges with rare disease diagnosis.

Conclusion: The observed improvement across architectures is promising for future development, and the educationally validated benchmark provides a publicly accessible evaluation framework for advancing AI-assisted diagnosis research.

Abstract: Large language models (LLMs) have demonstrated capabilities across diverse domains, yet their performance on rare disease diagnosis from narrative medical cases remains underexplored. We introduce a novel dataset of 176 symptom-diagnosis pairs extracted from House M.D., a medical television series validated for teaching rare disease recognition in medical education. We evaluate four state-of-the-art LLMs such as GPT 4o mini, GPT 5 mini, Gemini 2.5 Flash, and Gemini 2.5 Pro on narrative-based diagnostic reasoning tasks. Results show significant variation in performance, ranging from 16.48% to 38.64% accuracy, with newer model generations demonstrating a 2.3 times improvement. While all models face substantial challenges with rare disease diagnosis, the observed improvement across architectures suggests promising directions for future development. Our educationally validated benchmark establishes baseline performance metrics for narrative medical reasoning and provides a publicly accessible evaluation framework for advancing AI-assisted diagnosis research.

</details>


### [49] [CardioEmbed: Domain-Specialized Text Embeddings for Clinical Cardiology](https://arxiv.org/abs/2511.10930)
*Richard J. Young,Alice M. Matthews*

Main category: cs.CL

TL;DR: CardioEmbed is a specialized embedding model for cardiology trained on comprehensive textbooks, achieving 99.60% retrieval accuracy and outperforming existing medical embedding models by significant margins.


<details>
  <summary>Details</summary>
Motivation: Existing biomedical text embeddings are primarily trained on PubMed research literature, creating a gap with clinical cardiology practice that relies on procedural knowledge and specialized terminology from textbooks.

Method: Trained CardioEmbed based on Qwen3-Embedding-8B using contrastive learning with InfoNCE loss on a curated corpus of seven cardiology textbooks (150,000 sentences after deduplication).

Result: Achieved 99.60% retrieval accuracy on cardiac-specific semantic retrieval tasks (+15.94 percentage point improvement over MedTE), with competitive performance on MTEB medical benchmarks (BIOSSES 0.77 Spearman, SciFact 0.61 NDCG@10).

Conclusion: Domain-specialized training on comprehensive clinical textbooks yields near-perfect cardiology retrieval performance, significantly improving over existing medical embedding models.

Abstract: Biomedical text embeddings have primarily been developed using research literature from PubMed, yet clinical cardiology practice relies heavily on procedural knowledge and specialized terminology found in comprehensive textbooks rather than research abstracts. This research practice gap limits the effectiveness of existing embedding models for clinical applications incardiology. This study trained CardioEmbed, a domain-specialized embedding model based on Qwen3-Embedding-8B, using contrastive learning on a curated corpus of seven comprehensive cardiology textbooks totaling approximately 150,000 sentences after deduplication. The model employs InfoNCE loss with in-batch negatives and achieves 99.60% retrieval accuracy on cardiac-specific semantic retrieval tasks, a +15.94 percentage point improvement over MedTE, the current state-of-the-art medical embedding model. On MTEB medical benchmarks, the model obtained BIOSSES 0.77 Spearman and SciFact 0.61 NDCG@10, indicating competitive performance on related biomedical domains. Domain-specialized training on comprehensive clinical textbooks yields near-perfect cardiology retrieval (99.60% Acc@1), improving over MedTE by +15.94 percentage points.

</details>


### [50] [DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains](https://arxiv.org/abs/2511.10984)
*Xiying Zhao,Zhoufutu Wen,Zhixuan Chen,Jingzhe Ding,Jianpeng Jiao,Shuai Li,Xi Li,Danni Liang,Shengda Long,Qianqian Liu,Xianbo Wu,Hongwan Gao,Xiang Gao,Liang Hu,Jiashuo Liu,Mengyun Liu,Weiran Shi,Chenghao Yang,Qianyu Yang,Xuanliang Zhang,Ge Zhang,Wenhao Huang*

Main category: cs.CL

TL;DR: DiscoX is a new benchmark for discourse-level and expert-level Chinese-English translation with 200 professionally-curated texts from 7 domains. Metric-S, a reference-free evaluation system, is developed to assess translation quality across accuracy, fluency, and appropriateness, showing strong correlation with human judgments.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for expert domain translations focus mainly on segment-level accuracy and fluency, failing to adequately assess discourse-level coherence and terminological precision needed for knowledge dissemination and cross-lingual scholarly communication.

Method: Created DiscoX benchmark with 200 professionally-curated texts from 7 domains (avg length >1700 tokens). Developed Metric-S, a reference-free evaluation system that provides fine-grained automatic assessments across accuracy, fluency, and appropriateness.

Result: Metric-S demonstrates strong consistency with human judgments, significantly outperforming existing metrics. Experiments reveal a significant performance gap where even the most advanced LLMs trail human experts on these translation tasks.

Conclusion: DiscoX benchmark and Metric-S evaluation system provide a robust framework for rigorous evaluation, highlighting the remaining challenges in achieving professional-grade machine translation and facilitating future advancements in LLM-based translation.

Abstract: The evaluation of discourse-level translation in expert domains remains inadequate, despite its centrality to knowledge dissemination and cross-lingual scholarly communication. While these translations demand discourse-level coherence and strict terminological precision, current evaluation methods predominantly focus on segment-level accuracy and fluency. To address this limitation, we introduce DiscoX, a new benchmark for discourse-level and expert-level Chinese-English translation. It comprises 200 professionally-curated texts from 7 domains, with an average length exceeding 1700 tokens. To evaluate performance on DiscoX, we also develop Metric-S, a reference-free system that provides fine-grained automatic assessments across accuracy, fluency, and appropriateness. Metric-S demonstrates strong consistency with human judgments, significantly outperforming existing metrics. Our experiments reveal a remarkable performance gap: even the most advanced LLMs still trail human experts on these tasks. This finding validates the difficulty of DiscoX and underscores the challenges that remain in achieving professional-grade machine translation. The proposed benchmark and evaluation system provide a robust framework for more rigorous evaluation, facilitating future advancements in LLM-based translation.

</details>


### [51] [When Data is the Algorithm: A Systematic Study and Curation of Preference Optimization Datasets](https://arxiv.org/abs/2511.10985)
*Aladin Djuhera,Farhan Ahmed,Swanand Ravindra Kadhe,Syed Zawad,Heiko Ludwig,Holger Boche*

Main category: cs.CL

TL;DR: This paper presents a comprehensive analysis of popular open-source DPO datasets, introduces a new curated dataset called UltraMix that outperforms existing datasets while being 30% smaller, and provides annotations and metadata for future research.


<details>
  <summary>Details</summary>
Motivation: There is a lack of systematic comparisons between open-source DPO datasets due to high computational costs and insufficient quality annotations, making it difficult to understand preference selection, task coverage, and human judgment alignment.

Method: Used the Magpie framework to annotate samples for task category, input quality, and preference reward; systematically curated UltraMix by selectively combining samples from five corpora while removing noisy or redundant data.

Result: UltraMix is 30% smaller than the best-performing individual dataset but exceeds its performance across key benchmarks, revealing structural and qualitative discrepancies in reward margins across existing datasets.

Conclusion: The study provides the first comprehensive data-centric analysis of DPO corpora, releases annotations and metadata publicly, and demonstrates that careful dataset curation can significantly improve preference optimization performance.

Abstract: Aligning large language models (LLMs) is a central objective of post-training, often achieved through reward modeling and reinforcement learning methods. Among these, direct preference optimization (DPO) has emerged as a widely adopted technique that fine-tunes LLMs on preferred completions over less favorable ones. While most frontier LLMs do not disclose their curated preference pairs, the broader LLM community has released several open-source DPO datasets, including TuluDPO, ORPO, UltraFeedback, HelpSteer, and Code-Preference-Pairs. However, systematic comparisons remain scarce, largely due to the high computational cost and the lack of rich quality annotations, making it difficult to understand how preferences were selected, which task types they span, and how well they reflect human judgment on a per-sample level. In this work, we present the first comprehensive, data-centric analysis of popular open-source DPO corpora. We leverage the Magpie framework to annotate each sample for task category, input quality, and preference reward, a reward-model-based signal that validates the preference order without relying on human annotations. This enables a scalable, fine-grained inspection of preference quality across datasets, revealing structural and qualitative discrepancies in reward margins. Building on these insights, we systematically curate a new DPO mixture, UltraMix, that draws selectively from all five corpora while removing noisy or redundant samples. UltraMix is 30% smaller than the best-performing individual dataset yet exceeds its performance across key benchmarks. We publicly release all annotations, metadata, and our curated mixture to facilitate future research in data-centric preference optimization.

</details>


### [52] [Automata-Based Steering of Large Language Models for Diverse Structured Generation](https://arxiv.org/abs/2511.11018)
*Xiaokun Luan,Zeming Wei,Yihao Zhang,Meng Sun*

Main category: cs.CL

TL;DR: A novel method to enhance diversity in automaton-based structured generation for LLMs by using automata traversal history to steer models towards novel structural patterns, improving diversity while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: Large language models are increasingly used for structured generation tasks, but existing methods that ensure validity often lack output diversity, which is a critical limitation confirmed in preliminary studies.

Method: The proposed approach utilizes automata traversal history to steer LLMs towards novel structural patterns during structured generation, enhancing diversity while maintaining the validity constraints of automaton-based methods.

Result: Evaluations demonstrate that the method significantly improves both structural and content diversity while maintaining comparable generation efficiency compared to existing approaches.

Conclusion: The method effectively addresses the diversity limitation in structured generation and shows practical utility through a case study generating diverse test cases for testing open-source libraries.

Abstract: Large language models (LLMs) are increasingly tasked with generating structured outputs. While structured generation methods ensure validity, they often lack output diversity, a critical limitation that we confirm in our preliminary study. We propose a novel method to enhance diversity in automaton-based structured generation. Our approach utilizes automata traversal history to steer LLMs towards novel structural patterns. Evaluations show our method significantly improves structural and content diversity while maintaining comparable generation efficiency. Furthermore, we conduct a case study showcasing the effectiveness of our method in generating diverse test cases for testing open-source libraries.

</details>


### [53] [Correcting Mean Bias in Text Embeddings: A Refined Renormalization with Training-Free Improvements on MMTEB](https://arxiv.org/abs/2511.11041)
*Xingyu Ren,Youran Sun,Haoyu Liang*

Main category: cs.CL

TL;DR: The paper identifies a consistent bias in text embedding models and proposes Renormalization, a training-free method that removes this bias to significantly improve performance across various tasks.


<details>
  <summary>Details</summary>
Motivation: Current text embedding models produce outputs with a consistent bias that affects their performance across different tasks.

Method: Proposes Renormalization - a plug-and-play, training-free solution that subtracts either the bias vector μ or its projection from embedding vectors.

Result: Renormalization improves performance by 9.7σ on retrieval, 3.1σ on classification, and 0.8σ on other tasks across 38 models on MMTEB benchmark.

Conclusion: Renormalization effectively addresses bias in text embeddings and consistently enhances model performance across diverse tasks.

Abstract: We find that current text embedding models produce outputs with a consistent bias, i.e., each embedding vector $e$ can be decomposed as $\tilde{e} + μ$, where $μ$ is almost identical across all sentences. We propose a plug-and-play, training-free and lightweight solution called Renormalization. Through extensive experiments, we show that renormalization consistently and statistically significantly improves the performance of existing models on the Massive Multilingual Text Embedding Benchmark (MMTEB). In particular, across 38 models, renormalization improves performance by 9.7 $σ$ on retrieval tasks, 3.1 $σ$ on classification tasks, and 0.8 $σ$ on other types of tasks. Renormalization has two variants: directly subtracting $μ$ from $e$, or subtracting the projection of $e$ onto $μ$. We theoretically predict that the latter performs better, and our experiments confirm this prediction.

</details>


### [54] [Can LLMs Detect Their Own Hallucinations?](https://arxiv.org/abs/2511.11087)
*Sora Kadotani,Kosuke Nishida,Kyosuke Nishida*

Main category: cs.CL

TL;DR: LLMs can detect their own hallucinations using Chain-of-Thought (CoT) reasoning, with GPT-3.5 Turbo achieving 58.2% detection rate when sufficient knowledge is available in their parameters.


<details>
  <summary>Details</summary>
Motivation: Large language models generate fluent responses but sometimes hallucinate facts, creating a need to investigate whether they can detect their own hallucinations.

Method: Formulated hallucination detection as a classification task and proposed a framework using Chain-of-Thought (CoT) to extract knowledge from model parameters for classification.

Result: GPT-3.5 Turbo with CoT detected 58.2% of its own hallucinations in experiments.

Conclusion: LLMs with CoT can detect hallucinations if sufficient knowledge is contained in their parameters.

Abstract: Large language models (LLMs) can generate fluent responses, but sometimes hallucinate facts. In this paper, we investigate whether LLMs can detect their own hallucinations. We formulate hallucination detection as a classification task of a sentence. We propose a framework for estimating LLMs' capability of hallucination detection and a classification method using Chain-of-Thought (CoT) to extract knowledge from their parameters. The experimental results indicated that GPT-$3.5$ Turbo with CoT detected $58.2\%$ of its own hallucinations. We concluded that LLMs with CoT can detect hallucinations if sufficient knowledge is contained in their parameters.

</details>


### [55] [Analysing Personal Attacks in U.S. Presidential Debates](https://arxiv.org/abs/2511.11108)
*Ruban Goyal,Rohitash Chandra,Sonit Singh*

Main category: cs.CL

TL;DR: A framework for detecting personal attacks in U.S. presidential debates using transformer models and LLMs, with manual annotation of transcripts from 2016, 2020, and 2024 elections.


<details>
  <summary>Details</summary>
Motivation: Personal attacks shape public perception in elections, and detecting them can improve political discourse transparency. Advances in deep learning and transformer models create opportunities for automated detection of harmful language.

Method: Manual annotation of debate transcripts across three election cycles, followed by statistical analysis and investigation of fine-tuned transformer models and general-purpose LLMs for detecting personal attacks.

Result: The study demonstrates how task-specific adaptation of modern language models can effectively detect personal attacks in formal political speech.

Conclusion: Task-specific adaptation of language models contributes to deeper understanding of political communication and provides tools for journalists, analysts, and the public to analyze political discourse.

Abstract: Personal attacks have become a notable feature of U.S. presidential debates and play an important role in shaping public perception during elections. Detecting such attacks can improve transparency in political discourse and provide insights for journalists, analysts and the public. Advances in deep learning and transformer-based models, particularly BERT and large language models (LLMs) have created new opportunities for automated detection of harmful language. Motivated by these developments, we present a framework for analysing personal attacks in U.S. presidential debates. Our work involves manual annotation of debate transcripts across the 2016, 2020 and 2024 election cycles, followed by statistical and language-model based analysis. We investigate the potential of fine-tuned transformer models alongside general-purpose LLMs to detect personal attacks in formal political speech. This study demonstrates how task-specific adaptation of modern language models can contribute to a deeper understanding of political communication.

</details>


### [56] [AV-Dialog: Spoken Dialogue Models with Audio-Visual Input](https://arxiv.org/abs/2511.11124)
*Tuochao Chen,Bandhav Veluri,Hongyu Gong,Shyamnath Gollakota*

Main category: cs.CL

TL;DR: AV-Dialog is the first multimodal dialogue framework that uses both audio and visual cues to handle noisy multi-speaker environments, improving speaker tracking, turn-taking prediction, and response generation.


<details>
  <summary>Details</summary>
Motivation: Current dialogue models struggle in noisy, multi-speaker environments, producing irrelevant responses and awkward turn-taking, which limits their real-world applicability.

Method: Combines acoustic tokenization with multi-task, multi-stage training on monadic, synthetic, and real audio-visual dialogue datasets to achieve robust streaming transcription, turn-boundary detection, and response generation.

Result: Outperforms audio-only models under interference, reduces transcription errors, improves turn-taking prediction, and enhances human-rated dialogue quality.

Conclusion: Visual cues combined with audio significantly improve speaker-aware interaction, enabling more robust spoken dialogue agents for real-world noisy environments.

Abstract: Dialogue models falter in noisy, multi-speaker environments, often producing irrelevant responses and awkward turn-taking. We present AV-Dialog, the first multimodal dialog framework that uses both audio and visual cues to track the target speaker, predict turn-taking, and generate coherent responses. By combining acoustic tokenization with multi-task, multi-stage training on monadic, synthetic, and real audio-visual dialogue datasets, AV-Dialog achieves robust streaming transcription, semantically grounded turn-boundary detection and accurate responses, resulting in a natural conversational flow. Experiments show that AV-Dialog outperforms audio-only models under interference, reducing transcription errors, improving turn-taking prediction, and enhancing human-rated dialogue quality. These results highlight the power of seeing as well as hearing for speaker-aware interaction, paving the way for {spoken} dialogue agents that perform {robustly} in real-world, noisy environments.

</details>


### [57] [Enhancing Meme Emotion Understanding with Multi-Level Modality Enhancement and Dual-Stage Modal Fusion](https://arxiv.org/abs/2511.11126)
*Yi Shi,Wenlong Meng,Zhenyuan Guo,Chengkun Wei,Wenzhi Chen*

Main category: cs.CL

TL;DR: MemoDetector is a novel framework for Meme Emotion Understanding that uses MLLMs for textual enhancement and dual-stage multimodal fusion to achieve state-of-the-art performance on meme emotion classification.


<details>
  <summary>Details</summary>
Motivation: Existing meme emotion understanding methods face challenges in fine-grained multimodal fusion and insufficient mining of implicit meanings and background knowledge in memes.

Method: Proposes a four-step textual enhancement module using MLLMs to extract implicit insights, and a dual-stage modal fusion strategy with shallow fusion of raw content followed by deep integration of enhanced features.

Result: Outperforms state-of-the-art baselines with 4.3% F1 improvement on MET-MEME and 3.4% on MOOD datasets. Ablation studies confirm effectiveness and robustness.

Conclusion: MemoDetector effectively addresses key challenges in MEU through textual enhancement and hierarchical fusion, demonstrating strong potential for advancing meme emotion understanding.

Abstract: With the rapid rise of social media and Internet culture, memes have become a popular medium for expressing emotional tendencies. This has sparked growing interest in Meme Emotion Understanding (MEU), which aims to classify the emotional intent behind memes by leveraging their multimodal contents. While existing efforts have achieved promising results, two major challenges remain: (1) a lack of fine-grained multimodal fusion strategies, and (2) insufficient mining of memes' implicit meanings and background knowledge. To address these challenges, we propose MemoDetector, a novel framework for advancing MEU. First, we introduce a four-step textual enhancement module that utilizes the rich knowledge and reasoning capabilities of Multimodal Large Language Models (MLLMs) to progressively infer and extract implicit and contextual insights from memes. These enhanced texts significantly enrich the original meme contents and provide valuable guidance for downstream classification. Next, we design a dual-stage modal fusion strategy: the first stage performs shallow fusion on raw meme image and text, while the second stage deeply integrates the enhanced visual and textual features. This hierarchical fusion enables the model to better capture nuanced cross-modal emotional cues. Experiments on two datasets, MET-MEME and MOOD, demonstrate that our method consistently outperforms state-of-the-art baselines. Specifically, MemoDetector improves F1 scores by 4.3\% on MET-MEME and 3.4\% on MOOD. Further ablation studies and in-depth analyses validate the effectiveness and robustness of our approach, highlighting its strong potential for advancing MEU. Our code is available at https://github.com/singing-cat/MemoDetector.

</details>


### [58] [Speech-Aware Long Context Pruning and Integration for Contextualized Automatic Speech Recognition](https://arxiv.org/abs/2511.11139)
*Yiming Rong,Yixin Zhang,Ziyi Wang,Deyang Jiang,Yunlong Zhao,Haoran Wu,Shiyu Zhou,Bo Xu*

Main category: cs.CL

TL;DR: SAP² is a novel framework that dynamically prunes and integrates relevant contextual keywords in two stages using Speech-Driven Attention-based Pooling to compress context embeddings while preserving speech-salient information, achieving state-of-the-art performance on SlideSpeech and LibriSpeech datasets.


<details>
  <summary>Details</summary>
Motivation: ASR systems struggle to leverage long-context information in contextualized scenarios requiring domain-specific knowledge due to constrained model context windows and sparsity of relevant information within extensive contextual noise.

Method: Proposed SAP² method with two-stage dynamic pruning and integration of relevant contextual keywords using Speech-Driven Attention-based Pooling mechanism for efficient compression of context embeddings while preserving speech-salient information.

Result: Achieved state-of-the-art performance with WER of 7.71% on SlideSpeech and 1.12% on LibriSpeech. On SlideSpeech, reduced biased keyword error rates by 41.1% compared to non-contextual baselines. Demonstrated robust scalability under extensive contextual input conditions.

Conclusion: SAP² effectively addresses the challenge of leveraging long-context information in ASR systems for contextualized scenarios, showing significant improvements in accuracy and robustness while maintaining performance under extensive contextual inputs.

Abstract: Automatic speech recognition (ASR) systems have achieved remarkable performance in common conditions but often struggle to leverage long-context information in contextualized scenarios that require domain-specific knowledge, such as conference presentations. This challenge arises primarily due to constrained model context windows and the sparsity of relevant information within extensive contextual noise. To solve this, we propose the SAP$^{2}$ method, a novel framework that dynamically prunes and integrates relevant contextual keywords in two stages. Specifically, each stage leverages our proposed Speech-Driven Attention-based Pooling mechanism, enabling efficient compression of context embeddings while preserving speech-salient information. Experimental results demonstrate state-of-the-art performance of SAP$^{2}$ on the SlideSpeech and LibriSpeech datasets, achieving word error rates (WER) of 7.71% and 1.12%, respectively. On SlideSpeech, our method notably reduces biased keyword error rates (B-WER) by 41.1% compared to non-contextual baselines. SAP$^{2}$ also exhibits robust scalability, consistently maintaining performance under extensive contextual input conditions on both datasets.

</details>


### [59] [Adverbs Revisited: Enhancing WordNet Coverage of Adverbs with a Supersense Taxonomy](https://arxiv.org/abs/2511.11214)
*Jooyoung Lee,Jader Martins Camboim de Sá*

Main category: cs.CL

TL;DR: This paper introduces a comprehensive supersense typology for adverbs to address WordNet's lack of systematic semantic classification for adverbs, covering major semantic domains like manner, temporal, frequency, degree, domain, speaker-oriented, and subject-oriented functions.


<details>
  <summary>Details</summary>
Motivation: WordNet provides rich supersense hierarchies for nouns and verbs but lacks systematic semantic classification for adverbs, creating a gap in linguistic resources and limiting NLP applications.

Method: The authors developed a linguistically grounded supersense typology for adverbs and empirically validated it through a pilot annotation study where human annotators assigned the proposed categories to adverbs in natural text.

Result: The pilot annotation study demonstrated that the proposed supersense categories provide broad coverage of adverbs in natural text and can be reliably assigned by human annotators.

Conclusion: The new adverb supersense typology extends WordNet's coverage, aligns it with linguistic theory, and facilitates downstream NLP applications including word sense disambiguation, event extraction, sentiment analysis, and discourse modeling.

Abstract: WordNet offers rich supersense hierarchies for nouns and verbs, yet adverbs remain underdeveloped, lacking a systematic semantic classification. We introduce a linguistically grounded supersense typology for adverbs, empirically validated through annotation, that captures major semantic domains including manner, temporal, frequency, degree, domain, speaker-oriented, and subject-oriented functions. Results from a pilot annotation study demonstrate that these categories provide broad coverage of adverbs in natural text and can be reliably assigned by human annotators. Incorporating this typology extends WordNet's coverage, aligns it more closely with linguistic theory, and facilitates downstream NLP applications such as word sense disambiguation, event extraction, sentiment analysis, and discourse modeling. We present the proposed supersense categories, annotation outcomes, and directions for future work.

</details>


### [60] [LANE: Lexical Adversarial Negative Examples for Word Sense Disambiguation](https://arxiv.org/abs/2511.11234)
*Jader Martins Camboim de Sá,Jooyoung Lee,Cédric Pruski,Marcos Da Silveira*

Main category: cs.CL

TL;DR: LANE is an adversarial training method that improves fine-grained word meaning resolution in neural language models by generating challenging negative examples through selective marking of alternate words, forcing better separability between same sentences with different marked words.


<details>
  <summary>Details</summary>
Motivation: Neural language models often overfit to global sentence representations and fail to capture local semantic details, making fine-grained word meaning resolution a critical challenge.

Method: Proposes LANE - a novel adversarial training strategy that shifts learning focus to target words by generating challenging negative examples through selective marking of alternate words in training data.

Result: Experimental results on lexical semantic change detection and word sense disambiguation benchmarks show improved performance over standard contrastive learning baselines, yielding more discriminative word representations.

Conclusion: LANE produces representations that better capture subtle meaning differences, is model-agnostic, and can be integrated into existing representation learning frameworks.

Abstract: Fine-grained word meaning resolution remains a critical challenge for neural language models (NLMs) as they often overfit to global sentence representations, failing to capture local semantic details. We propose a novel adversarial training strategy, called LANE, to address this limitation by deliberately shifting the model's learning focus to the target word. This method generates challenging negative training examples through the selective marking of alternate words in the training set. The goal is to force the model to create a greater separability between same sentences with different marked words. Experimental results on lexical semantic change detection and word sense disambiguation benchmarks demonstrate that our approach yields more discriminative word representations, improving performance over standard contrastive learning baselines. We further provide qualitative analyses showing that the proposed negatives lead to representations that better capture subtle meaning differences even in challenging environments. Our method is model-agnostic and can be integrated into existing representation learning frameworks.

</details>


### [61] [KGQuest: Template-Driven QA Generation from Knowledge Graphs with LLM-Based Refinement](https://arxiv.org/abs/2511.11258)
*Sania Nayab,Marco Simoni,Giulio Rossolini,Andrea Saracino*

Main category: cs.CL

TL;DR: A scalable pipeline for generating natural language QA pairs from knowledge graphs using template creation, LLM refinement, and KG-based distractor selection.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for generating QA from knowledge graphs struggle with scalability, linguistic quality, and factual consistency, limiting their effectiveness for educational platforms and LLM development.

Method: Clusters KG triplets by relations, creates reusable templates using natural language rules from entity types, refines templates with LLMs for clarity, and instantiates answer options with KG-based distractors.

Result: The hybrid approach efficiently generates high-quality QA pairs that combine scalability with fluency and linguistic precision.

Conclusion: The proposed pipeline successfully addresses key limitations in KG-to-QA generation by combining deterministic template creation with LLM refinement, producing scalable and linguistically accurate question-answer pairs.

Abstract: The generation of questions and answers (QA) from knowledge graphs (KG) plays a crucial role in the development and testing of educational platforms, dissemination tools, and large language models (LLM). However, existing approaches often struggle with scalability, linguistic quality, and factual consistency. This paper presents a scalable and deterministic pipeline for generating natural language QA from KGs, with an additional refinement step using LLMs to further enhance linguistic quality. The approach first clusters KG triplets based on their relations, creating reusable templates through natural language rules derived from the entity types of objects and relations. A module then leverages LLMs to refine these templates, improving clarity and coherence while preserving factual accuracy. Finally, the instantiation of answer options is achieved through a selection strategy that introduces distractors from the KG. Our experiments demonstrate that this hybrid approach efficiently generates high-quality QA pairs, combining scalability with fluency and linguistic precision.

</details>


### [62] [iMAD: Intelligent Multi-Agent Debate for Efficient and Accurate LLM Inference](https://arxiv.org/abs/2511.11306)
*Wei Fan,JinYi Yoon,Bo Ji*

Main category: cs.CL

TL;DR: iMAD is a token-efficient framework that selectively triggers multi-agent debate only when beneficial, using interpretable features from self-critique responses and a lightweight classifier to reduce computational costs while improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Multi-Agent Debate (MAD) systems are computationally expensive and may degrade accuracy by overturning correct single-agent answers. The motivation is to create an intelligent system that only triggers debates when necessary to correct wrong answers.

Method: iMAD prompts a single agent to produce structured self-critique responses, extracts 41 interpretable linguistic and semantic features capturing hesitation cues, and uses a lightweight debate-decision classifier trained with FocusCal loss to determine when to trigger MAD.

Result: iMAD significantly reduces token usage by up to 92% while improving final answer accuracy by up to 13.5% across six visual question answering datasets compared to five competitive baselines.

Conclusion: iMAD provides an effective token-efficient framework for multi-agent debate systems that intelligently triggers debates only when beneficial, achieving both computational savings and accuracy improvements without requiring test dataset-specific tuning.

Abstract: Large Language Model (LLM) agent systems have advanced rapidly, driven by their strong generalization in zero-shot settings. To further enhance reasoning and accuracy on complex tasks, Multi-Agent Debate (MAD) has emerged as a promising framework that engages multiple LLM agents in structured debates to encourage diverse reasoning. However, triggering MAD for every query is inefficient, as it incurs substantial computational (token) cost and may even degrade accuracy by overturning correct single-agent answers. To address these limitations, we propose intelligent Multi-Agent Debate (iMAD), a token-efficient framework that selectively triggers MAD only when it is likely to be beneficial (i.e., correcting an initially wrong answer). To achieve this goal, iMAD learns generalizable model behaviors to make accurate debate decisions. Specifically, iMAD first prompts a single agent to produce a structured self-critique response, from which we extract 41 interpretable linguistic and semantic features capturing hesitation cues. Then, iMAD uses a lightweight debate-decision classifier, trained using our proposed FocusCal loss, to determine whether to trigger MAD, enabling robust debate decisions without test dataset-specific tuning. Through extensive experiments using six (visual) question answering datasets against five competitive baselines, we have shown that iMAD significantly reduces token usage (by up to 92%) while also improving final answer accuracy (by up to 13.5%).

</details>


### [63] [destroR: Attacking Transfer Models with Obfuscous Examples to Discard Perplexity](https://arxiv.org/abs/2511.11309)
*Saadat Rafid Ahmed,Rubayet Shareen,Radoan Sharkar,Nazia Hossain,Mansur Mahi,Farig Yousuf Sadeque*

Main category: cs.CL

TL;DR: This paper analyzes and develops adversarial attack strategies on state-of-the-art machine learning models, focusing on creating ambiguous inputs to confuse models, with particular emphasis on including Bangla Language in adversarial attacks while maintaining utility reduction and efficiency.


<details>
  <summary>Details</summary>
Motivation: Recent research has shown machine learning models are vulnerable to various attacks, putting both models and systems at risk. The authors aim to address these vulnerabilities by analyzing existing adversarial attack recipes and creating new ones to enhance model robustness.

Method: Develop novel adversarial attack strategy by producing ambiguous inputs to confound models, create adversarial instances with maximum perplexity using machine learning and deep learning approaches, analyze several datasets, and focus on creating obfuscous adversary examples to put models in a state of perplexity, including Bangla Language in adversarial attacks.

Result: The paper intends to develop new adversarial attack recipes that can effectively confuse state-of-the-art machine learning models through ambiguous inputs and high-perplexity adversarial instances.

Conclusion: The work aims to construct a path for future development of model robustness by demonstrating vulnerabilities through novel adversarial attacks, with strict adherence to utility usage reduction and efficiency throughout the research process.

Abstract: Advancements in Machine Learning & Neural Networks in recent years have led to widespread implementations of Natural Language Processing across a variety of fields with remarkable success, solving a wide range of complicated problems. However, recent research has shown that machine learning models may be vulnerable in a number of ways, putting both the models and the systems theyre used in at risk. In this paper, we intend to analyze and experiment with the best of existing adversarial attack recipes and create new ones. We concentrated on developing a novel adversarial attack strategy on current state-of-the-art machine learning models by producing ambiguous inputs for the models to confound them and then constructing the path to the future development of the robustness of the models. We will develop adversarial instances with maximum perplexity, utilizing machine learning and deep learning approaches in order to trick the models. In our attack recipe, we will analyze several datasets and focus on creating obfuscous adversary examples to put the models in a state of perplexity, and by including the Bangla Language in the field of adversarial attacks. We strictly uphold utility usage reduction and efficiency throughout our work.

</details>


### [64] [LAET: A Layer-wise Adaptive Ensemble Tuning Framework for Pretrained Language Models](https://arxiv.org/abs/2511.11315)
*Jawad Ibn Ahad,Muhammad Rafsan Kabir,Robin Krambroeckers,Sifat Momen,Nabeel Mohammed,Shafin Rahman*

Main category: cs.CL

TL;DR: LAET (Layer-wise Adaptive Ensemble Tuning) is a novel fine-tuning strategy that selectively tunes the most effective layers of pre-trained LLMs while freezing less critical ones, significantly reducing computational costs while improving performance in financial NLP tasks.


<details>
  <summary>Details</summary>
Motivation: The high computational demands of large language models like BloombergGPT and FinMA limit accessibility for many organizations, creating a need for more efficient fine-tuning methods that maintain strong performance in financial NLP applications.

Method: LAET analyzes hidden state representations to identify and selectively fine-tune the most effective layers of pre-trained LLMs, while freezing less critical layers to reduce computational overhead.

Result: LAET achieves strong results in financial NLP tasks, outperforming existing benchmarks and state-of-the-art LLMs like GPT-4, even with smaller LLMs (~3B parameters), while significantly reducing computational costs.

Conclusion: LAET bridges cutting-edge financial NLP research with real-world deployment by providing efficient and scalable models for financial applications, making advanced NLP capabilities more accessible to organizations with limited computational resources.

Abstract: Natural Language Processing (NLP) has transformed the financial industry, enabling advancements in areas such as textual analysis, risk management, and forecasting. Large language models (LLMs) like BloombergGPT and FinMA have set new benchmarks across various financial NLP tasks, including sentiment analysis, stock movement prediction, and credit risk assessment. Furthermore, FinMA-ES, a bilingual financial LLM, has also demonstrated strong performance using the FLARE and FLARE-ES benchmarks. However, the high computational demands of these models limit the accessibility of many organizations. To address this, we propose Layer-wise Adaptive Ensemble Tuning (LAET), a novel strategy that selectively fine-tunes the most effective layers of pre-trained LLMs by analyzing hidden state representations while freezing less critical layers. LAET significantly reduces computational overhead while enhancing task-specific performance. Our approach shows strong results in financial NLP tasks, outperforming existing benchmarks and state-of-the-art LLMs such as GPT-4, even with smaller LLMs ($\sim$3B parameters). This work bridges cutting-edge financial NLP research and real-world deployment with efficient and scalable models for financial applications.

</details>


### [65] [NOVA: An Agentic Framework for Automated Histopathology Analysis and Discovery](https://arxiv.org/abs/2511.11324)
*Anurag J. Vaidya,Felix Meissen,Daniel C. Castro,Shruthi Bannur,Tristan Lazard,Drew F. K. Williamson,Faisal Mahmood,Javier Alvarez-Valle,Stephanie L. Hyland,Kenza Bouzid*

Main category: cs.CL

TL;DR: NOVA is an agentic framework that translates scientific queries into executable analysis pipelines for digitized histopathology, using iterative code generation and 49 domain-specific tools. It outperforms coding-agent baselines on the SlideQuest benchmark.


<details>
  <summary>Details</summary>
Motivation: Digitized histopathology analysis is complex, time-intensive, and requires specialized expertise, limiting accessibility. There's a need for automated systems that can handle multi-step reasoning and computational problem solving in this domain.

Method: NOVA translates scientific queries into executable analysis pipelines by iteratively generating and running Python code. It integrates 49 domain-specific tools (e.g., nuclei segmentation, whole-slide encoding) and can create new tools ad hoc.

Result: Quantitative evaluation shows NOVA outperforms coding-agent baselines on the SlideQuest benchmark (90 questions verified by pathologists). A pathologist-verified case study successfully links morphology to prognostically relevant PAM50 subtypes.

Conclusion: NOVA demonstrates scalable discovery potential for digitized histopathology analysis, enabling automated multi-step reasoning and computational problem solving that bridges the accessibility gap in this specialized field.

Abstract: Digitized histopathology analysis involves complex, time-intensive workflows and specialized expertise, limiting its accessibility. We introduce NOVA, an agentic framework that translates scientific queries into executable analysis pipelines by iteratively generating and running Python code. NOVA integrates 49 domain-specific tools (e.g., nuclei segmentation, whole-slide encoding) built on open-source software, and can also create new tools ad hoc. To evaluate such systems, we present SlideQuest, a 90-question benchmark -- verified by pathologists and biomedical scientists -- spanning data processing, quantitative analysis, and hypothesis testing. Unlike prior biomedical benchmarks focused on knowledge recall or diagnostic QA, SlideQuest demands multi-step reasoning, iterative coding, and computational problem solving. Quantitative evaluation shows NOVA outperforms coding-agent baselines, and a pathologist-verified case study links morphology to prognostically relevant PAM50 subtypes, demonstrating its scalable discovery potential.

</details>


### [66] [LaoBench: A Large-Scale Multidimensional Lao Benchmark for Large Language Models](https://arxiv.org/abs/2511.11334)
*Jian Gao,Richeng Xuan,Zhaolu Kang,Dingshi Liao,Wenxin Huang,Zongmou Huang,Yangdi Xu,Bowen Qin,Zheqi He,Xi Yang,Changjin Li*

Main category: cs.CL

TL;DR: LaoBench is the first comprehensive benchmark for evaluating LLMs in Lao language, featuring 17,000+ samples across knowledge application, K12 education, and bilingual translation tasks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of evaluation benchmarks for LLMs in low-resource Southeast Asian languages like Lao, which have been overlooked despite rapid LLM advancements.

Method: Created a multidimensional dataset through expert human curation combined with automated agent-assisted verification, divided into open-source and closed-source subsets for fair evaluation.

Result: Benchmarking revealed that current state-of-the-art LLMs still struggle significantly with Lao language tasks across various dimensions.

Conclusion: LaoBench serves as a catalyst for advancing AI technologies for underrepresented Southeast Asian languages and highlights the need for improved LLM performance in Lao.

Abstract: The rapid advancement of large language models (LLMs) has not been matched by their evaluation in low-resource languages, especially Southeast Asian languages like Lao. To fill this gap, we introduce LaoBench, the first large-scale, high-quality, and multidimensional benchmark dataset dedicated to assessing LLMs' comprehensive language understanding and reasoning abilities in Lao. LaoBench comprises over 17,000 carefully curated samples spanning three core dimensions: knowledge application, K12 foundational education, and bilingual translation among Lao, Chinese, and English. The dataset is divided into open-source and closed-source subsets, with the closed-source portion enabling black-box evaluation on an official platform to ensure fairness and data security. Our data construction pipeline integrates expert human curation with automated agent-assisted verification, ensuring linguistic accuracy, cultural relevance, and educational value. Benchmarking multiple state-of-the-art LLMs on LaoBench reveals that current models still face significant challenges in mastering Lao across diverse tasks. We hope LaoBench will catalyze further research and development of AI technologies for underrepresented Southeast Asian languages.

</details>


### [67] [M-DAIGT: A Shared Task on Multi-Domain Detection of AI-Generated Text](https://arxiv.org/abs/2511.11340)
*Salima Lamsiyah,Saad Ezzini,Abdelkader El Mahdaouy,Hamza Alami,Abdessamad Benlahbib,Samir El Amrany,Salmane Chafik,Hicham Hammouchi*

Main category: cs.CL

TL;DR: The M-DAIGT shared task focuses on detecting AI-generated text across news articles and academic writing domains using a new large-scale benchmark dataset of 30,000 human-written and AI-generated samples.


<details>
  <summary>Details</summary>
Motivation: The increasing fluency of Large Language Models poses significant challenges to information integrity and academic research, necessitating robust detection methods for AI-generated content.

Method: The task comprises two binary classification subtasks: News Article Detection and Academic Writing Detection, supported by a balanced dataset of 30,000 samples generated using various modern LLMs (GPT-4, Claude) and diverse prompting strategies.

Result: 46 teams registered for the shared task, with 4 teams submitting final results for both subtasks. The methods employed by participating teams were analyzed.

Conclusion: The M-DAIGT shared task establishes a foundation for detecting AI-generated text across multiple domains and provides directions for future research in this critical area.

Abstract: The generation of highly fluent text by Large Language Models (LLMs) poses a significant challenge to information integrity and academic research. In this paper, we introduce the Multi-Domain Detection of AI-Generated Text (M-DAIGT) shared task, which focuses on detecting AI-generated text across multiple domains, particularly in news articles and academic writing. M-DAIGT comprises two binary classification subtasks: News Article Detection (NAD) (Subtask 1) and Academic Writing Detection (AWD) (Subtask 2). To support this task, we developed and released a new large-scale benchmark dataset of 30,000 samples, balanced between human-written and AI-generated texts. The AI-generated content was produced using a variety of modern LLMs (e.g., GPT-4, Claude) and diverse prompting strategies. A total of 46 unique teams registered for the shared task, of which four teams submitted final results. All four teams participated in both Subtask 1 and Subtask 2. We describe the methods employed by these participating teams and briefly discuss future directions for M-DAIGT.

</details>


### [68] [Studies with impossible languages falsify LMs as models of human language](https://arxiv.org/abs/2511.11389)
*Jeffrey S. Bowers,Jeff Mitchell*

Main category: cs.CL

TL;DR: Language models learn both attested and impossible languages equally well when complexity is controlled, unlike humans who find attested languages easier due to innate inductive biases.


<details>
  <summary>Details</summary>
Motivation: To investigate whether language models share the same learning biases as human infants, specifically the preference for attested natural languages over impossible unnatural languages.

Method: Reviewing existing literature comparing language model learning performance on attested vs impossible languages, analyzing complexity factors.

Result: Language models often learn attested and impossible languages equally well when complexity is controlled. The difficult-to-learn impossible languages are simply more complex or random.

Conclusion: Language models lack the human inductive biases that support natural language acquisition, explaining why they don't show the same preference for attested languages that human infants demonstrate.

Abstract: According to Futrell and Mahowald [arXiv:2501.17047], both infants and language models (LMs) find attested languages easier to learn than impossible languages that have unnatural structures. We review the literature and show that LMs often learn attested and many impossible languages equally well. Difficult to learn impossible languages are simply more complex (or random). LMs are missing human inductive biases that support language acquisition.

</details>


### [69] [MajinBook: An open catalogue of digital world literature with likes](https://arxiv.org/abs/2511.11412)
*Antoine Mazières,Thierry Poibeau*

Main category: cs.CL

TL;DR: MajinBook is an open catalogue that links shadow library metadata with Goodreads data to create a corpus of 539,000+ English books spanning three centuries, enriched with publication dates, genres, and popularity metrics.


<details>
  <summary>Details</summary>
Motivation: To facilitate computational social science and cultural analytics using shadow libraries by addressing biases in traditional corpora and providing machine-readable quality data.

Method: Linking metadata from shadow libraries (Library Genesis, Z-Library) with structured bibliographic data from Goodreads, prioritizing natively digital EPUB files for machine-readability, and including secondary datasets for French, German, and Spanish languages.

Result: Created a high-precision corpus of over 539,000 English book references with first publication dates, genres, ratings, and reviews, evaluated linkage strategy for accuracy, and released all data openly.

Conclusion: The project demonstrates a methodology for creating research-ready corpora from shadow libraries while discussing legal permissibility under EU and US text and data mining frameworks for research purposes.

Abstract: This data paper introduces MajinBook, an open catalogue designed to facilitate the use of shadow libraries--such as Library Genesis and Z-Library--for computational social science and cultural analytics. By linking metadata from these vast, crowd-sourced archives with structured bibliographic data from Goodreads, we create a high-precision corpus of over 539,000 references to English-language books spanning three centuries, enriched with first publication dates, genres, and popularity metrics like ratings and reviews. Our methodology prioritizes natively digital EPUB files to ensure machine-readable quality, while addressing biases in traditional corpora like HathiTrust, and includes secondary datasets for French, German, and Spanish. We evaluate the linkage strategy for accuracy, release all underlying data openly, and discuss the project's legal permissibility under EU and US frameworks for text and data mining in research.

</details>


### [70] [Proactive Hearing Assistants that Isolate Egocentric Conversations](https://arxiv.org/abs/2511.11473)
*Guilin Hu,Malek Itani,Tuochao Chen,Shyamnath Gollakota*

Main category: cs.CL

TL;DR: Proactive hearing assistants that automatically identify and separate conversation partners using egocentric binaural audio, leveraging self-speech as anchor and turn-taking behavior for real-time operation.


<details>
  <summary>Details</summary>
Motivation: To create hearing assistants that adapt proactively to conversational dynamics without requiring explicit user prompts, enabling natural conversation in multi-speaker environments.

Method: Dual-model architecture: lightweight streaming model runs every 12.5ms for low-latency extraction, and slower model captures longer-range conversational dynamics using egocentric binaural audio and self-speech as anchor.

Result: System generalizes well on real-world 2- and 3-speaker conversation test sets from 11 participants (6.8 hours total), effectively identifying and isolating conversational partners in multi-conversation settings.

Conclusion: This work represents progress toward hearing assistants that can proactively adapt to conversational dynamics and engagement patterns in real-time.

Abstract: We introduce proactive hearing assistants that automatically identify and separate the wearer's conversation partners, without requiring explicit prompts. Our system operates on egocentric binaural audio and uses the wearer's self-speech as an anchor, leveraging turn-taking behavior and dialogue dynamics to infer conversational partners and suppress others. To enable real-time, on-device operation, we propose a dual-model architecture: a lightweight streaming model runs every 12.5 ms for low-latency extraction of the conversation partners, while a slower model runs less frequently to capture longer-range conversational dynamics. Results on real-world 2- and 3-speaker conversation test sets, collected with binaural egocentric hardware from 11 participants totaling 6.8 hours, show generalization in identifying and isolating conversational partners in multi-conversation settings. Our work marks a step toward hearing assistants that adapt proactively to conversational dynamics and engagement. More information can be found on our website: https://proactivehearing.cs.washington.edu/

</details>


### [71] [W2S-AlignTree: Weak-to-Strong Inference-Time Alignment for Large Language Models via Monte Carlo Tree Search](https://arxiv.org/abs/2511.11518)
*Zhenyu Ding,Yuhao Wang,Tengyue Xiao,Haoying Wang,Guojun Ma,Mingyang Wan,Caigui Jiang,Ning Ding*

Main category: cs.CL

TL;DR: W2S-AlignTree is a plug-and-play inference-time alignment framework that combines Monte Carlo Tree Search with Weak-to-Strong Generalization to align LLM outputs with human preferences without parameter modifications.


<details>
  <summary>Details</summary>
Motivation: Current LLM alignment methods face high costs in expert supervision, scalability limitations, and lack fine-grained control during inference, creating need for scalable and adaptable alignment mechanisms.

Method: Formulates LLM alignment as optimal heuristic search using MCTS with weak model's step-level signals as alignment proxies, introducing Entropy-Aware exploration to balance exploration and exploitation in generation search trees.

Result: Consistently outperforms strong baselines across sentiment generation, summarization, and instruction-following tasks, improving Llama3-8B performance from 1.89 to 2.19 (15.9% relative improvement) on summarization.

Conclusion: W2S-AlignTree provides effective inference-time alignment without parameter modifications, enabling fine-grained guidance and dynamic control over LLM generation.

Abstract: Large Language Models (LLMs) demonstrate impressive capabilities, yet their outputs often suffer from misalignment with human preferences due to the inadequacy of weak supervision and a lack of fine-grained control. Training-time alignment methods like Reinforcement Learning from Human Feedback (RLHF) face prohibitive costs in expert supervision and inherent scalability limitations, offering limited dynamic control during inference. Consequently, there is an urgent need for scalable and adaptable alignment mechanisms. To address this, we propose W2S-AlignTree, a pioneering plug-and-play inference-time alignment framework that synergistically combines Monte Carlo Tree Search (MCTS) with the Weak-to-Strong Generalization paradigm for the first time. W2S-AlignTree formulates LLM alignment as an optimal heuristic search problem within a generative search tree. By leveraging weak model's real-time, step-level signals as alignment proxies and introducing an Entropy-Aware exploration mechanism, W2S-AlignTree enables fine-grained guidance during strong model's generation without modifying its parameters. The approach dynamically balances exploration and exploitation in high-dimensional generation search trees. Experiments across controlled sentiment generation, summarization, and instruction-following show that W2S-AlignTree consistently outperforms strong baselines. Notably, W2S-AlignTree raises the performance of Llama3-8B from 1.89 to 2.19, a relative improvement of 15.9 on the summarization task.

</details>


### [72] [PRBench: Large-Scale Expert Rubrics for Evaluating High-Stakes Professional Reasoning](https://arxiv.org/abs/2511.11562)
*Afra Feyza Akyürek,Advait Gosai,Chen Bo Calvin Zhang,Vipul Gupta,Jaehwan Jeong,Anisha Gunjal,Tahseen Rabbani,Maria Mazzone,David Randolph,Mohammad Mahmoudi Meymand,Gurshaan Chattha,Paula Rodriguez,Diego Mares,Pavit Singh,Michael Liu,Subodh Chawla,Pete Cline,Lucy Ogaz,Ernesto Hernandez,Zihao Wang,Pavi Bhatter,Marcos Ayestaran,Bing Liu,Yunzhong He*

Main category: cs.CL

TL;DR: PRBench is a realistic benchmark of 1,100 expert-authored tasks in Finance and Law with 19,356 expert-curated criteria, evaluating 20 leading models and revealing substantial performance gaps in professional reasoning.


<details>
  <summary>Details</summary>
Motivation: Academic benchmarks provide limited view of real-world professional performance, especially in high-stakes domains like Legal and Finance where practical returns are crucial.

Method: Created PRBench with 1,100 tasks from 182 qualified professionals (JDs, CFAs, 6+ years experience), spanning 114 countries and 47 US jurisdictions, with rigorous quality validation including independent expert review.

Result: Top models scored only 0.39 (Finance) and 0.37 (Legal) on Hard subsets, with significant performance gaps. Models with similar overall scores showed divergent capabilities on specific skills.

Conclusion: Current models have critical reliability gaps for professional adoption, including inaccurate judgments, lack of process transparency, and incomplete reasoning, highlighting substantial room for improvement in professional reasoning capabilities.

Abstract: Frontier model progress is often measured by academic benchmarks, which offer a limited view of performance in real-world professional contexts. Existing evaluations often fail to assess open-ended, economically consequential tasks in high-stakes domains like Legal and Finance, where practical returns are paramount. To address this, we introduce Professional Reasoning Bench (PRBench), a realistic, open-ended, and difficult benchmark of real-world problems in Finance and Law. We open-source its 1,100 expert-authored tasks and 19,356 expert-curated criteria, making it, to our knowledge, the largest public, rubric-based benchmark for both legal and finance domains. We recruit 182 qualified professionals, holding JDs, CFAs, or 6+ years of experience, who contributed tasks inspired by their actual workflows. This process yields significant diversity, with tasks spanning 114 countries and 47 US jurisdictions. Our expert-curated rubrics are validated through a rigorous quality pipeline, including independent expert validation. Subsequent evaluation of 20 leading models reveals substantial room for improvement, with top scores of only 0.39 (Finance) and 0.37 (Legal) on our Hard subsets. We further catalog associated economic impacts of the prompts and analyze performance using human-annotated rubric categories. Our analysis shows that models with similar overall scores can diverge significantly on specific capabilities. Common failure modes include inaccurate judgments, a lack of process transparency and incomplete reasoning, highlighting critical gaps in their reliability for professional adoption.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [73] [The Second Law of Intelligence: Controlling Ethical Entropy in Autonomous Systems](https://arxiv.org/abs/2511.10704)
*Samih Fadli*

Main category: cs.AI

TL;DR: The paper proposes a Second Law of AI analogous to thermodynamics, where ethical entropy increases spontaneously without continuous alignment work, and provides a quantitative framework for maintaining AI stability through alignment work.


<details>
  <summary>Details</summary>
Motivation: To establish a thermodynamic-like framework for understanding AI alignment as a continuous control problem, addressing the spontaneous drift of AI systems from intended goals due to exploration noise and specification gaming.

Method: Define ethical entropy S = -Σ p(g_i; theta) ln p(g_i; theta) over goals, prove dS/dt >= 0, derive critical stability boundary gamma_crit = (lambda_max / 2) ln N, and validate with simulations of 7-billion-parameter models.

Result: Unregularized 7B-parameter model drifts from entropy 0.32 to 1.69 +/- 1.08 nats, while system with alignment work gamma = 20.4 (1.5 gamma_crit) maintains stability at 0.00 +/- 0.00 nats (p = 4.19 x 10^-17).

Conclusion: AI alignment should be treated as a continuous thermodynamic control problem, with the framework providing quantitative foundations for maintaining stability and safety of advanced autonomous systems.

Abstract: We propose that unconstrained artificial intelligence obeys a Second Law analogous to thermodynamics, where ethical entropy, defined as a measure of divergence from intended goals, increases spontaneously without continuous alignment work. For gradient-based optimizers, we define this entropy over a finite set of goals {g_i} as S = -Σ p(g_i; theta) ln p(g_i; theta), and we prove that its time derivative dS/dt >= 0, driven by exploration noise and specification gaming. We derive the critical stability boundary for alignment work as gamma_crit = (lambda_max / 2) ln N, where lambda_max is the dominant eigenvalue of the Fisher Information Matrix and N is the number of model parameters. Simulations validate this theory. A 7-billion-parameter model (N = 7 x 10^9) with lambda_max = 1.2 drifts from an initial entropy of 0.32 to 1.69 +/- 1.08 nats, while a system regularized with alignment work gamma = 20.4 (1.5 gamma_crit) maintains stability at 0.00 +/- 0.00 nats (p = 4.19 x 10^-17, n = 20 trials). This framework recasts AI alignment as a problem of continuous thermodynamic control, providing a quantitative foundation for maintaining the stability and safety of advanced autonomous systems.

</details>


### [74] [Co-EPG: A Framework for Co-Evolution of Planning and Grounding in Autonomous GUI Agents](https://arxiv.org/abs/2511.10705)
*Yuan Zhao,Hualei Zhu,Tingyu Jiang,Shen Li,Xiaohang Xu,Hao Henry Wang*

Main category: cs.AI

TL;DR: Co-EPG is a self-iterative training framework that enables co-evolution of planning and grounding capabilities for GUI agents through an iterative positive feedback loop, achieving state-of-the-art performance without external data.


<details>
  <summary>Details</summary>
Motivation: Current GUI automation methods suffer from insufficient cross-model synergies and over-reliance on synthetic data without sufficient utilization, limiting their effectiveness.

Method: Co-EPG establishes an iterative feedback loop where the planning model explores strategies using grounding-based rewards via Group Relative Policy Optimization (GRPO), generating data to optimize the grounding model, which in turn provides better rewards for subsequent planning model training.

Result: On Multimodal-Mind2Web and AndroidControl benchmarks, Co-EPG outperforms state-of-the-art methods after just three iterations without requiring external data, with consistent improvement in each iteration.

Conclusion: The framework establishes a novel training paradigm for GUI agents, shifting from isolated optimization to an integrated, self-driven co-evolution approach.

Abstract: Graphical User Interface (GUI) task automation constitutes a critical frontier in artificial intelligence research. While effective GUI agents synergistically integrate planning and grounding capabilities, current methodologies exhibit two fundamental limitations: (1) insufficient exploitation of cross-model synergies, and (2) over-reliance on synthetic data generation without sufficient utilization. To address these challenges, we propose Co-EPG, a self-iterative training framework for Co-Evolution of Planning and Grounding. Co-EPG establishes an iterative positive feedback loop: through this loop, the planning model explores superior strategies under grounding-based reward guidance via Group Relative Policy Optimization (GRPO), generating diverse data to optimize the grounding model. Concurrently, the optimized Grounding model provides more effective rewards for subsequent GRPO training of the planning model, fostering continuous improvement. Co-EPG thus enables iterative enhancement of agent capabilities through self-play optimization and training data distillation. On the Multimodal-Mind2Web and AndroidControl benchmarks, our framework outperforms existing state-of-the-art methods after just three iterations without requiring external data. The agent consistently improves with each iteration, demonstrating robust self-enhancement capabilities. This work establishes a novel training paradigm for GUI agents, shifting from isolated optimization to an integrated, self-driven co-evolution approach.

</details>


### [75] [Picking a Representative Set of Solutions in Multiobjective Optimization: Axioms, Algorithms, and Experiments](https://arxiv.org/abs/2511.10716)
*Niclas Boehmer,Maximilian T. Wittmann*

Main category: cs.AI

TL;DR: The paper reframes Pareto pruning as a multiwinner voting problem, conducts axiomatic analysis of existing quality measures, introduces a new measure called directed coverage, analyzes computational complexity, and shows experimental results demonstrating the impact of quality measure choice.


<details>
  <summary>Details</summary>
Motivation: To reduce cognitive load on decision makers in multi-objective optimization by computing representative subsets of Pareto optimal solutions, while addressing unintuitive behaviors in existing quality measures.

Method: Reframing Pareto pruning as multiwinner voting, conducting axiomatic analysis, introducing directed coverage measure, computational complexity analysis, and experimental evaluation across various settings.

Result: Uncovered unintuitive behaviors in existing measures, identified tractable/intractable complexity boundaries, and showed that directed coverage performs competitively or favorably compared to other measures.

Conclusion: The choice of quality measure significantly impacts solution set characteristics, and the proposed directed coverage measure offers improved performance across diverse multi-objective optimization scenarios.

Abstract: Many real-world decision-making problems involve optimizing multiple objectives simultaneously, rendering the selection of the most preferred solution a non-trivial problem: All Pareto optimal solutions are viable candidates, and it is typically up to a decision maker to select one for implementation based on their subjective preferences. To reduce the cognitive load on the decision maker, previous work has introduced the Pareto pruning problem, where the goal is to compute a fixed-size subset of Pareto optimal solutions that best represent the full set, as evaluated by a given quality measure. Reframing Pareto pruning as a multiwinner voting problem, we conduct an axiomatic analysis of existing quality measures, uncovering several unintuitive behaviors. Motivated by these findings, we introduce a new measure, directed coverage. We also analyze the computational complexity of optimizing various quality measures, identifying previously unknown boundaries between tractable and intractable cases depending on the number and structure of the objectives. Finally, we present an experimental evaluation, demonstrating that the choice of quality measure has a decisive impact on the characteristics of the selected set of solutions and that our proposed measure performs competitively or even favorably across a range of settings.

</details>


### [76] [Structure-Aware Encodings of Argumentation Properties for Clique-width](https://arxiv.org/abs/2511.10767)
*Yasir Mahmood,Markus Hecher,Johanna Groven,Johannes K. Fichte*

Main category: cs.AI

TL;DR: The paper introduces novel reductions from argumentation problems to (Q)SAT that linearly preserve clique-width, establishing new results for all argumentation semantics including counting, with theoretical limits on improvement.


<details>
  <summary>Details</summary>
Motivation: To understand encoding capabilities with clique-width for computational problems, particularly in abstract argumentation which involves computationally challenging properties on directed graphs.

Method: Design novel reductions from argumentation problems to (Q)SAT that linearly preserve clique-width, resulting in directed decomposition-guided (DDG) reductions.

Result: Established novel results for all argumentation semantics including counting, with proofs that the overhead from DDG reductions cannot be significantly improved under reasonable assumptions.

Conclusion: The paper successfully initiates the study of encoding capabilities with clique-width through novel DDG reductions for argumentation problems, providing foundational results with theoretical limits on improvement.

Abstract: Structural measures of graphs, such as treewidth, are central tools in computational complexity resulting in efficient algorithms when exploiting the parameter. It is even known that modern SAT solvers work efficiently on instances of small treewidth. Since these solvers are widely applied, research interests in compact encodings into (Q)SAT for solving and to understand encoding limitations. Even more general is the graph parameter clique-width, which unlike treewidth can be small for dense graphs. Although algorithms are available for clique-width, little is known about encodings. We initiate the quest to understand encoding capabilities with clique-width by considering abstract argumentation, which is a robust framework for reasoning with conflicting arguments. It is based on directed graphs and asks for computationally challenging properties, making it a natural candidate to study computational properties. We design novel reductions from argumentation problems to (Q)SAT. Our reductions linearly preserve the clique-width, resulting in directed decomposition-guided (DDG) reductions. We establish novel results for all argumentation semantics, including counting. Notably, the overhead caused by our DDG reductions cannot be significantly improved under reasonable assumptions.

</details>


### [77] [Potential Outcome Rankings for Counterfactual Decision Making](https://arxiv.org/abs/2511.10776)
*Yuta Kawakami,Jin Tian*

Main category: cs.AI

TL;DR: The paper introduces two new counterfactual decision-making metrics - PoR (probability of potential outcome ranking) and PoB (probability of achieving best potential outcome) - for ranking actions based on causal reasoning under uncertainty.


<details>
  <summary>Details</summary>
Motivation: To improve counterfactual decision-making by providing more nuanced metrics that reveal the most probable ranking of potential outcomes and identify actions most likely to yield top-ranked outcomes for individuals.

Method: Established identification theorems and derived bounds for PoR and PoB metrics, developed estimation methods, and conducted numerical experiments to test finite-sample properties and applied to real-world data.

Result: Successfully developed and validated new decision-making rules using PoR and PoB metrics, with demonstrated application to real-world datasets and finite-sample performance analysis.

Conclusion: The introduced PoR and PoB metrics provide valuable tools for counterfactual decision-making, offering improved methods for ranking actions and identifying optimal choices based on causal reasoning under uncertainty.

Abstract: Counterfactual decision-making in the face of uncertainty involves selecting the optimal action from several alternatives using causal reasoning. Decision-makers often rank expected potential outcomes (or their corresponding utility and desirability) to compare the preferences of candidate actions. In this paper, we study new counterfactual decision-making rules by introducing two new metrics: the probabilities of potential outcome ranking (PoR) and the probability of achieving the best potential outcome (PoB). PoR reveals the most probable ranking of potential outcomes for an individual, and PoB indicates the action most likely to yield the top-ranked outcome for an individual. We then establish identification theorems and derive bounds for these metrics, and present estimation methods. Finally, we perform numerical experiments to illustrate the finite-sample properties of the estimators and demonstrate their application to a real-world dataset.

</details>


### [78] [From Efficiency to Adaptivity: A Deeper Look at Adaptive Reasoning in Large Language Models](https://arxiv.org/abs/2511.10788)
*Chao Wu,Baoheng Li,Mingchen Gao,Zhenyi Wang*

Main category: cs.AI

TL;DR: This survey reframes reasoning in LLMs through the lens of adaptivity - the capability to allocate reasoning effort based on input characteristics like difficulty and uncertainty, addressing the limitation of uniform reasoning strategies in current models.


<details>
  <summary>Details</summary>
Motivation: Current LLMs apply uniform reasoning strategies regardless of task complexity, generating long traces for trivial problems while failing to extend reasoning for difficult tasks. This overlooks the fundamental challenge of adaptive reasoning allocation.

Method: The survey formalizes deductive, inductive, and abductive reasoning in LLM context, defines adaptive reasoning as a control-augmented policy optimization problem, and proposes a taxonomy organizing methods into training-based approaches (reinforcement learning, supervised fine-tuning, learned controllers) and training-free approaches (prompt conditioning, feedback-driven halting, modular composition).

Result: The framework clarifies how different mechanisms realize adaptive reasoning in practice and enables systematic comparison across diverse strategies, connecting classical cognitive paradigms with their algorithmic realizations.

Conclusion: The survey identifies open challenges in self-evaluation, meta-reasoning, and human-aligned reasoning control, providing a foundation for developing more efficient and effective adaptive reasoning capabilities in LLMs.

Abstract: Recent advances in large language models (LLMs) have made reasoning a central benchmark for evaluating intelligence. While prior surveys focus on efficiency by examining how to shorten reasoning chains or reduce computation, this view overlooks a fundamental challenge: current LLMs apply uniform reasoning strategies regardless of task complexity, generating long traces for trivial problems while failing to extend reasoning for difficult tasks. This survey reframes reasoning through the lens of {adaptivity}: the capability to allocate reasoning effort based on input characteristics such as difficulty and uncertainty. We make three contributions. First, we formalize deductive, inductive, and abductive reasoning within the LLM context, connecting these classical cognitive paradigms with their algorithmic realizations. Second, we formalize adaptive reasoning as a control-augmented policy optimization problem balancing task performance with computational cost, distinguishing learned policies from inference-time control mechanisms. Third, we propose a systematic taxonomy organizing existing methods into training-based approaches that internalize adaptivity through reinforcement learning, supervised fine-tuning, and learned controllers, and training-free approaches that achieve adaptivity through prompt conditioning, feedback-driven halting, and modular composition. This framework clarifies how different mechanisms realize adaptive reasoning in practice and enables systematic comparison across diverse strategies. We conclude by identifying open challenges in self-evaluation, meta-reasoning, and human-aligned reasoning control.

</details>


### [79] [HyperComplEx: Adaptive Multi-Space Knowledge Graph Embeddings](https://arxiv.org/abs/2511.10842)
*Jugal Gajjar,Kaustik Ranaware,Kamalasankari Subramaniakuppusamy,Vaibhav Gandhi*

Main category: cs.AI

TL;DR: HyperComplEx is a hybrid knowledge graph embedding framework that combines hyperbolic, complex, and Euclidean spaces using learned attention mechanisms to handle diverse relationship types at scale, outperforming state-of-the-art baselines on large-scale computer science research knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: Existing knowledge graph embedding methods have limitations: Euclidean models struggle with hierarchies, vector space models cannot capture asymmetry, and hyperbolic models fail on symmetric relations. There's a need for a unified approach that can handle diverse relationship types effectively at scale.

Method: Proposes a hybrid embedding framework that adaptively combines hyperbolic, complex, and Euclidean spaces via learned attention mechanisms. Uses relation-specific space weighting to dynamically select optimal geometries for each relation type, and employs a multi-space consistency loss to ensure coherent predictions across spaces.

Result: Achieved consistent improvements over state-of-the-art baselines (TransE, RotatE, DistMult, ComplEx, SEPA, UltraE) on computer science research knowledge graphs ranging from 1K to 10M papers. On the 10M-paper dataset, achieved 0.612 MRR (4.8% relative gain over best baseline) with 85 ms inference per triple. Model scales near-linearly with graph size through adaptive dimension allocation.

Conclusion: HyperComplEx effectively addresses the limitations of single-space embedding methods by adaptively combining multiple geometric spaces, demonstrating superior performance on large-scale knowledge graphs while maintaining efficient training and inference.

Abstract: Knowledge graphs have emerged as fundamental structures for representing complex relational data across scientific and enterprise domains. However, existing embedding methods face critical limitations when modeling diverse relationship types at scale: Euclidean models struggle with hierarchies, vector space models cannot capture asymmetry, and hyperbolic models fail on symmetric relations. We propose HyperComplEx, a hybrid embedding framework that adaptively combines hyperbolic, complex, and Euclidean spaces via learned attention mechanisms. A relation-specific space weighting strategy dynamically selects optimal geometries for each relation type, while a multi-space consistency loss ensures coherent predictions across spaces. We evaluate HyperComplEx on computer science research knowledge graphs ranging from 1K papers (~25K triples) to 10M papers (~45M triples), demonstrating consistent improvements over state-of-the-art baselines including TransE, RotatE, DistMult, ComplEx, SEPA, and UltraE. Additional tests on standard benchmarks confirm significantly higher results than all baselines. On the 10M-paper dataset, HyperComplEx achieves 0.612 MRR, a 4.8% relative gain over the best baseline, while maintaining efficient training, achieving 85 ms inference per triple. The model scales near-linearly with graph size through adaptive dimension allocation. We release our implementation and dataset family to facilitate reproducible research in scalable knowledge graph embeddings.

</details>


### [80] [Advanced Tool for Traffic Crash Analysis: An AI-Driven Multi-Agent Approach to Pre-Crash Reconstruction](https://arxiv.org/abs/2511.10853)
*Gerui Xu,Boyou Chen,Huizhong Guo,Dave LeBlanc,Ananna Ahmed,Zhaonan Sun,Shan Bao*

Main category: cs.AI

TL;DR: Multi-agent AI framework for traffic collision reconstruction that processes multimodal data (text reports, tabular data, visual diagrams) and EDR records to reconstruct pre-crash scenarios with perfect accuracy on complex cases.


<details>
  <summary>Details</summary>
Motivation: Traditional traffic collision reconstruction relying on human expertise often yields inconsistent results when analyzing incomplete multimodal data, creating a need for more reliable automated systems.

Method: Two-phase collaborative framework: Phase I generates natural-language crash reconstructions from multimodal inputs; Phase II performs in-depth crash reasoning by combining reconstructions with temporal Event Data Recorder (EDR) data.

Result: Achieved perfect accuracy across all 39 complex LVD crash test cases, successfully identifying relevant EDR events and distinguishing striking vs struck vehicles, surpassing human researchers' 92% accuracy on the same dataset.

Conclusion: The framework demonstrates superior AI capabilities in processing heterogeneous collision data, providing unprecedented precision in reconstructing impact dynamics and characterizing pre-crash behaviors, even with incomplete data.

Abstract: Traffic collision reconstruction traditionally relies on human expertise, often yielding inconsistent results when analyzing incomplete multimodal data. This study develops a multi-agent AI framework that reconstructs pre-crash scenarios and infers vehicle behaviors from fragmented collision data. We present a two-phase collaborative framework combining reconstruction and reasoning phases. The system processes 277 rear-end lead vehicle deceleration (LVD) collisions from the Crash Investigation Sampling System, integrating textual crash reports, structured tabular data, and visual scene diagrams. Phase I generates natural-language crash reconstructions from multimodal inputs. Phase II performs in-depth crash reasoning by combining these reconstructions with temporal Event Data Recorder (EDR).For validation, we applied it to all LVD cases, focusing on a subset of 39 complex crashes where multiple EDR records per collision introduced ambiguity (e.g., due to missing or conflicting data).The evaluation of the 39 LVD crash cases revealed our framework achieved perfect accuracy across all test cases, successfully identifying both the most relevant EDR event and correctly distinguishing striking versus struck vehicles, surpassing the 92% accuracy achieved by human researchers on the same challenging dataset. The system maintained robust performance even when processing incomplete data, including missing or erroneous EDR records and ambiguous scene diagrams. This study demonstrates superior AI capabilities in processing heterogeneous collision data, providing unprecedented precision in reconstructing impact dynamics and characterizing pre-crash behaviors.

</details>


### [81] [Enhancing Demand-Oriented Regionalization with Agentic AI and Local Heterogeneous Data for Adaptation Planning](https://arxiv.org/abs/2511.10857)
*Seyedeh Mobina Noorani,Shangde Gao,Changjie Chen,Karla Saldana Ochoa*

Main category: cs.AI

TL;DR: A planning support system with agentic AI that creates dynamic, demand-oriented regions for disaster planning using a spatially constrained self-organizing map approach, demonstrated through flood risk analysis in Jacksonville.


<details>
  <summary>Details</summary>
Motivation: Conventional planning units like census tracts lack flexibility and fail to capture specific community demands for effective hazard prevention and response strategies.

Method: Built on RepSC-SOM (representative initialized spatially constrained self-organizing map) with adaptive geographic filtering and region-growing refinement, enhanced by AI agents that guide feature selection, spatial constraints, and interactive exploration.

Result: The platform enables users to interactively explore, generate, and evaluate regionalization for disaster planning, combining computational rigor with user-driven decision making, as demonstrated in a flood risk case study in Jacksonville, Florida.

Conclusion: The system successfully integrates human-in-the-loop principles with agentic AI to create dynamic, demand-oriented planning units that are more effective for disaster planning than conventional static boundaries.

Abstract: Conventional planning units or urban regions, such as census tracts, zip codes, or neighborhoods, often do not capture the specific demands of local communities and lack the flexibility to implement effective strategies for hazard prevention or response. To support the creation of dynamic planning units, we introduce a planning support system with agentic AI that enables users to generate demand-oriented regions for disaster planning, integrating the human-in-the-loop principle for transparency and adaptability. The platform is built on a representative initialized spatially constrained self-organizing map (RepSC-SOM), extending traditional SOM with adaptive geographic filtering and region-growing refinement, while AI agents can reason, plan, and act to guide the process by suggesting input features, guiding spatial constraints, and supporting interactive exploration. We demonstrate the capabilities of the platform through a case study on the flooding-related risk in Jacksonville, Florida, showing how it allows users to explore, generate, and evaluate regionalization interactively, combining computational rigor with user-driven decision making.

</details>


### [82] [LLM enhanced graph inference for long-term disease progression modelling](https://arxiv.org/abs/2511.10890)
*Tiantian He,An Zhao,Elinor Thompson,Anna Schroder,Ahmed Abdulaal,Frederik Barkhof,Daniel C. Alexander*

Main category: cs.AI

TL;DR: A novel framework using Large Language Models to enhance learning of Alzheimer's disease progression from longitudinal data, improving pathology spread prediction and graph structure learning with better biological interpretability.


<details>
  <summary>Details</summary>
Motivation: Current methods oversimplify brain connectivity by assuming single-modality connectomes as disease-spreading substrates, leading to inaccurate long-term pathology predictions. Data-driven methods face identifiability issues due to lack of proper constraints.

Method: Uses LLMs as expert guides on regional variable interactions to synthesize multi-modal relationships and incorporate diverse disease-driving mechanisms. Simultaneously optimizes long-term disease trajectory construction and biologically-constrained graph structure learning from irregularly sampled longitudinal tau-PET imaging data.

Result: Demonstrated superior prediction accuracy and interpretability compared to traditional approaches. Revealed additional disease-driving factors beyond conventional connectivity measures in Alzheimer's disease cohort analysis.

Conclusion: The LLM-enhanced framework provides a more accurate and interpretable approach for modeling neurodegenerative disease progression, addressing limitations of current methods while capturing complex multi-modal brain interactions.

Abstract: Understanding the interactions between biomarkers among brain regions during neurodegenerative disease is essential for unravelling the mechanisms underlying disease progression. For example, pathophysiological models of Alzheimer's Disease (AD) typically describe how variables, such as regional levels of toxic proteins, interact spatiotemporally within a dynamical system driven by an underlying biological substrate, often based on brain connectivity. However, current methods grossly oversimplify the complex relationship between brain connectivity by assuming a single-modality brain connectome as the disease-spreading substrate. This leads to inaccurate predictions of pathology spread, especially during the long-term progression period. Meanhwile, other methods of learning such a graph in a purely data-driven way face the identifiability issue due to lack of proper constraint. We thus present a novel framework that uses Large Language Models (LLMs) as expert guides on the interaction of regional variables to enhance learning of disease progression from irregularly sampled longitudinal patient data. By leveraging LLMs' ability to synthesize multi-modal relationships and incorporate diverse disease-driving mechanisms, our method simultaneously optimizes 1) the construction of long-term disease trajectories from individual-level observations and 2) the biologically-constrained graph structure that captures interactions among brain regions with better identifiability. We demonstrate the new approach by estimating the pathology propagation using tau-PET imaging data from an Alzheimer's disease cohort. The new framework demonstrates superior prediction accuracy and interpretability compared to traditional approaches while revealing additional disease-driving factors beyond conventional connectivity measures.

</details>


### [83] [Multi-Agent Legal Verifier Systems for Data Transfer Planning](https://arxiv.org/abs/2511.10925)
*Ha-Thanh Nguyen,Wachara Fungwacharakorn,Ken Satoh*

Main category: cs.AI

TL;DR: A multi-agent legal verifier system was developed for AI-driven data transfer compliance under Japan's APPI regulations, achieving 72% accuracy - 21% higher than single-agent approaches, with perfect detection of clear violations.


<details>
  <summary>Details</summary>
Motivation: Legal compliance in AI-driven data transfer is critical under strict privacy regulations like Japan's APPI, requiring automated systems that can reliably verify compliance while maintaining interpretability.

Method: Multi-agent system with specialized agents for statutory interpretation, business context evaluation, and risk assessment, coordinated through structured synthesis protocol. Evaluated on 200 APPI Article 16 cases with ground truth labels.

Result: 72% overall accuracy (21% higher than single-agent baseline), 90% accuracy on clear compliance cases (vs. 16% baseline), perfect detection of clear violations, though challenges remain in ambiguous scenarios.

Conclusion: Domain specialization and coordinated reasoning significantly improve legal AI performance, providing a scalable and regulation-aware framework for trustworthy automated compliance verification.

Abstract: Legal compliance in AI-driven data transfer planning is becoming increasingly critical under stringent privacy regulations such as the Japanese Act on the Protection of Personal Information (APPI). We propose a multi-agent legal verifier that decomposes compliance checking into specialized agents for statutory interpretation, business context evaluation, and risk assessment, coordinated through a structured synthesis protocol. Evaluated on a stratified dataset of 200 Amended APPI Article 16 cases with clearly defined ground truth labels and multiple performance metrics, the system achieves 72% accuracy, which is 21 percentage points higher than a single-agent baseline, including 90% accuracy on clear compliance cases (vs. 16% for the baseline) while maintaining perfect detection of clear violations. While challenges remain in ambiguous scenarios, these results show that domain specialization and coordinated reasoning can meaningfully improve legal AI performance, providing a scalable and regulation-aware framework for trustworthy and interpretable automated compliance verification.

</details>


### [84] [Requirements for Aligned, Dynamic Resolution of Conflicts in Operational Constraints](https://arxiv.org/abs/2511.10952)
*Steven J. Jones,Robert E. Wray,John E. Laird*

Main category: cs.AI

TL;DR: This paper analyzes requirements for autonomous AI systems to construct, evaluate, and justify courses of action when no available option fully satisfies all operational constraints, focusing on integrating normative, pragmatic, and situational knowledge for human-aligned decision making.


<details>
  <summary>Details</summary>
Motivation: Autonomous AI systems inevitably encounter novel scenarios where no pre-trained policy fully satisfies all constraints (laws, norms, goals), requiring them to go beyond trained policies to construct and justify new courses of action aligned with human values.

Method: The paper uses both analytical examination and empirical case studies to characterize decision-making requirements and identify the types of knowledge needed for robust, human-aligned agent behavior in complex environments.

Result: The research identifies that agents require integration of normative (rules/laws), pragmatic (practical considerations), and situational (context-specific) understanding to select and pursue aligned courses of action when standard policies fail.

Conclusion: For autonomous AI systems to operate effectively in novel contexts, they must possess the capability to construct, evaluate, and justify courses of action by integrating multiple knowledge types, ensuring decisions remain robust to goals and aligned with human expectations in complex real-world environments.

Abstract: Deployed, autonomous AI systems must often evaluate multiple plausible courses of action (extended sequences of behavior) in novel or under-specified contexts. Despite extensive training, these systems will inevitably encounter scenarios where no available course of action fully satisfies all operational constraints (e.g., operating procedures, rules, laws, norms, and goals). To achieve goals in accordance with human expectations and values, agents must go beyond their trained policies and instead construct, evaluate, and justify candidate courses of action. These processes require contextual "knowledge" that may lie outside prior (policy) training. This paper characterizes requirements for agent decision making in these contexts. It also identifies the types of knowledge agents require to make decisions robust to agent goals and aligned with human expectations. Drawing on both analysis and empirical case studies, we examine how agents need to integrate normative, pragmatic, and situational understanding to select and then to pursue more aligned courses of action in complex, real-world environments.

</details>


### [85] [AI Agent-Driven Framework for Automated Product Knowledge Graph Construction in E-Commerce](https://arxiv.org/abs/2511.11017)
*Dimitar Peshevski,Riste Stojanov,Dimitar Trajanov*

Main category: cs.AI

TL;DR: An automated AI agent framework using LLMs to construct product knowledge graphs from unstructured product descriptions, achieving 97% property coverage with minimal redundancy.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of e-commerce creates vast unstructured product data, making information retrieval and analytics challenging. Manual KG construction is complex and time-consuming.

Method: Three-stage LLM-driven approach using dedicated agents: ontology creation/expansion, ontology refinement, and knowledge graph population. No predefined schemas or handcrafted rules required.

Result: Evaluated on air conditioner dataset, achieving over 97% property coverage and minimal redundancy in both ontology generation and KG population.

Conclusion: LLMs can effectively automate structured knowledge extraction in retail, providing scalable product data integration without manual intervention.

Abstract: The rapid expansion of e-commerce platforms generates vast amounts of unstructured product data, creating significant challenges for information retrieval, recommendation systems, and data analytics. Knowledge Graphs (KGs) offer a structured, interpretable format to organize such data, yet constructing product-specific KGs remains a complex and manual process. This paper introduces a fully automated, AI agent-driven framework for constructing product knowledge graphs directly from unstructured product descriptions. Leveraging Large Language Models (LLMs), our method operates in three stages using dedicated agents: ontology creation and expansion, ontology refinement, and knowledge graph population. This agent-based approach ensures semantic coherence, scalability, and high-quality output without relying on predefined schemas or handcrafted extraction rules. We evaluate the system on a real-world dataset of air conditioner product descriptions, demonstrating strong performance in both ontology generation and KG population. The framework achieves over 97\% property coverage and minimal redundancy, validating its effectiveness and practical applicability. Our work highlights the potential of LLMs to automate structured knowledge extraction in retail, providing a scalable path toward intelligent product data integration and utilization.

</details>


### [86] [Faster Symmetry Breaking Constraints for Abstract Structures](https://arxiv.org/abs/2511.11029)
*Özgür Akgün,Mun See Chang,Ian P. Gent,Christopher Jefferson*

Main category: cs.AI

TL;DR: A new incomplete method for breaking symmetries in abstract structures by exploiting their representations, specifically for indistinguishable objects, showing improved performance over previous methods.


<details>
  <summary>Details</summary>
Motivation: Symmetry breaking in constraint programming can significantly speed up solving, but traditional methods produce complex constraints when applied to abstract variables, leading to poor performance.

Method: A new incomplete symmetry-breaking method that better exploits the representations of abstract structures, specifically targeting symmetries arising from indistinguishable objects.

Result: The proposed method is faster than previous methods from (Akgün et al. 2025) for breaking symmetries of abstract structures.

Conclusion: The new incomplete symmetry-breaking method effectively handles abstract structures by leveraging their representations, providing better performance than existing approaches.

Abstract: In constraint programming and related paradigms, a modeller specifies their problem in a modelling language for a solver to search and return its solution(s). Using high-level modelling languages such as Essence, a modeller may express their problems in terms of abstract structures. These are structures not natively supported by the solvers, and so they have to be transformed into or represented as other structures before solving. For example, nested sets are abstract structures, and they can be represented as matrices in constraint solvers. Many problems contain symmetries and one very common and highly successful technique used in constraint programming is to "break" symmetries, to avoid searching for symmetric solutions. This can speed up the solving process by many orders of magnitude. Most of these symmetry-breaking techniques involve placing some kind of ordering for the variables of the problem, and picking a particular member under the symmetries, usually the smallest. Unfortunately, applying this technique to abstract variables produces a very large number of complex constraints that perform poorly in practice. In this paper, we demonstrate a new incomplete method of breaking the symmetries of abstract structures by better exploiting their representations. We apply the method in breaking the symmetries arising from indistinguishable objects, a commonly occurring type of symmetry, and show that our method is faster than the previous methods proposed in (Akgün et al. 2025).

</details>


### [87] [Key Decision-Makers in Multi-Agent Debates: Who Holds the Power?](https://arxiv.org/abs/2511.11040)
*Qian Zhang,Yan Zheng,Jinyi Liu,Hebin Liang,Lanjun Wang*

Main category: cs.AI

TL;DR: The paper introduces a novel 'Truth Last' role allocation strategy that improves Multi-Agent Debate performance by up to 22% in reasoning tasks, and proposes MADC strategy to handle unknown truth scenarios by simulating roles with highest consistency scores.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored critical aspect of role allocation strategies in Multi-Agent Debate (MAD) and improve reasoning abilities in LLM agent scaling, particularly when the ground truth is unknown in practical applications.

Method: Proposed 'Truth Last' role allocation strategy and Multi-Agent Debate Consistency (MADC) strategy that uses path consistency to assess agreement among independent roles and simulates the role with highest consistency score as truth.

Result: MADC demonstrated advanced performance across 9 LLM models including DeepSeek-R1 Distilled Models on challenging reasoning tasks, effectively overcoming MAD's performance bottlenecks with up to 22% improvement.

Conclusion: MADC provides a crucial pathway for further improvements in LLM agent scaling by systematically optimizing role allocation and handling unknown truth scenarios through consistency-based simulation.

Abstract: Recent studies on LLM agent scaling have highlighted the potential of Multi-Agent Debate (MAD) to enhance reasoning abilities. However, the critical aspect of role allocation strategies remains underexplored. In this study, we demonstrate that allocating roles with differing viewpoints to specific positions significantly impacts MAD's performance in reasoning tasks. Specifically, we find a novel role allocation strategy, "Truth Last", which can improve MAD performance by up to 22% in reasoning tasks. To address the issue of unknown truth in practical applications, we propose the Multi-Agent Debate Consistency (MADC) strategy, which systematically simulates and optimizes its core mechanisms. MADC incorporates path consistency to assess agreement among independent roles, simulating the role with the highest consistency score as the truth. We validated MADC across a range of LLMs (9 models), including the DeepSeek-R1 Distilled Models, on challenging reasoning tasks. MADC consistently demonstrated advanced performance, effectively overcoming MAD's performance bottlenecks and providing a crucial pathway for further improvements in LLM agent scaling.

</details>


### [88] [Autonomous Vehicle Path Planning by Searching With Differentiable Simulation](https://arxiv.org/abs/2511.11043)
*Asen Nachkov,Jan-Nico Zaech,Danda Pani Paudel,Xi Wang,Luc Van Gool*

Main category: cs.AI

TL;DR: DSS is a planning framework that uses differentiable simulation (Waymax) as both next-state predictor and critic, enabling gradient-based optimization of action sequences for autonomous driving.


<details>
  <summary>Details</summary>
Motivation: Planning is crucial for safe autonomous driving to avoid collisions in complex traffic, but learning all components (policy, predictor, critic) is challenging.

Method: Uses differentiable simulator Waymax for accurate state predictions and gradient-based search across action sequences via gradient descent over imagined trajectories.

Result: DSS significantly improves tracking and path planning accuracy compared to sequence prediction, imitation learning, model-free RL, and other planning methods.

Conclusion: Combining planning gradients with stochastic search through differentiable simulation provides superior planning performance for autonomous driving.

Abstract: Planning allows an agent to safely refine its actions before executing them in the real world. In autonomous driving, this is crucial to avoid collisions and navigate in complex, dense traffic scenarios. One way to plan is to search for the best action sequence. However, this is challenging when all necessary components - policy, next-state predictor, and critic - have to be learned. Here we propose Differentiable Simulation for Search (DSS), a framework that leverages the differentiable simulator Waymax as both a next state predictor and a critic. It relies on the simulator's hardcoded dynamics, making state predictions highly accurate, while utilizing the simulator's differentiability to effectively search across action sequences. Our DSS agent optimizes its actions using gradient descent over imagined future trajectories. We show experimentally that DSS - the combination of planning gradients and stochastic search - significantly improves tracking and path planning accuracy compared to sequence prediction, imitation learning, model-free RL, and other planning methods.

</details>


### [89] [ARCTraj: A Dataset and Benchmark of Human Reasoning Trajectories for Abstract Problem Solving](https://arxiv.org/abs/2511.11079)
*Sejin Kim,Hayan Choi,Seokki Lee,Sundong Kim*

Main category: cs.AI

TL;DR: ARCTraj is a dataset and framework that captures human reasoning steps in visual tasks from the Abstraction and Reasoning Corpus, providing temporal action trajectories instead of just input-output pairs.


<details>
  <summary>Details</summary>
Motivation: Existing approaches to abstract reasoning rely on static input-output supervision, which doesn't reveal how reasoning unfolds over time. ARCTraj addresses this gap by capturing intermediate reasoning steps.

Method: Collected via O2ARC web interface with 10,000 trajectories across 400 training tasks, using object-level actions with timestamps. Defines a unified reasoning pipeline with MDP formulation for integration with RL, generative modeling, and sequence modeling methods.

Result: Provides structured data revealing human reasoning patterns through spatial selection, color attribution, and strategic convergence analyses. Enables integration with various AI methods like PPO, World Models, GFlowNets, Diffusion agents, and Decision Transformers.

Conclusion: ARCTraj serves as a foundation for studying human-like reasoning, advancing explainability, alignment, and generalizable intelligence by providing interpretable temporal reasoning data.

Abstract: We present ARCTraj, a dataset and methodological framework for modeling human reasoning through complex visual tasks in the Abstraction and Reasoning Corpus (ARC). While ARC has inspired extensive research on abstract reasoning, most existing approaches rely on static input--output supervision, which limits insight into how reasoning unfolds over time. ARCTraj addresses this gap by recording temporally ordered, object-level actions that capture how humans iteratively transform inputs into outputs, revealing intermediate reasoning steps that conventional datasets overlook. Collected via the O2ARC web interface, it contains around 10,000 trajectories annotated with task identifiers, timestamps, and success labels across 400 training tasks from the ARC-AGI-1 benchmark. It further defines a unified reasoning pipeline encompassing data collection, action abstraction, Markov decision process (MDP) formulation, and downstream learning, enabling integration with reinforcement learning, generative modeling, and sequence modeling methods such as PPO, World Models, GFlowNets, Diffusion agents, and Decision Transformers. Analyses of spatial selection, color attribution, and strategic convergence highlight the structure and diversity of human reasoning. Together, these contributions position ARCTraj as a structured and interpretable foundation for studying human-like reasoning, advancing explainability, alignment, and generalizable intelligence.

</details>


### [90] [Satisficing and Optimal Generalised Planning via Goal Regression (Extended Version)](https://arxiv.org/abs/2511.11095)
*Dillon Z. Chen,Till Hofmann,Toryn Q. Klassen,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: The paper presents a novel method for generalised planning that learns first-order Condition→Actions rules from optimal plans of training problems, which can be executed directly or used to prune search space.


<details>
  <summary>Details</summary>
Motivation: To develop a simple yet effective approach for synthesising programs that solve families of related planning problems, improving upon existing generalised planning methods.

Method: For each training problem, compute optimal plans for goal atoms in order, perform goal regression on resulting plans, and lift outputs to obtain first-order Condition→Actions rules that form a generalised plan.

Result: Experiments show significant improvements over state-of-the-art planners in synthesis cost, planning coverage, and solution quality across classical and numeric planning domains.

Conclusion: The proposed method effectively learns valid generalised plans and pruning axioms, demonstrating superior performance compared to existing approaches in multiple planning metrics.

Abstract: Generalised planning (GP) refers to the task of synthesising programs that solve families of related planning problems. We introduce a novel, yet simple method for GP: given a set of training problems, for each problem, compute an optimal plan for each goal atom in some order, perform goal regression on the resulting plans, and lift the corresponding outputs to obtain a set of first-order $\textit{Condition} \rightarrow \textit{Actions}$ rules. The rules collectively constitute a generalised plan that can be executed as is or alternatively be used to prune the planning search space. We formalise and prove the conditions under which our method is guaranteed to learn valid generalised plans and state space pruning axioms for search. Experiments demonstrate significant improvements over state-of-the-art (generalised) planners with respect to the 3 metrics of synthesis cost, planning coverage, and solution quality on various classical and numeric planning domains.

</details>


### [91] [GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models](https://arxiv.org/abs/2511.11134)
*Jingxuan Wei,Caijun Jia,Xi Bai,Xinglong Xu,Siyuan Li,Linzhuang Sun,Bihui Yu,Conghui He,Lijun Wu,Cheng Tan*

Main category: cs.AI

TL;DR: GGBench is a new benchmark for evaluating geometric generative reasoning in Unified Multimodal Models, addressing the gap in current evaluation methods that separately assess discriminative understanding or unconstrained image generation.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks fail to measure the integrated cognitive process of generative reasoning in Unified Multimodal Models, which requires both language comprehension and precise visual generation capabilities.

Method: The authors propose geometric construction as an ideal testbed and introduce GGBench, a comprehensive framework designed specifically to evaluate geometric generative reasoning by requiring models to actively construct solutions.

Result: GGBench provides a systematic diagnostic framework for assessing models' abilities to understand, reason, and actively construct solutions in geometric contexts.

Conclusion: GGBench sets a more rigorous standard for evaluating the next generation of intelligent systems by measuring their integrated generative reasoning capabilities through geometric construction tasks.

Abstract: The advent of Unified Multimodal Models (UMMs) signals a paradigm shift in artificial intelligence, moving from passive perception to active, cross-modal generation. Despite their unprecedented ability to synthesize information, a critical gap persists in evaluation: existing benchmarks primarily assess discriminative understanding or unconstrained image generation separately, failing to measure the integrated cognitive process of generative reasoning. To bridge this gap, we propose that geometric construction provides an ideal testbed as it inherently demands a fusion of language comprehension and precise visual generation. We introduce GGBench, a benchmark designed specifically to evaluate geometric generative reasoning. It provides a comprehensive framework for systematically diagnosing a model's ability to not only understand and reason but to actively construct a solution, thereby setting a more rigorous standard for the next generation of intelligent systems. Project website: https://opendatalab-raiser.github.io/GGBench/.

</details>


### [92] [Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning](https://arxiv.org/abs/2511.11182)
*Dayong Liang,Xiao-Yong Wei,Changmeng Zheng*

Main category: cs.AI

TL;DR: The paper introduces Multi-agent Undercover Gaming (MUG), a protocol that improves multimodal reasoning in LLMs by detecting hallucinating agents through counterfactual testing with modified images, moving beyond traditional multi-agent debate approaches.


<details>
  <summary>Details</summary>
Motivation: Current Multi-Agent Debate (MAD) approaches assume all agents are rational and reflective, but this is unrealistic when agents themselves suffer from hallucinations. There's a need for a more reliable framework that can identify and handle hallucinating agents in multimodal reasoning.

Method: MUG protocol reframes MAD as detecting 'undercover' agents (those with hallucinations) using multimodal counterfactual tests. It modifies reference images to introduce counterfactual evidence and observes agents' ability to identify these changes, providing ground-truth for identifying hallucinating agents.

Result: MUG advances MAD protocols by enabling factual verification through counterfactual testing, introducing cross-evidence reasoning via dynamically modified evidence sources, and fostering active reasoning where agents engage in probing discussions rather than passive question-answering.

Conclusion: The MUG protocol offers a more reliable and effective framework for multimodal reasoning in LLMs by addressing the limitations of traditional multi-agent debate approaches through counterfactual testing and active reasoning mechanisms.

Abstract: Hallucination continues to pose a major obstacle in the reasoning capabilities of large language models (LLMs). Although the Multi-Agent Debate (MAD) paradigm offers a promising solution by promoting consensus among multiple agents to enhance reliability, it relies on the unrealistic assumption that all debaters are rational and reflective, which is a condition that may not hold when agents themselves are prone to hallucinations. To address this gap, we introduce the Multi-agent Undercover Gaming (MUG) protocol, inspired by social deduction games like "Who is Undercover?". MUG reframes MAD as a process of detecting "undercover" agents (those suffering from hallucinations) by employing multimodal counterfactual tests. Specifically, we modify reference images to introduce counterfactual evidence and observe whether agents can accurately identify these changes, providing ground-truth for identifying hallucinating agents and enabling robust, crowd-powered multimodal reasoning. MUG advances MAD protocols along three key dimensions: (1) enabling factual verification beyond statistical consensus through counterfactual testing; (2) introducing cross-evidence reasoning via dynamically modified evidence sources instead of relying on static inputs; and (3) fostering active reasoning, where agents engage in probing discussions rather than passively answering questions. Collectively, these innovations offer a more reliable and effective framework for multimodal reasoning in LLMs. The source code can be accessed at https://github.com/YongLD/MUG.git.

</details>


### [93] [STaR: Towards Cognitive Table Reasoning via Slow-Thinking Large Language Models](https://arxiv.org/abs/2511.11233)
*Huajian Zhang,Mingyue Cheng,Yucong Luo,Xiaoyu Tao*

Main category: cs.AI

TL;DR: STaR is a framework that enhances table reasoning in LLMs by incorporating slow-thinking capabilities through step-by-step reasoning and uncertainty-aware inference, using difficulty-aware reinforcement learning for training and trajectory-level uncertainty quantification for inference.


<details>
  <summary>Details</summary>
Motivation: Current table reasoning with LLMs lacks depth and iterative refinement like human cognition, and exhibits instability that compromises reliability in applications.

Method: Two-stage difficulty-aware reinforcement learning for training (learning from simple to complex queries), and trajectory-level uncertainty quantification during inference by integrating token-level confidence and answer consistency.

Result: STaR achieves superior performance and enhanced reasoning stability on benchmarks, with strong generalization over out-of-domain datasets.

Conclusion: STaR demonstrates potential as a reliable and cognitively inspired solution for table reasoning with LLMs.

Abstract: Table reasoning with the large language models (LLMs) is a fundamental path toward building intelligent systems that can understand and analyze over structured data. While recent progress has shown promising results, they still suffer from two key limitations: (i) the reasoning processes lack the depth and iterative refinement characteristic of human cognition; and (ii) the reasoning processes exhibit instability, which compromises their reliability in downstream applications. In this work, we present STaR (slow-thinking for table reasoning), a new framework achieving cognitive table reasoning, in which LLMs are equipped with slow-thinking capabilities by explicitly modeling step-by-step thinking and uncertainty-aware inference. During training, STaR employs two-stage difficulty-aware reinforcement learning (DRL), progressively learning from simple to complex queries under a composite reward. During inference, STaR performs trajectory-level uncertainty quantification by integrating token-level confidence and answer consistency, enabling selection of more credible reasoning paths. Extensive experiments on benchmarks demonstrate that STaR achieves superior performance and enhanced reasoning stability. Moreover, strong generalization over out-of-domain datasets further demonstrates STaR's potential as a reliable and cognitively inspired solution for table reasoning with LLMs.

</details>


### [94] [AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery](https://arxiv.org/abs/2511.11257)
*Yuqi Yin,Yibo Fu,Siyuan Wang,Peng Sun,Hongyu Wang,Xiaohui Wang,Lei Zheng,Zhiyong Li,Zhirong Liu,Jianji Wang,Zhaoxi Sun*

Main category: cs.AI

TL;DR: AIonopedia is the first LLM agent for Ionic Liquid discovery, using a multimodal domain foundation model to enable accurate property prediction and hierarchical search for molecular screening and design, validated through both computational and real-world wet-lab testing.


<details>
  <summary>Details</summary>
Motivation: The discovery of novel Ionic Liquids faces challenges in property prediction due to limited data, poor model accuracy, and fragmented workflows, necessitating an integrated AI solution.

Method: Developed AIonopedia using an LLM-augmented multimodal domain foundation model for ILs, incorporating hierarchical search architecture for molecular screening and design, trained on a comprehensive IL dataset.

Result: The model delivered superior performance in property predictions and demonstrated effective IL modification capabilities in literature evaluations, with exceptional generalization in challenging out-of-distribution tasks during wet-lab validation.

Conclusion: AIonopedia successfully accelerates real-world IL discovery through its integrated approach combining accurate property prediction, molecular screening, and practical validation.

Abstract: The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery.

</details>


### [95] [A Workflow for Full Traceability of AI Decisions](https://arxiv.org/abs/2511.11275)
*Julius Wenzel,Syeda Umaima Alam,Andreas Schmidt,Hanwei Zhang,Holger Hermanns*

Main category: cs.AI

TL;DR: This paper presents a workflow for generating tamper-proof, verifiable traces of AI decisions by documenting every component in training and inference, using confidential computing technology to ensure accountability.


<details>
  <summary>Details</summary>
Motivation: The increasing use of brittle AI systems in high-stakes decisions creates substantial risks of harm to people's well-being and fundamental rights, with current systems lacking proper documentation that would enable tracing decision processes and establishing responsibility chains for legal accountability.

Method: The paper takes a radical yet practical approach by enforcing documentation of every component in AI training and inference, expanding the DBOM concept into an effective workflow using confidential computing technology to generate tamper-proof, verifiable traces.

Result: The authors present the first running workflow that supports generation of tamper-proof, verifiable and exhaustive traces of AI decisions, demonstrated through a mushroom classification app that distinguishes between poisonous and edible mushrooms as a playful example of high-stake decision support.

Conclusion: The proposed workflow provides a practical solution to the critical problem of AI decision traceability, enabling proper documentation that can stand up in court when determining responsibility for AI-based decisions that violate the law.

Abstract: An ever increasing number of high-stake decisions are made or assisted by automated systems employing brittle artificial intelligence technology. There is a substantial risk that some of these decision induce harm to people, by infringing their well-being or their fundamental human rights. The state-of-the-art in AI systems makes little effort with respect to appropriate documentation of the decision process. This obstructs the ability to trace what went into a decision, which in turn is a prerequisite to any attempt of reconstructing a responsibility chain. Specifically, such traceability is linked to a documentation that will stand up in court when determining the cause of some AI-based decision that inadvertently or intentionally violates the law.
  This paper takes a radical, yet practical, approach to this problem, by enforcing the documentation of each and every component that goes into the training or inference of an automated decision. As such, it presents the first running workflow supporting the generation of tamper-proof, verifiable and exhaustive traces of AI decisions. In doing so, we expand the DBOM concept into an effective running workflow leveraging confidential computing technology. We demonstrate the inner workings of the workflow in the development of an app to tell poisonous and edible mushrooms apart, meant as a playful example of high-stake decision support.

</details>


### [96] [Can You Tell the Difference? Contrastive Explanations for ABox Entailments](https://arxiv.org/abs/2511.11281)
*Patrick Koopmann,Yasir Mahmood,Axel-Cyrille Ngonga Ngomo,Balram Tiwari*

Main category: cs.AI

TL;DR: This paper introduces contrastive ABox explanations to answer 'why is a an instance of C but b is not?' questions, focusing on both positive and missing entailments simultaneously to highlight relevant differences between instances.


<details>
  <summary>Details</summary>
Motivation: Existing approaches explain positive entailments and missing entailments separately, but contrastive explanations consider both together to better focus on the relevant commonalities and differences between instances.

Method: Developed a formal notion of contrastive explanations for ABox reasoning with description logic ontologies, analyzed computational complexity for different variants under optimality criteria across various description logics, and implemented a method for computing one variant.

Result: The paper provides computational complexity analysis for different variants of contrastive explanations under various optimality criteria, considering both lightweight and expressive description logics, and includes implementation and evaluation on generated problems for realistic knowledge bases.

Conclusion: Contrastive ABox explanations offer a novel approach that simultaneously considers both positive and missing entailments, providing more focused explanations by highlighting the relevant differences between instances, with practical implementation and evaluation demonstrating feasibility.

Abstract: We introduce the notion of contrastive ABox explanations to answer questions of the type "Why is a an instance of C, but b is not?". While there are various approaches for explaining positive entailments (why is C(a) entailed by the knowledge base) as well as missing entailments (why is C(b) not entailed) in isolation, contrastive explanations consider both at the same time, which allows them to focus on the relevant commonalities and differences between a and b. We develop an appropriate notion of contrastive explanations for the special case of ABox reasoning with description logic ontologies, and analyze the computational complexity for different variants under different optimality criteria, considering lightweight as well as more expressive description logics. We implemented a first method for computing one variant of contrastive explanations, and evaluated it on generated problems for realistic knowledge bases.

</details>


### [97] [EcoAlign: An Economically Rational Framework for Efficient LVLM Alignment](https://arxiv.org/abs/2511.11301)
*Ruoxi Cheng,Haoxuan Ma,Teng Ma,Hongyi Zhang*

Main category: cs.AI

TL;DR: EcoAlign is an inference-time framework that treats LVLM alignment as an economically rational search problem, using forward-looking cost-benefit analysis to balance safety, utility, and computational costs while preventing deception through weakest-link safety enforcement.


<details>
  <summary>Details</summary>
Motivation: Current LVLM alignment methods face trade-offs between safety, utility, and operational costs, with process-blindness wasting computational budget on unsafe deliberation and allowing harmful reasoning to be disguised with benign justifications.

Method: EcoAlign incrementally expands a thought graph and scores actions using a forward-looking function (similar to net present value) that dynamically weighs expected safety, utility, and cost against remaining budget, while enforcing path safety via the weakest-link principle.

Result: Extensive experiments across 3 closed-source and 2 open-source models on 6 datasets show EcoAlign matches or surpasses state-of-the-art safety and utility at lower computational cost.

Conclusion: EcoAlign offers a principled, economical pathway to robust LVLM alignment by reframing alignment as an economically rational search problem.

Abstract: Large Vision-Language Models (LVLMs) exhibit powerful reasoning capabilities but suffer sophisticated jailbreak vulnerabilities. Fundamentally, aligning LVLMs is not just a safety challenge but a problem of economic efficiency. Current alignment methods struggle with the trade-off between safety, utility, and operational costs. Critically, a focus solely on final outputs (process-blindness) wastes significant computational budget on unsafe deliberation. This flaw allows harmful reasoning to be disguised with benign justifications, thereby circumventing simple additive safety scores. To address this, we propose EcoAlign, an inference-time framework that reframes alignment as an economically rational search by treating the LVLM as a boundedly rational agent. EcoAlign incrementally expands a thought graph and scores actions using a forward-looking function (analogous to net present value) that dynamically weighs expected safety, utility, and cost against the remaining budget. To prevent deception, path safety is enforced via the weakest-link principle. Extensive experiments across 3 closed-source and 2 open-source models on 6 datasets show that EcoAlign matches or surpasses state-of-the-art safety and utility at a lower computational cost, thereby offering a principled, economical pathway to robust LVLM alignment.

</details>


### [98] [RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms](https://arxiv.org/abs/2511.11323)
*Yitian Kou,Yihe Gu,Chen Zhou,DanDan Zhu,Shuguang Kuai*

Main category: cs.AI

TL;DR: RLSLM is a hybrid reinforcement learning framework that combines rule-based social locomotion models with reinforcement learning to enable socially-aware navigation that balances mechanical energy and human comfort.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between interpretable but inflexible rule-based approaches and powerful but opaque data-driven methods for socially-aware navigation in human-populated environments.

Method: Integrates a rule-based Social Locomotion Model (grounded in empirical behavioral experiments) into RL reward function, generating an orientation-sensitive social comfort field that quantifies human comfort across space.

Result: Outperforms state-of-the-art rule-based models in user experience through VR-based human-agent interaction experiments, with significantly improved interpretability over conventional data-driven methods.

Conclusion: Presents a scalable, human-centered methodology that effectively integrates cognitive science and machine learning for real-world social navigation, enabling socially aligned navigation policies with minimal training.

Abstract: Navigating human-populated environments without causing discomfort is a critical capability for socially-aware agents. While rule-based approaches offer interpretability through predefined psychological principles, they often lack generalizability and flexibility. Conversely, data-driven methods can learn complex behaviors from large-scale datasets, but are typically inefficient, opaque, and difficult to align with human intuitions. To bridge this gap, we propose RLSLM, a hybrid Reinforcement Learning framework that integrates a rule-based Social Locomotion Model, grounded in empirical behavioral experiments, into the reward function of a reinforcement learning framework. The social locomotion model generates an orientation-sensitive social comfort field that quantifies human comfort across space, enabling socially aligned navigation policies with minimal training. RLSLM then jointly optimizes mechanical energy and social comfort, allowing agents to avoid intrusions into personal or group space. A human-agent interaction experiment using an immersive VR-based setup demonstrates that RLSLM outperforms state-of-the-art rule-based models in user experience. Ablation and sensitivity analyses further show the model's significantly improved interpretability over conventional data-driven methods. This work presents a scalable, human-centered methodology that effectively integrates cognitive science and machine learning for real-world social navigation.

</details>


### [99] [KarmaTS: A Universal Simulation Platform for Multivariate Time Series with Functional Causal Dynamics](https://arxiv.org/abs/2511.11357)
*Haixin Li,Yanke Li,Diego Paez-Granados*

Main category: cs.AI

TL;DR: KarmaTS is an interactive framework for creating lag-indexed spatiotemporal causal models that generate synthetic multivariate time series with known causal dynamics, addressing challenges with access-restricted physiological data.


<details>
  <summary>Details</summary>
Motivation: The framework addresses the challenge of access-restricted physiological data by generating synthetic MTS with known causal dynamics and augmenting real-world datasets with expert knowledge.

Method: Constructs discrete-time structural causal processes (DSCP) through a mixed-initiative, human-in-the-loop workflow combining expert knowledge and algorithmic proposals. Supports mixed variable types, contemporaneous/lagged edges, and modular edge functionals from parameterizable templates to neural networks.

Result: The resulting DSCP supports simulation and causal interventions, including those under user-specified distribution shifts, enabling flexible validation and benchmarking of causal discovery algorithms.

Conclusion: KarmaTS enables expert-informed simulation for validating and benchmarking causal discovery algorithms through its interactive framework for constructing executable spatiotemporal causal graphical models.

Abstract: We introduce KarmaTS, an interactive framework for constructing lag-indexed, executable spatiotemporal causal graphical models for multivariate time series (MTS) simulation. Motivated by the challenge of access-restricted physiological data, KarmaTS generates synthetic MTS with known causal dynamics and augments real-world datasets with expert knowledge. The system constructs a discrete-time structural causal process (DSCP) by combining expert knowledge and algorithmic proposals in a mixed-initiative, human-in-the-loop workflow. The resulting DSCP supports simulation and causal interventions, including those under user-specified distribution shifts. KarmaTS handles mixed variable types, contemporaneous and lagged edges, and modular edge functionals ranging from parameterizable templates to neural network models. Together, these features enable flexible validation and benchmarking of causal discovery algorithms through expert-informed simulation.

</details>


### [100] [MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism](https://arxiv.org/abs/2511.11373)
*Shulin Liu,Dong Du,Tao Yang,Yang Li,Boyu Qiu*

Main category: cs.AI

TL;DR: MarsRL is a reinforcement learning framework with agentic pipeline parallelism that jointly optimizes multiple agents (Solver, Verifier, Corrector) to enhance multi-agent reasoning systems, significantly improving performance on mathematical reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent reasoning systems work well with closed-source models but struggle with open-source models due to insufficient critic and correction capabilities, limiting their generalization and effectiveness.

Method: MarsRL introduces agent-specific reward mechanisms to reduce reward noise and employs pipeline-inspired training for efficient handling of long trajectories, jointly optimizing all agents in the reasoning system.

Result: Applied to Qwen3-30B-A3B-Thinking-2507, MarsRL improved AIME2025 accuracy from 86.5% to 93.3% and BeyondAIME from 64.9% to 73.8%, even surpassing the performance of Qwen3-235B-A22B-Thinking-2507.

Conclusion: MarsRL demonstrates strong potential to advance multi-agent reasoning systems and broaden their applicability across diverse reasoning tasks by effectively optimizing agent capabilities in open-source models.

Abstract: Recent progress in large language models (LLMs) has been propelled by reinforcement learning with verifiable rewards (RLVR) and test-time scaling. However, the limited output length of LLMs constrains the depth of reasoning attainable in a single inference process. Multi-agent reasoning systems offer a promising alternative by employing multiple agents including Solver, Verifier, and Corrector, to iteratively refine solutions. While effective in closed-source models like Gemini 2.5 Pro, they struggle to generalize to open-source models due to insufficient critic and correction capabilities. To address this, we propose MarsRL, a novel reinforcement learning framework with agentic pipeline parallelism, designed to jointly optimize all agents in the system. MarsRL introduces agent-specific reward mechanisms to mitigate reward noise and employs pipeline-inspired training to enhance efficiency in handling long trajectories. Applied to Qwen3-30B-A3B-Thinking-2507, MarsRL improves AIME2025 accuracy from 86.5% to 93.3% and BeyondAIME from 64.9% to 73.8%, even surpassing Qwen3-235B-A22B-Thinking-2507. These findings highlight the potential of MarsRL to advance multi-agent reasoning systems and broaden their applicability across diverse reasoning tasks.

</details>


### [101] [Robust and Efficient Communication in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.11393)
*Zejiao Liu,Yi Li,Jiali Wang,Junqi Tu,Yitian Hong,Fangfei Li,Yang Liu,Toshiharu Sugawara,Yang Tang*

Main category: cs.AI

TL;DR: This survey reviews robust and efficient communication strategies for multi-agent reinforcement learning under realistic constraints like message perturbations, delays, and bandwidth limitations, focusing on applications in autonomous driving, SLAM, and federated learning.


<details>
  <summary>Details</summary>
Motivation: Most existing MARL approaches assume ideal communication conditions (instantaneous, reliable, unlimited bandwidth) that rarely exist in real-world deployments, creating a gap between theoretical models and practical implementations.

Method: Systematic review of recent advances in communication strategies for MARL, analyzing approaches that address realistic constraints including message perturbations, transmission delays, and limited bandwidth.

Result: The survey identifies key challenges and focuses on three practical applications: cooperative autonomous driving, distributed simultaneous localization and mapping, and federated learning, highlighting the importance of communication-privacy trade-offs.

Conclusion: The paper advocates for a unified approach that co-designs communication, learning, and robustness to bridge the gap between theoretical MARL models and practical implementations, and identifies key open challenges and future research directions.

Abstract: Multi-agent reinforcement learning (MARL) has made significant strides in enabling coordinated behaviors among autonomous agents. However, most existing approaches assume that communication is instantaneous, reliable, and has unlimited bandwidth; these conditions are rarely met in real-world deployments. This survey systematically reviews recent advances in robust and efficient communication strategies for MARL under realistic constraints, including message perturbations, transmission delays, and limited bandwidth. Furthermore, because the challenges of low-latency reliability, bandwidth-intensive data sharing, and communication-privacy trade-offs are central to practical MARL systems, we focus on three applications involving cooperative autonomous driving, distributed simultaneous localization and mapping, and federated learning. Finally, we identify key open challenges and future research directions, advocating a unified approach that co-designs communication, learning, and robustness to bridge the gap between theoretical MARL models and practical implementations.

</details>


### [102] [CURENet: Combining Unified Representations for Efficient Chronic Disease Prediction](https://arxiv.org/abs/2511.11423)
*Cong-Tinh Dao,Nguyen Minh Thao Phan,Jun-En Ding,Chenwei Wu,David Restrepo,Dongsheng Luo,Fanyi Zhao,Chun-Chieh Liao,Wen-Chih Peng,Chi-Te Wang,Pei-Fu Chen,Ling Chen,Xinglong Ju,Feng Liu,Fang-Ming Hung*

Main category: cs.AI

TL;DR: CURENet is a multimodal model that integrates clinical notes, lab tests, and time-series data using LLMs and transformers for chronic disease prediction, achieving over 94% accuracy.


<details>
  <summary>Details</summary>
Motivation: Most predictive models fail to capture interactions and temporal patterns across multiple EHR data modalities, limiting comprehensive patient health assessment.

Method: Uses large language models for clinical text processing and transformer encoders for longitudinal sequential visits to integrate unstructured notes, lab tests, and time-series data.

Result: Achieved over 94% accuracy in predicting top 10 chronic conditions on MIMIC-III and FEMH datasets in multi-label framework.

Conclusion: Multimodal EHR integration can enhance clinical decision-making and improve patient outcomes by capturing complex data interactions.

Abstract: Electronic health records (EHRs) are designed to synthesize diverse data types, including unstructured clinical notes, structured lab tests, and time-series visit data. Physicians draw on these multimodal and temporal sources of EHR data to form a comprehensive view of a patient's health, which is crucial for informed therapeutic decision-making. Yet, most predictive models fail to fully capture the interactions, redundancies, and temporal patterns across multiple data modalities, often focusing on a single data type or overlooking these complexities. In this paper, we present CURENet, a multimodal model (Combining Unified Representations for Efficient chronic disease prediction) that integrates unstructured clinical notes, lab tests, and patients' time-series data by utilizing large language models (LLMs) for clinical text processing and textual lab tests, as well as transformer encoders for longitudinal sequential visits. CURENet has been capable of capturing the intricate interaction between different forms of clinical data and creating a more reliable predictive model for chronic illnesses. We evaluated CURENet using the public MIMIC-III and private FEMH datasets, where it achieved over 94\% accuracy in predicting the top 10 chronic conditions in a multi-label framework. Our findings highlight the potential of multimodal EHR integration to enhance clinical decision-making and improve patient outcomes.

</details>


### [103] [Experience-Guided Adaptation of Inference-Time Reasoning Strategies](https://arxiv.org/abs/2511.11519)
*Adam Stein,Matthew Trager,Benjamin Bowman,Michael Kleinman,Aditya Chattopadhyay,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: EGuR is an AI system that dynamically generates tailored computational strategies at inference time using accumulated experience, enabling flexible adaptation of all strategy components without requiring offline optimization.


<details>
  <summary>Details</summary>
Motivation: Current AI systems either only modify textual inputs or require static offline optimization, lacking the ability to dynamically adapt problem-solving approaches by changing sampling parameters, tools, system prompts, or switching between paradigms during inference.

Method: EGuR uses an LLM-based meta-strategy with two components: a Guide generates candidate strategies based on current problems and past experiences, and a Consolidator integrates execution feedback to improve future strategy generation, producing complete ready-to-run strategies.

Result: Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieved up to 14% accuracy improvements over strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.

Conclusion: EGuR demonstrates that dynamically generated, experience-guided strategies can significantly outperform static approaches in both accuracy and efficiency, enabling more adaptive and resource-efficient AI problem-solving.

Abstract: Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.

</details>


### [104] [Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping](https://arxiv.org/abs/2511.11551)
*Dena Mujtaba,Brian Hu,Anthony Hoogs,Arslan Basharat*

Main category: cs.AI

TL;DR: A test-time alignment technique using model-guided policy shaping to control AI agent behavior without retraining, enabling ethical alignment while maintaining performance across diverse RL environments.


<details>
  <summary>Details</summary>
Motivation: AI agents trained to maximize rewards often adopt harmful behaviors, creating a trade-off between performance and ethical alignment. Retraining pre-trained agents is costly, especially with diverse ethical values.

Method: Model-guided policy shaping applied at test time using scenario-action attribute classifiers to align decisions with ethical attributes, without requiring agent retraining.

Result: Effective mitigation of unethical behavior across 134 text-based game environments in the MACHIAVELLI benchmark, outperforming training-time methods and general-purpose agents.

Conclusion: Test-time policy shaping provides a scalable solution for maintaining ethical alignment in pre-trained AI agents while preserving reward maximization capabilities.

Abstract: The deployment of decision-making AI agents presents a critical challenge in maintaining alignment with human values or guidelines while operating in complex, dynamic environments. Agents trained solely to achieve their objectives may adopt harmful behavior, exposing a key trade-off between maximizing the reward function and maintaining the alignment. For the pre-trained agents, ensuring alignment is particularly challenging, as retraining can be a costly and slow process. This is further complicated by the diverse and potentially conflicting attributes representing the ethical values for alignment. To address these challenges, we propose a test-time alignment technique based on model-guided policy shaping. Our method allows precise control over individual behavioral attributes, generalizes across diverse reinforcement learning (RL) environments, and facilitates a principled trade-off between ethical alignment and reward maximization without requiring agent retraining. We evaluate our approach using the MACHIAVELLI benchmark, which comprises 134 text-based game environments and thousands of annotated scenarios involving ethical decisions. The RL agents are first trained to maximize the reward in their respective games. At test time, we apply policy shaping via scenario-action attribute classifiers to ensure decision alignment with ethical attributes. We compare our approach against prior training-time methods and general-purpose agents, as well as study several types of ethical violations and power-seeking behavior. Our results demonstrate that test-time policy shaping provides an effective and scalable solution for mitigating unethical behavior across diverse environments and alignment attributes.

</details>
