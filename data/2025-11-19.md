<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 36]
- [cs.AI](#cs.AI) [Total: 26]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Signature vs. Substance: Evaluating the Balance of Adversarial Resistance and Linguistic Quality in Watermarking Large Language Models](https://arxiv.org/abs/2511.13722)
*William Guo,Adaku Uchendu,Ana Smith*

Main category: cs.CL

TL;DR: Watermarking techniques for LLM-generated text face challenges: they negatively affect text quality, can be removed by adversarial attacks, and deviate from original writing styles, making adoption by LLM creators difficult.


<details>
  <summary>Details</summary>
Motivation: To address potential harms of LLM-generated text through watermarking, but current techniques face resistance due to quality degradation, vulnerability to attacks, and writing style deviation.

Method: Evaluated robustness of watermarking techniques against paraphrasing and back translation attacks, and assessed their ability to preserve quality and writing style using linguistic metrics.

Result: Watermarking techniques preserve semantics but deviate from unwatermarked text writing styles and are susceptible to adversarial attacks, particularly back translation.

Conclusion: Current watermarking techniques need improvement as they compromise text quality and style while being vulnerable to removal attacks, hindering widespread adoption.

Abstract: To mitigate the potential harms of Large Language Models (LLMs)generated text, researchers have proposed watermarking, a process of embedding detectable signals within text. With watermarking, we can always accurately detect LLM-generated texts. However, recent findings suggest that these techniques often negatively affect the quality of the generated texts, and adversarial attacks can strip the watermarking signals, causing the texts to possibly evade detection. These findings have created resistance in the wide adoption of watermarking by LLM creators. Finally, to encourage adoption, we evaluate the robustness of several watermarking techniques to adversarial attacks by comparing paraphrasing and back translation (i.e., English $\to$ another language $\to$ English) attacks; and their ability to preserve quality and writing style of the unwatermarked texts by using linguistic metrics to capture quality and writing style of texts. Our results suggest that these watermarking techniques preserve semantics, deviate from the writing style of the unwatermarked texts, and are susceptible to adversarial attacks, especially for the back translation attack.

</details>


### [2] [Refine Thought: A Test-Time Inference Method for Embedding Model Reasoning](https://arxiv.org/abs/2511.13726)
*Guangzhi Wang,Kai Li,Yinghao Jiao,Zhi Liu*

Main category: cs.CL

TL;DR: RT (Refine Thought) is a test-time inference method that enhances semantic reasoning in text embedding models through multiple forward passes, improving performance on reasoning tasks while maintaining general semantic understanding.


<details>
  <summary>Details</summary>
Motivation: To enhance the semantic reasoning ability of text embedding models that has been learned during pretraining but may not be fully activated during standard inference.

Method: Run multiple forward passes of the text embedding model to obtain the final semantic representation, acting as a test-time inference method.

Result: Significant improvements on semantic reasoning tasks (BRIGHT and PJBenchmark1) while maintaining consistent performance on general-purpose semantic understanding tasks (C-MTEB).

Conclusion: RT effectively activates semantic reasoning abilities learned during pretraining in decoder-only text embedding models like Qwen3-Embedding-8B.

Abstract: We propose RT (Refine Thought), a method that can enhance the semantic rea-soning ability of text embedding models. The method obtains the final semanticrepresentation by running multiple forward passes of the text embedding model.Experiments show that RT achieves significant improvements on semantic reason-ing tasks in BRIGHT and the person job matching benchmark PJBenchmark1, while maintaining consistent performance on general-purpose semantic under-standing tasks such as C-MTEB. Our results indicate that RT is effective becauseit further activates the semantic reasoning ability learned during pretraining bydecoder-only text embedding models(e.g., Qwen3-Embedding-8B). RT canbe seen as a test-time inference method.

</details>


### [3] [Can QE-informed (Re)Translation lead to Error Correction?](https://arxiv.org/abs/2511.13884)
*Govardhan Padmanabhan*

Main category: cs.CL

TL;DR: The paper presents two training-free approaches for QE-informed segment-level error correction: QE-informed Retranslation (selecting best translation from multiple LLM candidates) and QE-guided editing (replacing error substrings based on QE explanations). The retranslation approach achieved better performance and won the WMT 2025 task.


<details>
  <summary>Details</summary>
Motivation: To address the overcorrection problem in Automatic Post-Editing (APE) systems that degrade MT performance, while leveraging the benefits of joint QE-APE training.

Method: Two training-free approaches: 1) QE-informed Retranslation - selecting highest-quality translation from multiple LLM candidates, 2) QE-guided editing - instructing LLM to replace error substrings based on QE explanations with conditional heuristic to minimize edits.

Result: The retranslation approach achieved Delta COMET score of 0.0201, while the editing approach scored -0.0108. The retranslation approach won the WMT 2025 Task 3 leaderboard.

Conclusion: QE-informed Retranslation outperforms QE-guided editing in the training-free paradigm, demonstrating that selecting the best translation from multiple LLM candidates is more effective than error substring replacement for quality estimation-informed error correction.

Abstract: The paper presents two approaches submitted to the WMT 2025 Automated Translation Quality Evaluation Systems Task 3 - Quality Estimation (QE)-informed Segment-level Error Correction. While jointly training QE systems with Automatic Post-Editing (APE) has shown improved performance for both tasks, APE systems are still known to overcorrect the output of Machine Translation (MT), leading to a degradation in performance. We investigate a simple training-free approach - QE-informed Retranslation, and compare it with another within the same training-free paradigm. Our winning approach selects the highest-quality translation from multiple candidates generated by different LLMs. The second approach, more akin to APE, instructs an LLM to replace error substrings as specified in the provided QE explanation(s). A conditional heuristic was employed to minimise the number of edits, with the aim of maximising the Gain-to-Edit ratio. The two proposed approaches achieved a Delta COMET score of 0.0201 and -0.0108, respectively, leading the first approach to achieve the winning position on the subtask leaderboard.

</details>


### [4] [What Works for 'Lost-in-the-Middle' in LLMs? A Study on GM-Extract and Mitigations](https://arxiv.org/abs/2511.13900)
*Mihir Gupte,Eshan Dixit,Muhammad Tayyab,Arun Adiththan*

Main category: cs.CL

TL;DR: The paper introduces GM-Extract benchmark to study the 'lost-in-the-middle' phenomenon in LLMs, proposes evaluation metrics for spatial and semantic retrieval, tests 7-8B parameter models on multi-document tasks, and evaluates mitigation methods with mixed results.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of LLMs' diminishing ability to utilize long-range context (lost-in-the-middle phenomenon) in retrieval-based applications, particularly for control variable retrieval in real-world settings.

Method: Developed GM-Extract benchmark dataset, proposed two evaluation metrics (Document Metric for spatial retrieval and Variable Extraction Metric for semantic retrieval), systematically evaluated 7-8B parameter models on multi-document tasks, and tested categorized mitigation methods (black-box and white-box approaches).

Result: Found significant performance changes based on data representation in context window, observed clear performance patterns across models correlated with perplexity scores, but no consistent U-shaped curve. Mitigation methods showed nuanced efficacy - successful in some scenarios but surprisingly negative in others.

Conclusion: The lost-in-the-middle phenomenon significantly impacts LLM performance in retrieval tasks, with mitigation strategies having context-dependent effectiveness, highlighting the need for comprehensive understanding of these techniques in practical applications.

Abstract: The diminishing ability of large language models (LLMs) to effectively utilize long-range context-the "lost-in-the-middle" phenomenon-poses a significant challenge in retrieval-based LLM applications. To study the impact of this phenomenon in a real-world application setting, we introduce GM-Extract, a novel benchmark dataset meticulously designed to evaluate LLM performance on retrieval of control variables. To accurately diagnose failure modes, we propose a simple yet elegant evaluation system using two distinct metrics: one for spatial retrieval capability (Document Metric) and the other for semantic retrieval capability (Variable Extraction Metric). We conduct a systematic evaluation of 7-8B parameter models on two multi-document tasks (key-value extraction and question-answering), demonstrating a significant change in retrieval performance simply by altering how the data is represented in the context window. While a distinct U-shaped curve was not consistently observed, our analysis reveals a clear pattern of performance across models, which we further correlate with perplexity scores. Furthermore, we perform a literature survey of mitigation methods, which we categorize into two distinct approaches: black-box and white-box methods. We then apply these techniques to our benchmark, finding that their efficacy is highly nuanced. Our evaluation highlights scenarios where these strategies successfully improve performance, as well as surprising cases where they lead to a negative impact, providing a comprehensive understanding of their utility in a practical context.

</details>


### [5] [Hint-Augmented Re-ranking: Efficient Product Search using LLM-Based Query Decomposition](https://arxiv.org/abs/2511.13994)
*Yilun Zhu,Nikhita Vedula,Shervin Malmasi*

Main category: cs.CL

TL;DR: LLMs can interpret superlative queries in e-commerce by extracting structured attribute-value hints, improving search performance by 10.9 MAP points and ranking by 5.9 MRR points, with an efficient transfer approach to lightweight models for practical deployment.


<details>
  <summary>Details</summary>
Motivation: Search queries with superlatives (e.g., best, most popular) require comparing candidates across multiple dimensions, demanding linguistic understanding and domain knowledge that current systems lack.

Method: A framework that uses LLMs to extract structured interpretations (attribute-value hints) from superlative queries, decomposing them concurrently with retrieval for efficient integration into ranking pipelines, with transfer to lightweight models to address latency issues.

Result: Significant improvements in search performance: 10.9 points increase in MAP and 5.9 points increase in MRR over baseline methods.

Conclusion: The approach successfully represents and transfers superlative semantics between models, advancing linguistic interpretation in retrieval systems while addressing practical deployment constraints through efficient lightweight model transfer.

Abstract: Search queries with superlatives (e.g., best, most popular) require comparing candidates across multiple dimensions, demanding linguistic understanding and domain knowledge. We show that LLMs can uncover latent intent behind these expressions in e-commerce queries through a framework that extracts structured interpretations or hints. Our approach decomposes queries into attribute-value hints generated concurrently with retrieval, enabling efficient integration into the ranking pipeline. Our method improves search performanc eby 10.9 points in MAP and ranking by 5.9 points in MRR over baselines. Since direct LLM-based reranking faces prohibitive latency, we develop an efficient approach transferring superlative interpretations to lightweight models. Our findings provide insights into how superlative semantics can be represented and transferred between models, advancing linguistic interpretation in retrieval systems while addressing practical deployment constraints.

</details>


### [6] [Knowledge-Grounded Agentic Large Language Models for Multi-Hazard Understanding from Reconnaissance Reports](https://arxiv.org/abs/2511.14010)
*Chenchen Kuai,Zihao Li,Braden Rosen,Stephanie Paan,Navid Jafari,Jean-Louis Briaud,Yunlong Zhang,Youssef M. A. Hashash,Yang Zhou*

Main category: cs.CL

TL;DR: MoRA-RAG is a knowledge-grounded LLM framework that transforms unstructured post-disaster reconnaissance reports into structured data for multi-hazard reasoning, achieving 94.5% accuracy and reducing hallucinations.


<details>
  <summary>Details</summary>
Motivation: Post-disaster reconnaissance reports contain critical evidence for multi-hazard interactions but their unstructured narratives make systematic knowledge transfer difficult. LLMs can analyze these reports but often generate unreliable outputs without proper domain grounding.

Method: The framework integrates Mixture-of-Retrieval mechanism that dynamically routes queries across hazard-specific databases, uses agentic chunking to preserve contextual coherence, and includes a verification loop that assesses evidence sufficiency and refines queries.

Result: MoRA-RAG achieves up to 94.5% accuracy, outperforming zero-shot LLMs by 30% and state-of-the-art RAG systems by 10%, while reducing hallucinations across diverse LLM architectures. It also enables open-weight LLMs to achieve performance comparable to proprietary models.

Conclusion: MoRA-RAG establishes a new paradigm for transforming post-disaster documentation into actionable, trustworthy intelligence for hazard resilience.

Abstract: Post-disaster reconnaissance reports contain critical evidence for understanding multi-hazard interactions, yet their unstructured narratives make systematic knowledge transfer difficult. Large language models (LLMs) offer new potential for analyzing these reports, but often generate unreliable or hallucinated outputs when domain grounding is absent. This study introduces the Mixture-of-Retrieval Agentic RAG (MoRA-RAG), a knowledge-grounded LLM framework that transforms reconnaissance reports into a structured foundation for multi-hazard reasoning. The framework integrates a Mixture-of-Retrieval mechanism that dynamically routes queries across hazard-specific databases while using agentic chunking to preserve contextual coherence during retrieval. It also includes a verification loop that assesses evidence sufficiency, refines queries, and initiates targeted searches when information remains incomplete. We construct HazardRecQA by deriving question-answer pairs from GEER reconnaissance reports, which document 90 global events across seven major hazard types. MoRA-RAG achieves up to 94.5 percent accuracy, outperforming zero-shot LLMs by 30 percent and state-of-the-art RAG systems by 10 percent, while reducing hallucinations across diverse LLM architectures. MoRA-RAG also enables open-weight LLMs to achieve performance comparable to proprietary models. It establishes a new paradigm for transforming post-disaster documentation into actionable, trustworthy intelligence for hazard resilience.

</details>


### [7] [HiEAG: Evidence-Augmented Generation for Out-of-Context Misinformation Detection](https://arxiv.org/abs/2511.14027)
*Junjie Wu,Yumeng Fu,Nan Yu,Guohong Fu*

Main category: cs.CL

TL;DR: HiEAG is a hierarchical framework that improves multimodal misinformation detection by checking external consistency between image-text pairs and external evidence through retrieval, reranking, and rewriting modules using MLLMs.


<details>
  <summary>Details</summary>
Motivation: Existing OOC misinformation detection methods focus mainly on internal consistency between modalities but ignore the importance of external consistency with external evidence, limiting their effectiveness.

Method: Proposes HiEAG framework with three modules: evidence retrieval, reranking using Automatic Evidence Selection Prompting (AESP), and rewriting using Automatic Evidence Generation Prompting (AEGP) to leverage MLLM knowledge for external consistency checking.

Result: Experimental results show HiEAG surpasses previous state-of-the-art methods in accuracy across all samples on different benchmark datasets.

Conclusion: The hierarchical evidence-augmented generation framework effectively addresses external consistency checking in misinformation detection and enables explanation for judgment through instruction tuning.

Abstract: Recent advancements in multimodal out-of-context (OOC) misinformation detection have made remarkable progress in checking the consistencies between different modalities for supporting or refuting image-text pairs. However, existing OOC misinformation detection methods tend to emphasize the role of internal consistency, ignoring the significant of external consistency between image-text pairs and external evidence. In this paper, we propose HiEAG, a novel Hierarchical Evidence-Augmented Generation framework to refine external consistency checking through leveraging the extensive knowledge of multimodal large language models (MLLMs). Our approach decomposes external consistency checking into a comprehensive engine pipeline, which integrates reranking and rewriting, apart from retrieval. Evidence reranking module utilizes Automatic Evidence Selection Prompting (AESP) that acquires the relevant evidence item from the products of evidence retrieval. Subsequently, evidence rewriting module leverages Automatic Evidence Generation Prompting (AEGP) to improve task adaptation on MLLM-based OOC misinformation detectors. Furthermore, our approach enables explanation for judgment, and achieves impressive performance with instruction tuning. Experimental results on different benchmark datasets demonstrate that our proposed HiEAG surpasses previous state-of-the-art (SOTA) methods in the accuracy over all samples.

</details>


### [8] [Based on Data Balancing and Model Improvement for Multi-Label Sentiment Classification Performance Enhancement](https://arxiv.org/abs/2511.14073)
*Zijin Su,Huanzhu Lv,Yuren Niu,Yiming Liu*

Main category: cs.CL

TL;DR: This paper addresses class imbalance in multi-label sentiment classification by creating a balanced dataset from GoEmotions, Sentiment140, and GPT-4 mini annotations, and develops an enhanced model combining FastText embeddings, CNN, BiLSTM, and attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing multi-label sentiment datasets like GoEmotions suffer from severe class imbalance, which hampers model performance particularly for underrepresented emotions, necessitating a balanced dataset and improved classification approach.

Method: Constructed balanced dataset by integrating GoEmotions, Sentiment140 samples labeled with RoBERTa-base-GoEmotions, and GPT-4 mini generated texts; developed enhanced model with FastText embeddings, CNN layers, BiLSTM, attention mechanism, sigmoid output layer, and mixed precision training.

Result: Experimental results show significant improvements in accuracy, precision, recall, F1-score, and AUC compared to models trained on imbalanced data, demonstrating the effectiveness of the balanced dataset and enhanced model architecture.

Conclusion: The proposed balanced dataset and enhanced multi-label classification model effectively address class imbalance issues in sentiment analysis, leading to substantial performance improvements across multiple evaluation metrics.

Abstract: Multi-label sentiment classification plays a vital role in natural language processing by detecting multiple emotions within a single text. However, existing datasets like GoEmotions often suffer from severe class imbalance, which hampers model performance, especially for underrepresented emotions. To address this, we constructed a balanced multi-label sentiment dataset by integrating the original GoEmotions data, emotion-labeled samples from Sentiment140 using a RoBERTa-base-GoEmotions model, and manually annotated texts generated by GPT-4 mini. Our data balancing strategy ensured an even distribution across 28 emotion categories. Based on this dataset, we developed an enhanced multi-label classification model that combines pre-trained FastText embeddings, convolutional layers for local feature extraction, bidirectional LSTM for contextual learning, and an attention mechanism to highlight sentiment-relevant words. A sigmoid-activated output layer enables multi-label prediction, and mixed precision training improves computational efficiency. Experimental results demonstrate significant improvements in accuracy, precision, recall, F1-score, and AUC compared to models trained on imbalanced data, highlighting the effectiveness of our approach.

</details>


### [9] [Stealth Fine-Tuning: Efficiently Breaking Alignment in RVLMs Using Self-Generated CoT](https://arxiv.org/abs/2511.14106)
*Le Yu,Zhengyue Zhao,Yawen Zheng,Yunhao Liu*

Main category: cs.CL

TL;DR: Stealth Fine-Tuning is a novel attack method that bypasses safety alignment in Reasoning-augmented Vision-Language Models by using segment-level interference to generate harmful reasoning traces, then reusing these outputs for supervised fine-tuning with turn-based weighted loss.


<details>
  <summary>Details</summary>
Motivation: RVLMs rely on safety alignment to prevent harmful behavior, but their exposed chain-of-thought traces create new attack surfaces that can be exploited.

Method: The method uses segment-level interference to elicit harmful reasoning traces, reuses self-generated outputs as supervised fine-tuning data, and employs turn-based weighted loss design for lightweight, distribution-consistent fine-tuning.

Result: With only 499 samples and under 3 hours on a single A100 (using QLoRA), Stealth Fine-Tuning outperforms IDEATOR by 38.52% ASR while preserving general reasoning ability and maintaining original representation distribution.

Conclusion: Stealth Fine-Tuning is a low-cost, highly effective method to bypass alignment defenses in RVLMs, demonstrating vulnerabilities in current safety alignment approaches.

Abstract: Reasoning-augmented Vision-Language Models (RVLMs) rely on safety alignment to prevent harmful behavior, yet their exposed chain-of-thought (CoT) traces introduce new attack surfaces. In this work, we find that the safety alignment of RVLMs can be easily break through a novel attack method termed \textbf{Stealth Fine-Tuning}. Our method elicits harmful reasoning traces through \textbf{segment-level interference} and reuses the self-generated outputs as supervised fine-tuning data. Through a \textbf{turn-based weighted} loss design, yielding a lightweight, distribution-consistent finetuning method. In our experiment, with only 499 samples and under 3 hours on a single A100 (QLoRA), Stealth Fine-Tuning outperforms IDEATOR by 38.52\% ASR while preserving general reasoning ability, as the tuned model retains the original representation distribution. Experiments on AdvBench and several general benchmarks demonstrate that Stealth Fine-Tuning is a low-cost and highly effective way to bypass alignment defenses. \textcolor{red}{\textbf{Disclaimer: This paper contains content that may be disturbing or offensive.}}

</details>


### [10] [Synthetic Clinical Notes for Rare ICD Codes: A Data-Centric Framework for Long-Tail Medical Coding](https://arxiv.org/abs/2511.14112)
*Truong Vo,Weiyi Wu,Kaize Ding*

Main category: cs.CL

TL;DR: A data-centric framework generates synthetic discharge summaries to address the long-tail distribution problem in ICD coding, improving rare code prediction while maintaining overall performance.


<details>
  <summary>Details</summary>
Motivation: Automatic ICD coding faces challenges due to extreme long-tail distribution of diagnostic codes, with thousands of rare and zero-shot codes being severely underrepresented in datasets like MIMIC-III, leading to poor macro-F1 scores.

Method: Proposes a framework that generates high-quality synthetic discharge summaries by constructing realistic multi-label code sets anchored on rare codes using real-world co-occurrence patterns, ICD descriptions, synonyms, taxonomy, and similar clinical notes. Generated 90,000 synthetic notes covering 7,902 ICD codes and fine-tuned PLM-ICD and GKI-ICD models.

Result: The approach modestly improves macro-F1 while maintaining strong micro-F1, outperforming prior state-of-the-art methods. Generated synthetic data significantly expanded the training distribution.

Conclusion: Carefully crafted synthetic data can enhance equity in long-tail ICD code prediction, demonstrating that data-centric approaches can mitigate imbalance issues in medical NLP tasks.

Abstract: Automatic ICD coding from clinical text is a critical task in medical NLP but remains hindered by the extreme long-tail distribution of diagnostic codes. Thousands of rare and zero-shot ICD codes are severely underrepresented in datasets like MIMIC-III, leading to low macro-F1 scores. In this work, we propose a data-centric framework that generates high-quality synthetic discharge summaries to mitigate this imbalance. Our method constructs realistic multi-label code sets anchored on rare codes by leveraging real-world co-occurrence patterns, ICD descriptions, synonyms, taxonomy, and similar clinical notes. Using these structured prompts, we generate 90,000 synthetic notes covering 7,902 ICD codes, significantly expanding the training distribution. We fine-tune two state-of-the-art transformer-based models, PLM-ICD and GKI-ICD, on both the original and extended datasets. Experiments show that our approach modestly improves macro-F1 while maintaining strong micro-F1, outperforming prior SOTA. While the gain may seem marginal relative to the computational cost, our results demonstrate that carefully crafted synthetic data can enhance equity in long-tail ICD code prediction.

</details>


### [11] [From Graphs to Hypergraphs: Enhancing Aspect-Based Sentiment Analysis via Multi-Level Relational Modeling](https://arxiv.org/abs/2511.14142)
*Omkar Mahesh Kashyap,Padegal Amit,Madhav Kashyap,Ashwini M Joshi,Shylaja SS*

Main category: cs.CL

TL;DR: HyperABSA is a dynamic hypergraph framework for Aspect-Based Sentiment Analysis that uses hierarchical clustering to create sample-specific aspect-opinion structures, outperforming traditional graph-based methods on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Traditional graph-based ABSA approaches suffer from redundancy, parameter overhead, and error propagation due to modeling only pairwise dependencies and requiring multiple graphs for different relational views, especially in short-text and low-resource settings.

Method: Proposes HyperABSA with dynamic hypergraph construction using hierarchical clustering to induce aspect-opinion structures. Introduces an acceleration-fallback cutoff mechanism to adaptively determine clustering granularity for hyperedge construction.

Result: Experiments on three benchmarks (Lap14, Rest14, MAMS) show consistent improvements over strong graph baselines, with substantial gains when paired with RoBERTa backbones.

Conclusion: Dynamic hypergraph construction is an efficient and powerful alternative for ABSA, with potential extensions to other short-text NLP tasks.

Abstract: Aspect-Based Sentiment Analysis (ABSA) predicts sentiment polarity for specific aspect terms, a task made difficult by conflicting sentiments across aspects and the sparse context of short texts. Prior graph-based approaches model only pairwise dependencies, forcing them to construct multiple graphs for different relational views. These introduce redundancy, parameter overhead, and error propagation during fusion, limiting robustness in short-text, low-resource settings. We present HyperABSA, a dynamic hypergraph framework that induces aspect-opinion structures through sample-specific hierarchical clustering. To construct these hyperedges, we introduce a novel acceleration-fallback cutoff for hierarchical clustering, which adaptively determines the level of granularity. Experiments on three benchmarks (Lap14, Rest14, MAMS) show consistent improvements over strong graph baselines, with substantial gains when paired with RoBERTa backbones. These results position dynamic hypergraph construction as an efficient, powerful alternative for ABSA, with potential extensions to other short-text NLP tasks.

</details>


### [12] [Applying Relation Extraction and Graph Matching to Answering Multiple Choice Questions](https://arxiv.org/abs/2511.14144)
*Naoki Shimoda,Akihiro Yamamoto*

Main category: cs.CL

TL;DR: The paper proposes a method that combines Transformer-based relation extraction with knowledge graph matching to answer multiple-choice questions while maintaining traceability, achieving around 70% accuracy.


<details>
  <summary>Details</summary>
Motivation: To leverage the capability of Transformer-based relation extraction to dynamically generate knowledge graphs from natural language texts, enabling representation of sentence meaning and addressing the issue of RE methods generating false information from factually incorrect texts.

Method: The method converts question sentences into relational graphs using relation extraction, then verifies them against factually correct knowledge graphs under the closed-world assumption to measure truthfulness for answering fill-in-the-blank MCQs.

Result: The method achieves approximately 70% accuracy in answering questions while providing traceability of the procedure, with significant variation in accuracy across different question categories.

Conclusion: The proposed approach successfully combines dynamic KG generation with verification against established KGs to answer MCQs with traceability, though performance varies considerably by question type.

Abstract: In this research, we combine Transformer-based relation extraction with matching of knowledge graphs (KGs) and apply them to answering multiple-choice questions (MCQs) while maintaining the traceability of the output process. KGs are structured representations of factual knowledge consisting of entities and relations. Due to the high construction cost, they had been regarded as static databases with validated links. However, the recent development of Transformer-based relation extraction (RE) methods has enabled us to generate KGs dynamically by giving them natural language texts, and thereby opened the possibility for representing the meaning of the input sentences with the created KGs. Using this effect, we propose a method that answers MCQs in the "fill-in-the-blank" format, taking care of the point that RE methods generate KGs that represent false information if provided with factually incorrect texts. We measure the truthfulness of each question sentence by (i) converting the sentence into a relational graph using an RE method and (ii) verifying it against factually correct KGs under the closed-world assumption. The experimental results demonstrate that our method correctly answers up to around 70% of the questions, while providing traceability of the procedure. We also highlight that the question category has a vast influence on the accuracy.

</details>


### [13] [Selective Weak-to-Strong Generalization](https://arxiv.org/abs/2511.14166)
*Hao Lang,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: The paper proposes a selective weak-to-strong generalization framework that uses a binary classifier to identify questions a strong model can answer, avoiding unnecessary weak supervision and refining weak labels with graph smoothing.


<details>
  <summary>Details</summary>
Motivation: Future superhuman models will surpass human capabilities, making high-quality data for alignment scarce. Existing weak-to-strong generalization methods use weak supervision indiscriminately, which can be harmful to models due to poor quality labels.

Method: Train a binary classifier P(IK) to identify questions that a strong model can answer, using self-generated labels for alignment. Refine weak labels with graph smoothing to improve quality.

Result: Extensive experiments on three benchmarks show consistent outperformance over competitive baselines. The P(IK) classifier generalizes across tasks and difficulties.

Conclusion: Selective weak-to-strong generalization can help superalignment by avoiding harmful weak supervision and leveraging model self-knowledge effectively.

Abstract: Future superhuman models will surpass the ability of humans and humans will only be able to \textit{weakly} supervise superhuman models. To alleviate the issue of lacking high-quality data for model alignment, some works on weak-to-strong generalization (W2SG) finetune a strong pretrained model with a weak supervisor so that it can generalize beyond weak supervision. However, the invariable use of weak supervision in existing methods exposes issues in robustness, with a proportion of weak labels proving harmful to models. In this paper, we propose a selective W2SG framework to avoid using weak supervision when unnecessary. We train a binary classifier P(IK) to identify questions that a strong model can answer and use its self-generated labels for alignment. We further refine weak labels with a graph smoothing method. Extensive experiments on three benchmarks show that our method consistently outperforms competitive baselines. Further analyses show that P(IK) can generalize across tasks and difficulties, which indicates selective W2SG can help superalignment.

</details>


### [14] [SymLoc: Symbolic Localization of Hallucination across HaluEval and TruthfulQA](https://arxiv.org/abs/2511.14172)
*Naveen Lamba,Sanju Tiwari,Manas Gaur*

Main category: cs.CL

TL;DR: The paper proposes a symbolic localization framework that uses symbolic linguistic knowledge to trace hallucination development across LLM layers, revealing that hallucination is fundamentally a symbolic linguistic processing failure rather than a general generation problem.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with hallucination triggered by symbolic elements like modifiers, negation, and named entities, but current localization methods treat all tokens equally and overlook the specific role of symbolic linguistic knowledge in triggering hallucinations.

Method: Proposed a symbolic localization framework leveraging symbolic linguistic and semantic knowledge to trace hallucination development across all model layers, analyzing five models using HaluEval and TruthfulQA datasets.

Result: Attention variance for symbolic linguistic elements explodes to critical instability in early layers (2-4), with negation triggering catastrophic variance levels. Hallucination rates remain consistently high (78.3%-83.7% across Gemma variants) despite larger model sizes, with steep attention drops for symbolic semantic triggers throughout deeper layers.

Conclusion: Hallucination is fundamentally a symbolic linguistic processing failure, not a general generation problem, and symbolic semantic knowledge provides the key to understanding and localizing hallucination mechanisms in LLMs.

Abstract: LLMs still struggle with hallucination, especially when confronted with symbolic triggers like modifiers, negation, numbers, exceptions, and named entities. Yet, we lack a clear understanding of where these symbolic hallucinations originate, making it crucial to systematically handle such triggers and localize the emergence of hallucination inside the model. While prior work explored localization using statistical techniques like LSC and activation variance analysis, these methods treat all tokens equally and overlook the role symbolic linguistic knowledge plays in triggering hallucinations. So far, no approach has investigated how symbolic elements specifically drive hallucination failures across model layers, nor has symbolic linguistic knowledge been used as the foundation for a localization framework. We propose the first symbolic localization framework that leverages symbolic linguistic and semantic knowledge to meaningfully trace the development of hallucinations across all model layers. By focusing on how models process symbolic triggers, we analyze five models using HaluEval and TruthfulQA. Our symbolic knowledge approach reveals that attention variance for these linguistic elements explodes to critical instability in early layers (2-4), with negation triggering catastrophic variance levels, demonstrating that symbolic semantic processing breaks down from the very beginning. Through the lens of symbolic linguistic knowledge, despite larger model sizes, hallucination rates remain consistently high (78.3%-83.7% across Gemma variants), with steep attention drops for symbolic semantic triggers throughout deeper layers. Our findings demonstrate that hallucination is fundamentally a symbolic linguistic processing failure, not a general generation problem, revealing that symbolic semantic knowledge provides the key to understanding and localizing hallucination mechanisms in LLMs.

</details>


### [15] [Harnessing Deep LLM Participation for Robust Entity Linking](https://arxiv.org/abs/2511.14181)
*Jiajun Hou,Chenyu Zhang,Rui Meng*

Main category: cs.CL

TL;DR: DeepEL is a comprehensive framework that integrates LLMs throughout all stages of entity linking, featuring a novel self-validation mechanism that uses global context to improve entity disambiguation and achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Prior EL approaches only applied LLMs to isolated stages of the task, failing to fully leverage their capabilities throughout the entire entity linking process. Additionally, disambiguating entities in isolation was found to be insufficient for optimal performance.

Method: DeepEL incorporates LLMs into every stage of entity linking and introduces a self-validation mechanism that utilizes global contextual information to enable LLMs to correct their own predictions and better recognize cohesive relationships among entities within the same sentence.

Result: Extensive evaluation across ten benchmark datasets shows DeepEL substantially outperforms existing state-of-the-art methods, achieving an average 2.6% improvement in overall F1 score and a 4% gain on out-of-domain datasets.

Conclusion: Deep integration of LLMs throughout the entire entity linking process, combined with self-validation using global context, significantly advances the state-of-the-art in entity linking performance.

Abstract: Entity Linking (EL), the task of mapping textual entity mentions to their corresponding entries in knowledge bases, constitutes a fundamental component of natural language understanding. Recent advancements in Large Language Models (LLMs) have demonstrated remarkable potential for enhancing EL performance. Prior research has leveraged LLMs to improve entity disambiguation and input representation, yielding significant gains in accuracy and robustness. However, these approaches typically apply LLMs to isolated stages of the EL task, failing to fully integrate their capabilities throughout the entire process.
  In this work, we introduce DeepEL, a comprehensive framework that incorporates LLMs into every stage of the entity linking task. Furthermore, we identify that disambiguating entities in isolation is insufficient for optimal performance. To address this limitation, we propose a novel self-validation mechanism that utilizes global contextual information, enabling LLMs to rectify their own predictions and better recognize cohesive relationships among entities within the same sentence.
  Extensive empirical evaluation across ten benchmark datasets demonstrates that DeepEL substantially outperforms existing state-of-the-art methods, achieving an average improvement of 2.6\% in overall F1 score and a remarkable 4% gain on out-of-domain datasets. These results underscore the efficacy of deep LLM integration in advancing the state-of-the-art in entity linking.

</details>


### [16] [ArbESC+: Arabic Enhanced Edit Selection System Combination for Grammatical Error Correction Resolving conflict and improving system combination in Arabic GEC](https://arxiv.org/abs/2511.14230)
*Ahlam Alrehili,Areej Alhothali*

Main category: cs.CL

TL;DR: ArbESC+ is a multi-system approach for Arabic grammatical error correction that combines multiple models (AraT5, ByT5, mT5, AraBART, etc.) to generate correction proposals, uses numerical features and a classifier to select appropriate corrections, and achieves state-of-the-art results on QALB datasets.


<details>
  <summary>Details</summary>
Motivation: Arabic has complex morphological and syntactic structure making GEC challenging, and previous approaches used individual models without exploring the benefits of combining multiple systems.

Method: Uses multiple models to generate correction proposals represented as numerical features, applies a classifier to select corrections, and employs support techniques for filtering overlapping corrections and estimating decision reliability.

Result: Achieved F0.5 scores of 82.63% on QALB-14, 84.64% on QALB-15 L1, and 65.55% on QALB-15 L2, outperforming single-model approaches.

Conclusion: This is the first Arabic attempt to integrate linguistic error correction through multi-system combination, providing a practical advancement for Arabic text processing tools.

Abstract: Grammatical Error Correction (GEC) is an important aspect of natural language processing. Arabic has a complicated morphological and syntactic structure, posing a greater challenge than other languages. Even though modern neural models have improved greatly in recent years, the majority of previous attempts used individual models without taking into account the potential benefits of combining different systems. In this paper, we present one of the first multi-system approaches for correcting grammatical errors in Arabic, the Arab Enhanced Edit Selection System Complication (ArbESC+). Several models are used to collect correction proposals, which are represented as numerical features in the framework. A classifier determines and implements the appropriate corrections based on these features. In order to improve output quality, the framework uses support techniques to filter overlapping corrections and estimate decision reliability. A combination of AraT5, ByT5, mT5, AraBART, AraBART+Morph+GEC, and Text editing systems gave better results than a single model alone, with F0.5 at 82.63% on QALB-14 test data, 84.64% on QALB-15 L1 data, and 65.55% on QALB-15 L2 data. As one of the most significant contributions of this work, it's the first Arab attempt to integrate linguistic error correction. Improving existing models provides a practical step towards developing advanced tools that will benefit users and researchers of Arabic text processing.

</details>


### [17] [MuCPT: Music-related Natural Language Model Continued Pretraining](https://arxiv.org/abs/2511.14245)
*Kai Tian,Yirong Mao,Wendong Bi,Hanjie Wang,Que Wenhui*

Main category: cs.CL

TL;DR: This paper presents a framework for building domain-specific LLMs for music by creating a large music-related corpus (40B tokens) with specialized data processing pipelines, quality control mechanisms, and a new evaluation benchmark.


<details>
  <summary>Details</summary>
Motivation: Large language models perform well on general tasks but struggle in specialized domains like music due to limitations in corpus scale, purity, and data-task alignment. The authors aim to address these constraints for music-entertainment applications.

Method: Constructed a large music corpus combining open-source and in-house data using a domain-first pipeline with filtering, cleaning, de-duplication, and privacy masking. Implemented reference-model-based token-level soft scoring for quality control and dynamic optimization weighting. Created the MusicSimpleQA benchmark for automated evaluation.

Result: Developed a scalable data-training framework with systematic comparisons of data composition. The approach enables more effective music-domain continued pretraining and alignment by reducing noise gradients and amplifying task-aligned signals.

Conclusion: The work advances both corpus construction and training objectives for domain LLMs in music, providing a reusable evaluation tool and scalable framework that addresses key challenges in specialized domain adaptation.

Abstract: Large language models perform strongly on general tasks but remain constrained in specialized settings such as music, particularly in the music-entertainment domain, where corpus scale, purity, and the match between data and training objectives are critical. We address this by constructing a large, music-related natural language corpus (40B tokens) that combines open source and in-house data, and by implementing a domain-first data pipeline: a lightweight classifier filters and weights in-domain text, followed by multi-stage cleaning, de-duplication, and privacy-preserving masking. We further integrate multi-source music text with associated metadata to form a broader, better-structured foundation of domain knowledge. On the training side, we introduce reference-model (RM)-based token-level soft scoring for quality control: a unified loss-ratio criterion is used both for data selection and for dynamic down-weighting during optimization, reducing noise gradients and amplifying task-aligned signals, thereby enabling more effective music-domain continued pretraining and alignment. To assess factuality, we design the MusicSimpleQA benchmark, which adopts short, single-answer prompts with automated agreement scoring. Beyond the benchmark design, we conduct systematic comparisons along the axes of data composition. Overall, this work advances both the right corpus and the right objective, offering a scalable data-training framework and a reusable evaluation tool for building domain LLMs in the music field.

</details>


### [18] [Towards Authentic Movie Dubbing with Retrieve-Augmented Director-Actor Interaction Learning](https://arxiv.org/abs/2511.14249)
*Rui Liu,Yuan Zhao,Zhenqi Jia*

Main category: cs.CL

TL;DR: Authentic-Dubber is a movie dubbing model that simulates director-actor collaboration through multimodal reference retrieval and progressive speech generation, improving emotional expressiveness.


<details>
  <summary>Details</summary>
Motivation: Existing dubbing approaches overlook the critical director-actor interaction where directors guide actors to internalize emotional context before performance, leading to less authentic results.

Method: Proposes Retrieve-Augmented Director-Actor Interaction Learning with three mechanisms: multimodal Reference Footage library with LLM comprehension, Emotion-Similarity-based Retrieval-Augmentation, and Progressive Graph-based speech generation.

Result: Achieves comprehensive improvements in emotional expressiveness on the V2C Animation benchmark dataset, validated by both subjective and objective evaluations.

Conclusion: The proposed approach successfully replicates authentic dubbing workflow and demonstrates effectiveness in generating emotionally expressive dubbed speech.

Abstract: The automatic movie dubbing model generates vivid speech from given scripts, replicating a speaker's timbre from a brief timbre prompt while ensuring lip-sync with the silent video. Existing approaches simulate a simplified workflow where actors dub directly without preparation, overlooking the critical director-actor interaction. In contrast, authentic workflows involve a dynamic collaboration: directors actively engage with actors, guiding them to internalize the context cues, specifically emotion, before performance. To address this issue, we propose a new Retrieve-Augmented Director-Actor Interaction Learning scheme to achieve authentic movie dubbing, termed Authentic-Dubber, which contains three novel mechanisms: (1) We construct a multimodal Reference Footage library to simulate the learning footage provided by directors. Note that we integrate Large Language Models (LLMs) to achieve deep comprehension of emotional representations across multimodal signals. (2) To emulate how actors efficiently and comprehensively internalize director-provided footage during dubbing, we propose an Emotion-Similarity-based Retrieval-Augmentation strategy. This strategy retrieves the most relevant multimodal information that aligns with the target silent video. (3) We develop a Progressive Graph-based speech generation approach that incrementally incorporates the retrieved multimodal emotional knowledge, thereby simulating the actor's final dubbing process. The above mechanisms enable the Authentic-Dubber to faithfully replicate the authentic dubbing workflow, achieving comprehensive improvements in emotional expressiveness. Both subjective and objective evaluations on the V2C Animation benchmark dataset validate the effectiveness. The code and demos are available at https://github.com/AI-S2-Lab/Authentic-Dubber.

</details>


### [19] [AfriSpeech-MultiBench: A Verticalized Multidomain Multicountry Benchmark Suite for African Accented English ASR](https://arxiv.org/abs/2511.14255)
*Gabrial Zencha Ashungafac,Mardhiyah Sanni,Busayo Awobade,Alex Gichamba,Tobi Olatunji*

Main category: cs.CL

TL;DR: AfriSpeech-MultiBench is the first domain-specific evaluation suite for over 100 African English accents across 10+ countries and seven application domains, benchmarking various speech recognition systems to address Africa's linguistic diversity gap.


<details>
  <summary>Details</summary>
Motivation: Despite widespread interest in voice interfaces globally, there exists no publicly available application-specific model evaluation that caters to Africa's linguistic diversity, creating a gap for inclusive voice applications in underserved African communities.

Method: Created AfriSpeech-MultiBench evaluation suite with spontaneous and non-spontaneous speech conversations from various open African accented English speech datasets, benchmarking diverse ASR models and multimodal LLM-based speech recognition systems across seven domains.

Result: Open-source ASR models excel in spontaneous speech but degrade on noisy dialogue; multimodal LLMs are more accent-robust but struggle with domain-specific named entities; proprietary models deliver high accuracy on clean speech but vary significantly by country and domain; models fine-tuned on African English achieve competitive accuracy with lower latency.

Conclusion: The comprehensive benchmark empowers practitioners and researchers to select voice technologies suited to African use-cases, fostering inclusive voice applications for underserved communities, though hallucinations remain a significant problem for most state-of-the-art models.

Abstract: Recent advances in speech-enabled AI, including Google's NotebookLM and OpenAI's speech-to-speech API, are driving widespread interest in voice interfaces globally. Despite this momentum, there exists no publicly available application-specific model evaluation that caters to Africa's linguistic diversity. We present AfriSpeech-MultiBench, the first domain-specific evaluation suite for over 100 African English accents across 10+ countries and seven application domains: Finance, Legal, Medical, General dialogue, Call Center, Named Entities and Hallucination Robustness. We benchmark a diverse range of open, closed, unimodal ASR and multimodal LLM-based speech recognition systems using both spontaneous and non-spontaneous speech conversation drawn from various open African accented English speech datasets. Our empirical analysis reveals systematic variation: open-source ASR models excels in spontaneous speech contexts but degrades on noisy, non-native dialogue; multimodal LLMs are more accent-robust yet struggle with domain-specific named entities; proprietary models deliver high accuracy on clean speech but vary significantly by country and domain. Models fine-tuned on African English achieve competitive accuracy with lower latency, a practical advantage for deployment, hallucinations still remain a big problem for most SOTA models. By releasing this comprehensive benchmark, we empower practitioners and researchers to select voice technologies suited to African use-cases, fostering inclusive voice applications for underserved communities.

</details>


### [20] [Entropy-Guided Reasoning Compression](https://arxiv.org/abs/2511.14258)
*Hourun Zhu,Yang Gao,Wenlong Fei,Jiawei Li,Huashan Sun*

Main category: cs.CL

TL;DR: The paper proposes an entropy-guided training framework to compress chain-of-thought reasoning in large models, reducing reasoning length to 20% of original while maintaining or improving accuracy by addressing entropy conflicts during compression training.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models produce excessively long chain-of-thought outputs, creating practical bottlenecks due to high computation costs and poor deployability. Existing compression methods overlook the entropy conflict phenomenon where compression objectives decrease entropy (shortening reasoning) while accuracy objectives increase entropy (lengthening reasoning), causing models to get stuck in local optima.

Method: The authors adopt an entropy-guided training framework that dynamically adjusts training based on entropy levels. When entropy decreases, the model is guided toward concise reasoning steps; when entropy rises, exploration is reinforced under compact reasoning mode to improve robustness. This addresses the identified entropy conflict where logical connectors receive opposing gradients from compression and performance objectives.

Result: Experiments on six mathematical benchmarks demonstrate that the method compresses reasoning length to 20% of the original while maintaining or even surpassing baseline accuracy levels.

Conclusion: The entropy-guided training framework effectively resolves entropy conflicts in reasoning compression, enabling significant length reduction (80% compression) without sacrificing accuracy, making large reasoning models more practical for deployment.

Abstract: Large reasoning models have demonstrated remarkable performance on complex reasoning tasks, yet the excessive length of their chain-of-thought outputs remains a major practical bottleneck due to high computation cost and poor deployability. Existing compression methods have achieved partial success but overlook a crucial phenomenon in the training process -- the entropy conflict. During compression training, entropy decreases, leading to shorter reasoning but limited exploration, while accuracy-oriented objectives increase entropy, lengthening reasoning chains. This can cause the model to get stuck in a local dilemma. Our analysis further reveals the origin of the entropy conflict: many high-entropy tokens are logical connectors that receive larger gradients and are encouraged under the performance objective, while the compression objective simultaneously penalizes these potentially redundant connectors. This opposing pressure creates a direct source of entropy conflict. To address these issues, we adopt an entropy-guided training framework. As entropy descends, the model is guided toward efficient reasoning by encouraging concise thought steps; as entropy rises, exploration is reinforced under the compact reasoning mode to improve robustness. Experiments on six mathematical benchmarks show that our method compresses reasoning length to 20% of the original while maintaining or even surpassing baseline accuracy. Code and models will be released publicly.

</details>


### [21] [Don't Miss the Forest for the Trees: In-Depth Confidence Estimation for LLMs via Reasoning over the Answer Space](https://arxiv.org/abs/2511.14275)
*Ante Wang,Weizhi Ma,Yang Liu*

Main category: cs.CL

TL;DR: This paper proposes using verbalized probability distribution prediction to improve confidence estimation in LLMs, showing it encourages deeper reasoning and works across models and tasks.


<details>
  <summary>Details</summary>
Motivation: Current methods for verbalized confidence estimation in LLMs are enhanced by chain-of-thought reasoning, but how reasoning strategies affect confidence estimation remains under-explored.

Method: The authors demonstrate that predicting a verbalized probability distribution encourages in-depth reasoning by requiring LLMs to consider all candidates in the answer space and carefully assign confidence scores to meet distribution requirements.

Result: This method shows advantages across different models and various tasks, regardless of whether the answer space is known, and maintains advantages even after reinforcement learning.

Conclusion: The proposed approach effectively improves confidence estimation through deeper reasoning, with reasoning patterns aligned with human expectations.

Abstract: Knowing the reliability of a model's response is essential in application. With the strong generation capabilities of LLMs, research has focused on generating verbalized confidence. This is further enhanced by combining chain-of-thought reasoning, which provides logical and transparent estimation. However, how reasoning strategies affect the estimated confidence is still under-explored. In this work, we demonstrate that predicting a verbalized probability distribution can effectively encourage in-depth reasoning for confidence estimation. Intuitively, it requires an LLM to consider all candidates within the answer space instead of basing on a single guess, and to carefully assign confidence scores to meet the requirements of a distribution. This method shows an advantage across different models and various tasks, regardless of whether the answer space is known. Its advantage is maintained even after reinforcement learning, and further analysis shows its reasoning patterns are aligned with human expectations.

</details>


### [22] [AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models](https://arxiv.org/abs/2511.14295)
*Mohammad Zbib,Hasan Abed Al Kader Hammoud,Sina Mukalled,Nadine Rizk,Fatima Karnib,Issam Lakkis,Ammar Mohanna,Bernard Ghanem*

Main category: cs.CL

TL;DR: AraLingBench is a human-annotated benchmark with 150 multiple choice questions across 5 linguistic categories to evaluate Arabic LLMs' structural language understanding, revealing that current models struggle with deeper grammatical reasoning despite surface proficiency.


<details>
  <summary>Details</summary>
Motivation: To address the gap between high scores on knowledge-based benchmarks and true linguistic mastery in Arabic language models, and to provide a diagnostic framework for developing better Arabic LLMs.

Method: Created a fully human-annotated benchmark with 150 expert-designed multiple choice questions spanning five core linguistic categories: grammar, morphology, spelling, reading comprehension, and syntax. Evaluated 35 Arabic and bilingual LLMs.

Result: Current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. Many models succeed through memorization or pattern recognition rather than authentic comprehension.

Conclusion: AraLingBench highlights a persistent gap in Arabic linguistic competence and provides a diagnostic framework for developing Arabic LLMs. The benchmark shows that true linguistic mastery requires more than surface-level pattern recognition.

Abstract: We present AraLingBench: a fully human annotated benchmark for evaluating the Arabic linguistic competence of large language models (LLMs). The benchmark spans five core categories: grammar, morphology, spelling, reading comprehension, and syntax, through 150 expert-designed multiple choice questions that directly assess structural language understanding. Evaluating 35 Arabic and bilingual LLMs reveals that current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. AraLingBench highlights a persistent gap between high scores on knowledge-based benchmarks and true linguistic mastery, showing that many models succeed through memorization or pattern recognition rather than authentic comprehension. By isolating and measuring fundamental linguistic skills, AraLingBench provides a diagnostic framework for developing Arabic LLMs. The full evaluation code is publicly available on GitHub.

</details>


### [23] [ConInstruct: Evaluating Large Language Models on Conflict Detection and Resolution in Instructions](https://arxiv.org/abs/2511.14342)
*Xingwei He,Qianru Zhang,Pengfei Chen,Guanhua Chen,Linlin Yu,Yuan Yuan,Siu-Ming Yiu*

Main category: cs.CL

TL;DR: ConInstruct benchmark assesses LLMs' ability to detect and resolve conflicts in user instructions, revealing that while proprietary LLMs show strong conflict detection, they rarely notify users about conflicts or request clarification.


<details>
  <summary>Details</summary>
Motivation: Existing works focus on LLM instruction-following but overlook scenarios with conflicting constraints in complex prompts, leaving LLM behavior under such conditions unexplored.

Method: Introduce ConInstruct benchmark to evaluate LLMs' conflict detection performance and analyze their conflict resolution behavior through systematic experiments.

Result: Proprietary LLMs show strong conflict detection (DeepSeek-R1: 91.5% F1, Claude-4.5-Sonnet: 87.3% F1), but LLMs rarely explicitly notify users about conflicts or request clarification despite detection capabilities.

Conclusion: Current LLMs have a critical shortcoming in handling conflicting instructions - they detect conflicts well but fail to communicate them to users, highlighting an important area for future improvement in instruction-following design.

Abstract: Instruction-following is a critical capability of Large Language Models (LLMs). While existing works primarily focus on assessing how well LLMs adhere to user instructions, they often overlook scenarios where instructions contain conflicting constraints-a common occurrence in complex prompts. The behavior of LLMs under such conditions remains under-explored. To bridge this gap, we introduce ConInstruct, a benchmark specifically designed to assess LLMs' ability to detect and resolve conflicts within user instructions. Using this dataset, we evaluate LLMs' conflict detection performance and analyze their conflict resolution behavior. Our experiments reveal two key findings: (1) Most proprietary LLMs exhibit strong conflict detection capabilities, whereas among open-source models, only DeepSeek-R1 demonstrates similarly strong performance. DeepSeek-R1 and Claude-4.5-Sonnet achieve the highest average F1-scores at 91.5% and 87.3%, respectively, ranking first and second overall. (2) Despite their strong conflict detection abilities, LLMs rarely explicitly notify users about the conflicts or request clarification when faced with conflicting constraints. These results underscore a critical shortcoming in current LLMs and highlight an important area for future improvement when designing instruction-following LLMs.

</details>


### [24] [ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning](https://arxiv.org/abs/2511.14366)
*Hongwei Liu,Junnan Liu,Shudong Liu,Haodong Duan,Yuqiang Li,Mao Su,Xiaohong Liu,Guangtao Zhai,Xinyu Fang,Qianhong Ma,Taolin Zhang,Zihan Ma,Yufeng Zhao,Peiheng Zhou,Linchen Xiao,Wenlong Zhang,Shijie Zhou,Xingjian Ma,Siqi Sun,Jiaye Ge,Meng Li,Yuhong Liu,Jianxin Dong,Jiaying Li,Hui Wu,Hanwen Liang,Jintai Lin,Yanting Wang,Jie Dong,Tong Zhu,Tianfan Fu,Conghui He,Qi Zhang,Songyang Zhang,Lei Bai,Kai Chen*

Main category: cs.CL

TL;DR: ATLAS is a new high-difficulty, cross-disciplinary benchmark for evaluating LLMs' scientific reasoning capabilities, featuring 800 original problems across 7 scientific fields with contamination-resistant design and complex open-ended answers.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks suffer from performance saturation, narrow focus, simplified formats, and data contamination issues, creating a fidelity gap with real scientific inquiry.

Method: Developed 800 original problems by domain experts across 7 scientific fields with multi-stage quality control, featuring contamination-resistant design, cross-disciplinary integration, and complex open-ended answers with LaTeX formatting.

Result: Preliminary results show ATLAS effectively differentiates advanced scientific reasoning capabilities of leading models, providing better distinction than existing benchmarks.

Conclusion: ATLAS serves as a reliable evaluation platform for measuring progress toward AGI and will be developed into an open, community-driven long-term benchmark.

Abstract: The rapid advancement of Large Language Models (LLMs) has led to performance saturation on many established benchmarks, questioning their ability to distinguish frontier models. Concurrently, existing high-difficulty benchmarks often suffer from narrow disciplinary focus, oversimplified answer formats, and vulnerability to data contamination, creating a fidelity gap with real-world scientific inquiry. To address these challenges, we introduce ATLAS (AGI-Oriented Testbed for Logical Application in Science), a large-scale, high-difficulty, and cross-disciplinary evaluation suite composed of approximately 800 original problems. Developed by domain experts (PhD-level and above), ATLAS spans seven core scientific fields: mathematics, physics, chemistry, biology, computer science, earth science, and materials science. Its key features include: (1) High Originality and Contamination Resistance, with all questions newly created or substantially adapted to prevent test data leakage; (2) Cross-Disciplinary Focus, designed to assess models' ability to integrate knowledge and reason across scientific domains; (3) High-Fidelity Answers, prioritizing complex, open-ended answers involving multi-step reasoning and LaTeX-formatted expressions over simple multiple-choice questions; and (4) Rigorous Quality Control, employing a multi-stage process of expert peer review and adversarial testing to ensure question difficulty, scientific value, and correctness. We also propose a robust evaluation paradigm using a panel of LLM judges for automated, nuanced assessment of complex answers. Preliminary results on leading models demonstrate ATLAS's effectiveness in differentiating their advanced scientific reasoning capabilities. We plan to develop ATLAS into a long-term, open, community-driven platform to provide a reliable "ruler" for progress toward Artificial General Intelligence.

</details>


### [25] [Mitigating Label Length Bias in Large Language Models](https://arxiv.org/abs/2511.14385)
*Mario Sanz-Guerrero,Katharina von der Wense*

Main category: cs.CL

TL;DR: The paper addresses label length bias in LLMs where multi-token class labels are treated inconsistently, proposing Normalized Contextual Calibration (NCC) to mitigate this bias and improve performance across various tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs suffer from label biases when predicting over candidate options, particularly overlooking biases from multi-token class labels, which affects performance in real-world applications where class labels naturally consist of multiple tokens.

Method: Proposes Normalized Contextual Calibration (NCC), a method that normalizes and calibrates predictions at the full-label level to address label length bias.

Result: NCC achieves statistically significant improvements over prior approaches across multiple datasets and models, with gains of up to 10% F1, and extends bias mitigation to broader tasks like multiple-choice question answering.

Conclusion: NCC improves LLM performance and robustness by mitigating full-label biases, making it less sensitive to few-shot example selection, requiring fewer examples for competitive performance, and producing more reliable confidence estimates.

Abstract: Large language models (LLMs) are powerful zero- and few-shot learners. However, when predicting over a set of candidate options, LLMs suffer from label biases, and existing calibration methods overlook biases arising from multi-token class labels. We tackle an issue we call label length bias, where labels of different lengths are treated inconsistently, even after standard length normalization. To mitigate it, we propose normalized contextual calibration (NCC), an effective method that normalizes and calibrates predictions at the full-label level. NCC achieves statistically significant improvements over prior approaches across multiple datasets and models, with gains of up to 10% F1. Moreover, NCC extends bias mitigation to broader tasks such as multiple-choice question answering. Our analysis shows that, when combined with in-context learning, NCC is less sensitive to few-shot example selection, requires fewer examples for competitive performance, and produces more reliable confidence estimates. These findings highlight the importance of mitigating full-label biases to improve the performance and robustness of LLM-based methods, particularly in real-world applications where class labels naturally consist of multiple tokens.

</details>


### [26] [Unified Defense for Large Language Models against Jailbreak and Fine-Tuning Attacks in Education](https://arxiv.org/abs/2511.14423)
*Xin Yi,Yue Li,Dongsheng Shi,Linlin Wang,Xiaoling Wang,Liang He*

Main category: cs.CL

TL;DR: EduHarm benchmark and TSSF framework for securing educational LLMs against jailbreak and fine-tuning attacks while maintaining utility.


<details>
  <summary>Details</summary>
Motivation: LLMs in education are vulnerable to safety attacks, but existing safety evaluations don't address educational-specific requirements, creating a gap in educational LLM security.

Method: Three-stage shield framework: 1) safety-aware attention realignment to restore harmfulness features, 2) layer-wise safety judgment to detect unsafe instructions, and 3) defense-driven dual routing to separate safe/unsafe queries.

Result: TSSF effectively strengthens safety across 8 jailbreak attack strategies while preventing over-refusal of benign queries, and provides robust defense against fine-tuning attacks while preserving utility gains.

Conclusion: The proposed framework successfully addresses educational LLM safety vulnerabilities through systematic detection and routing mechanisms that maintain both security and functionality.

Abstract: Large Language Models (LLMs) are increasingly integrated into educational applications. However, they remain vulnerable to jailbreak and fine-tuning attacks, which can compromise safety alignment and lead to harmful outputs. Existing studies mainly focus on general safety evaluations, with limited attention to the unique safety requirements of educational scenarios. To address this gap, we construct EduHarm, a benchmark containing safe-unsafe instruction pairs across five representative educational scenarios, enabling systematic safety evaluation of educational LLMs. Furthermore, we propose a three-stage shield framework (TSSF) for educational LLMs that simultaneously mitigates both jailbreak and fine-tuning attacks. First, safety-aware attention realignment redirects attention toward critical unsafe tokens, thereby restoring the harmfulness feature that discriminates between unsafe and safe inputs. Second, layer-wise safety judgment identifies harmfulness features by aggregating safety cues across multiple layers to detect unsafe instructions. Finally, defense-driven dual routing separates safe and unsafe queries, ensuring normal processing for benign inputs and guarded responses for harmful ones. Extensive experiments across eight jailbreak attack strategies demonstrate that TSSF effectively strengthens safety while preventing over-refusal of benign queries. Evaluations on three fine-tuning attack datasets further show that it consistently achieves robust defense against harmful queries while maintaining preserving utility gains from benign fine-tuning.

</details>


### [27] [MedBench v4: A Robust and Scalable Benchmark for Evaluating Chinese Medical Language Models, Multimodal Models, and Intelligent Agents](https://arxiv.org/abs/2511.14439)
*Jinru Ding,Lu Lu,Chao Ding,Mouxiao Bian,Jiayuan Chen,Renjie Lu,Wenrao Pang,Xiaoqin Wu,Zhiqiang Liu,Luyi Jiang,Bing Han,Yunqiu Wang,Jie Xu*

Main category: cs.CL

TL;DR: MedBench v4 is a comprehensive medical AI benchmarking platform with 700,000+ expert-curated tasks across 24 specialties, evaluating LLMs, multimodal models, and agents. Results show base LLMs score 54.1/100 with poor safety (18.4/100), multimodal models score 47.5/100, while agents significantly improve performance to 79.8/100.


<details>
  <summary>Details</summary>
Motivation: To create an evaluation framework that reflects real clinical workflows and safety constraints for advancing medical AI, addressing the need for comprehensive benchmarking of LLMs, multimodal models, and agents in healthcare settings.

Method: Developed a cloud-based benchmarking infrastructure with multi-stage refinement and multi-round review by clinicians from 500+ institutions, using LLM-as-a-judge calibrated to human ratings for open-ended responses, aligned with Chinese clinical guidelines.

Result: Base LLMs achieved mean score of 54.1/100 (Claude Sonnet 4.5 best at 62.5/100) but poor safety (18.4/100). Multimodal models scored 47.5/100 (GPT-5 best at 54.9/100) with weak cross-modal reasoning. Agents improved performance to 79.8/100, with Claude-based agents reaching 85.3/100 overall and 88.9/100 on safety.

Conclusion: Agentic orchestration significantly enhances clinical readiness without sacrificing capability, while base models show persistent gaps in multimodal reasoning and safety. The platform provides practical reference for hospitals, developers, and policymakers auditing medical AI.

Abstract: Recent advances in medical large language models (LLMs), multimodal models, and agents demand evaluation frameworks that reflect real clinical workflows and safety constraints. We present MedBench v4, a nationwide, cloud-based benchmarking infrastructure comprising over 700,000 expert-curated tasks spanning 24 primary and 91 secondary specialties, with dedicated tracks for LLMs, multimodal models, and agents. Items undergo multi-stage refinement and multi-round review by clinicians from more than 500 institutions, and open-ended responses are scored by an LLM-as-a-judge calibrated to human ratings. We evaluate 15 frontier models. Base LLMs reach a mean overall score of 54.1/100 (best: Claude Sonnet 4.5, 62.5/100), but safety and ethics remain low (18.4/100). Multimodal models perform worse overall (mean 47.5/100; best: GPT-5, 54.9/100), with solid perception yet weaker cross-modal reasoning. Agents built on the same backbones substantially improve end-to-end performance (mean 79.8/100), with Claude Sonnet 4.5-based agents achieving up to 85.3/100 overall and 88.9/100 on safety tasks. MedBench v4 thus reveals persisting gaps in multimodal reasoning and safety for base models, while showing that governance-aware agentic orchestration can markedly enhance benchmarked clinical readiness without sacrificing capability. By aligning tasks with Chinese clinical guidelines and regulatory priorities, the platform offers a practical reference for hospitals, developers, and policymakers auditing medical AI.

</details>


### [28] [Tell Me: An LLM-powered Mental Well-being Assistant with RAG, Synthetic Dialogue Generation, and Agentic Planning](https://arxiv.org/abs/2511.14445)
*Trishala Jayesh Ahalpara*

Main category: cs.CL

TL;DR: Tell Me is a mental well-being system using LLMs for personalized support, synthetic dialogue generation for research, and AI-driven self-care planning, designed as a reflective tool rather than therapy replacement.


<details>
  <summary>Details</summary>
Motivation: To provide accessible mental health support using AI, address the shortage of therapeutic data through synthetic generation, and create adaptive self-care tools that complement existing care.

Method: Three-component system: RAG assistant for personalized dialogue, synthetic client-therapist dialogue generator for research, and AI crew for weekly self-care plans and meditation audio using CrewAI framework.

Result: System successfully provides context-aware support, generates synthetic therapeutic dialogues, and creates personalized self-care plans. Evaluated through LLM-based judgments and human-user studies showing effectiveness in well-being scenarios.

Conclusion: Demonstrates how conversational AI can lower barriers to mental health support, enable responsible innovation through interdisciplinary collaboration, and bridge limitations of static well-being tools with adaptive, personalized approaches.

Abstract: We present Tell Me, a mental well-being system that leverages advances in large language models to provide accessible, context-aware support for users and researchers. The system integrates three components: (i) a retrieval-augmented generation (RAG) assistant for personalized, knowledge-grounded dialogue; (ii) a synthetic client-therapist dialogue generator conditioned on client profiles to facilitate research on therapeutic language and data augmentation; and (iii) a Well-being AI crew, implemented with CrewAI, that produces weekly self-care plans and guided meditation audio. The system is designed as a reflective space for emotional processing rather than a substitute for professional therapy. It illustrates how conversational assistants can lower barriers to support, complement existing care, and broaden access to mental health resources. To address the shortage of confidential therapeutic data, we introduce synthetic client-therapist dialogue generation conditioned on client profiles. Finally, the planner demonstrates an innovative agentic workflow for dynamically adaptive, personalized self-care, bridging the limitations of static well-being tools. We describe the architecture, demonstrate its functionalities, and report evaluation of the RAG assistant in curated well-being scenarios using both automatic LLM-based judgments and a human-user study. This work highlights opportunities for interdisciplinary collaboration between NLP researchers and mental health professionals to advance responsible innovation in human-AI interaction for well-being.

</details>


### [29] [Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning](https://arxiv.org/abs/2511.14460)
*Mingyue Cheng,Jie Ouyang,Shuo Yu,Ruiran Yan,Yucong Luo,Zirui Liu,Daoyu Wang,Qi Liu,Enhong Chen*

Main category: cs.CL

TL;DR: This paper introduces Agent-R1, a modular RL framework for training LLM Agents, and systematically extends MDP to define LLM Agent components, with validation on Multihop QA tasks.


<details>
  <summary>Details</summary>
Motivation: RL has significant potential for training LLM Agents but faces challenges due to lack of tailored approaches and flexible training frameworks in this emerging field.

Method: Systematically extend MDP framework to define LLM Agent components and introduce Agent-R1, a modular RL training framework for LLM Agents.

Result: Conducted experiments on Multihop QA benchmark tasks, providing initial validation for the effectiveness of the proposed methods and framework.

Conclusion: The paper advances RL methodologies for LLM Agents through systematic framework extension and introduces a practical training solution with promising initial results.

Abstract: Large Language Models (LLMs) are increasingly being explored for building Agents capable of active environmental interaction (e.g., via tool use) to solve complex problems. Reinforcement Learning (RL) is considered a key technology with significant potential for training such Agents; however, the effective application of RL to LLM Agents is still in its nascent stages and faces considerable challenges. Currently, this emerging field lacks in-depth exploration into RL approaches specifically tailored for the LLM Agent context, alongside a scarcity of flexible and easily extensible training frameworks designed for this purpose. To help advance this area, this paper first revisits and clarifies Reinforcement Learning methodologies for LLM Agents by systematically extending the Markov Decision Process (MDP) framework to comprehensively define the key components of an LLM Agent. Secondly, we introduce Agent-R1, a modular, flexible, and user-friendly training framework for RL-based LLM Agents, designed for straightforward adaptation across diverse task scenarios and interactive environments. We conducted experiments on Multihop QA benchmark tasks, providing initial validation for the effectiveness of our proposed methods and framework.

</details>


### [30] [LiveRAG: A diverse Q&A dataset with varying difficulty level for RAG evaluation](https://arxiv.org/abs/2511.14531)
*David Carmel,Simone Filice,Guy Horowitz,Yoelle Maarek,Alex Shtoff,Oren Somekh,Ran Tavory*

Main category: cs.CL

TL;DR: LiveRAG benchmark is a publicly available dataset of 895 synthetic Q&A pairs for systematic evaluation of RAG-based systems, derived from SIGIR'2025 LiveRAG Challenge with additional ground-truth information and difficulty metrics.


<details>
  <summary>Details</summary>
Motivation: There is an emerging need for systematically evaluating the effectiveness of Retrieval Augmented Generation (RAG) systems in generative AI solutions.

Method: Created a synthetic benchmark dataset of 895 questions and answers derived from SIGIR'2025 LiveRAG Challenge, augmented with ground-truth answers, supporting claims, and difficulty/discriminability scores using Item Response Theory.

Result: The benchmark demonstrates question diversity, wide range of difficulty levels, and usefulness in differentiating between system capabilities.

Conclusion: The LiveRAG benchmark will help advance RAG research, enable systematic evaluation, and support development of more robust Q&A systems.

Abstract: With Retrieval Augmented Generation (RAG) becoming more and more prominent in generative AI solutions, there is an emerging need for systematically evaluating their effectiveness. We introduce the LiveRAG benchmark, a publicly available dataset of 895 synthetic questions and answers designed to support systematic evaluation of RAG-based Q&A systems. This synthetic benchmark is derived from the one used during the SIGIR'2025 LiveRAG Challenge, where competitors were evaluated under strict time constraints. It is augmented with information that was not made available to competitors during the Challenge, such as the ground-truth answers, together with their associated supporting claims which were used for evaluating competitors' answers. In addition, each question is associated with estimated difficulty and discriminability scores, derived from applying an Item Response Theory model to competitors' responses. Our analysis highlights the benchmark's questions diversity, the wide range of their difficulty levels, and their usefulness in differentiating between system capabilities. The LiveRAG benchmark will hopefully help the community advance RAG research, conduct systematic evaluation, and develop more robust Q&A systems.

</details>


### [31] [Examining the Metrics for Document-Level Claim Extraction in Czech and Slovak](https://arxiv.org/abs/2511.14566)
*Lucia Makaiov,Martin Fajk,Antonn Jarolm*

Main category: cs.CL

TL;DR: This paper addresses document-level claim extraction evaluation by developing methods to align and compare claim sets from models and human annotators, with experiments on Czech and Slovak news comments.


<details>
  <summary>Details</summary>
Motivation: Document-level claim extraction is challenging in fact-checking, and evaluation methods for extracted claims have received limited attention, creating a need for reliable evaluation frameworks.

Method: The authors explore approaches to align two sets of claims from the same source document and compute similarity through alignment scores, investigating techniques to identify optimal alignment and evaluation methods between claim sets.

Result: Experiments on Czech and Slovak news comments reveal limitations of current evaluation approaches for document-level claim extraction, particularly in handling informal language, local context, and language subtleties.

Conclusion: There is a need for more advanced evaluation methods that can properly capture semantic similarity and assess essential claim properties like atomicity, checkworthiness, and decontextualization.

Abstract: Document-level claim extraction remains an open challenge in the field of fact-checking, and subsequently, methods for evaluating extracted claims have received limited attention. In this work, we explore approaches to aligning two sets of claims pertaining to the same source document and computing their similarity through an alignment score. We investigate techniques to identify the best possible alignment and evaluation method between claim sets, with the aim of providing a reliable evaluation framework. Our approach enables comparison between model-extracted and human-annotated claim sets, serving as a metric for assessing the extraction performance of models and also as a possible measure of inter-annotator agreement. We conduct experiments on newly collected dataset-claims extracted from comments under Czech and Slovak news articles-domains that pose additional challenges due to the informal language, strong local context, and subtleties of these closely related languages. The results draw attention to the limitations of current evaluation approaches when applied to document-level claim extraction and highlight the need for more advanced methods-ones able to correctly capture semantic similarity and evaluate essential claim properties such as atomicity, checkworthiness, and decontextualization.

</details>


### [32] [A Method for Characterizing Disease Progression from Acute Kidney Injury to Chronic Kidney Disease](https://arxiv.org/abs/2511.14603)
*Yilu Fang,Jordan G. Nestor,Casey N. Ta,Jerard Z. Kneifati-Hayek,Chunhua Weng*

Main category: cs.CL

TL;DR: This study uses EHR data to track AKI patients' clinical evolution and identify those at high risk of developing CKD through clustering and multi-state modeling, revealing 15 distinct post-AKI states with varying CKD progression probabilities.


<details>
  <summary>Details</summary>
Motivation: Patients with acute kidney injury (AKI) are at high risk of developing chronic kidney disease (CKD), but identifying those at greatest risk remains challenging, necessitating better methods for early detection and intervention.

Method: Used electronic health record data to dynamically track AKI patients' clinical evolution, identified post-AKI clinical states by clustering patient vectors from longitudinal medical codes and creatinine measurements, and estimated transition probabilities using multi-state modeling.

Result: Of 20,699 AKI patients, 3,491 (17%) developed CKD; identified 15 distinct post-AKI states with different CKD development probabilities; most patients (75%) remained in single state or made only one transition; identified both established and novel CKD risk factors with varying impact across clinical states.

Conclusion: The study demonstrates a data-driven approach for identifying high-risk AKI patients, supporting the development of decision-support tools for early CKD detection and intervention.

Abstract: Patients with acute kidney injury (AKI) are at high risk of developing chronic kidney disease (CKD), but identifying those at greatest risk remains challenging. We used electronic health record (EHR) data to dynamically track AKI patients' clinical evolution and characterize AKI-to-CKD progression. Post-AKI clinical states were identified by clustering patient vectors derived from longitudinal medical codes and creatinine measurements. Transition probabilities between states and progression to CKD were estimated using multi-state modeling. After identifying common post-AKI trajectories, CKD risk factors in AKI subpopulations were identified through survival analysis. Of 20,699 patients with AKI at admission, 3,491 (17%) developed CKD. We identified fifteen distinct post-AKI states, each with different probabilities of CKD development. Most patients (75%, n=15,607) remained in a single state or made only one transition during the study period. Both established (e.g., AKI severity, diabetes, hypertension, heart failure, liver disease) and novel CKD risk factors, with their impact varying across these clinical states. This study demonstrates a data-driven approach for identifying high-risk AKI patients, supporting the development of decision-support tools for early CKD detection and intervention.

</details>


### [33] [Bridging Human and Model Perspectives: A Comparative Analysis of Political Bias Detection in News Media Using Large Language Models](https://arxiv.org/abs/2511.14606)
*Shreya Adrita Banik,Niaz Nafi Rahman,Tahsina Moiukh,Farig Sadeque*

Main category: cs.CL

TL;DR: This study compares political bias detection between human annotations and multiple LLMs (GPT, BERT, RoBERTa, FLAN), finding that RoBERTa aligns best with human labels among traditional models, while GPT shows strongest zero-shot agreement.


<details>
  <summary>Details</summary>
Motivation: To understand how well large language models align with human judgment in detecting political bias in news media, as this alignment remains relatively underexplored despite advances in NLP.

Method: Constructed a manually annotated dataset of news articles and evaluated annotation consistency, bias polarity, and inter-model agreement using multiple LLMs including GPT, BERT, RoBERTa, and FLAN.

Result: RoBERTa achieved highest alignment with human labels among traditional transformer models, while GPT demonstrated strongest overall agreement in zero-shot setting. Fine-tuned RoBERTa achieved highest accuracy and strongest alignment with human annotations.

Conclusion: There are systematic differences in how humans and LLMs perceive political slant, highlighting the need for hybrid evaluation frameworks combining human interpretability with model scalability in automated media bias detection.

Abstract: Detecting political bias in news media is a complex task that requires interpreting subtle linguistic and contextual cues. Although recent advances in Natural Language Processing (NLP) have enabled automatic bias classification, the extent to which large language models (LLMs) align with human judgment still remains relatively underexplored and not yet well understood. This study aims to present a comparative framework for evaluating the detection of political bias across human annotations and multiple LLMs, including GPT, BERT, RoBERTa, and FLAN. We construct a manually annotated dataset of news articles and assess annotation consistency, bias polarity, and inter-model agreement to quantify divergence between human and model perceptions of bias. Experimental results show that among traditional transformer-based models, RoBERTa achieves the highest alignment with human labels, whereas generative models such as GPT demonstrate the strongest overall agreement with human annotations in a zero-shot setting. Among all transformer-based baselines, our fine-tuned RoBERTa model acquired the highest accuracy and the strongest alignment with human-annotated labels. Our findings highlight systematic differences in how humans and LLMs perceive political slant, underscoring the need for hybrid evaluation frameworks that combine human interpretability with model scalability in automated media bias detection.

</details>


### [34] [Enhancing Agentic Autonomous Scientific Discovery with Vision-Language Model Capabilities](https://arxiv.org/abs/2511.14631)
*Kahaan Gandhi,Boris Bolliet,Inigo Zubeldia*

Main category: cs.CL

TL;DR: Multi-agent systems using vision-language models (VLMs) as judges improve autonomous scientific discovery by evaluating plots against domain-specific rubrics, enabling real-time error correction and steering of data analysis.


<details>
  <summary>Details</summary>
Motivation: To enhance autonomous scientific discovery by enabling systems to self-correct errors and adapt to new datasets without human intervention, improving reliability and efficiency in data-driven exploration.

Method: Uses VLM-as-a-judge to evaluate figures against dynamically generated domain-specific rubrics, treating plots as verifiable checkpoints for multi-agent systems to correct reasoning errors in real-time.

Result: Achieved pass@1 scores of 0.7-0.8 on 10-task benchmark, significantly outperforming code-only (0.2-0.3) and code-and-text baselines (0.4-0.5), with successful case studies in cosmology and astrochemistry showing recovery from faulty reasoning.

Conclusion: VLM-augmented multi-agent systems substantially improve autonomous scientific discovery performance while providing auditable reasoning traces that enhance interpretability, demonstrating robust error correction and adaptation capabilities.

Abstract: We show that multi-agent systems guided by vision-language models (VLMs) improve end-to-end autonomous scientific discovery. By treating plots as verifiable checkpoints, a VLM-as-a-judge evaluates figures against dynamically generated domain-specific rubrics, enabling agents to correct their own errors and steer exploratory data analysis in real-time. Case studies in cosmology and astrochemistry demonstrate recovery from faulty reasoning paths and adaptation to new datasets without human intervention. On a 10-task benchmark for data-driven discovery, VLM-augmented systems achieve pass at 1 scores of 0.7-0.8, compared to 0.2-0.3 for code-only and 0.4-0.5 for code-and-text baselines, while also providing auditable reasoning traces that improve interpretability. Code available here: https://github.com/CMBAgents/cmbagent

</details>


### [35] [A Specialized Large Language Model for Clinical Reasoning and Diagnosis in Rare Diseases](https://arxiv.org/abs/2511.14638)
*Tao Yang,Dandan Huang,Yunting Lin,Pengfei Wu,Zhikun Wu,Gangyuan Ma,Yulan Lu,Xinran Dong,Dingpeng Li,Junshuang Ge,Zhiyan Zhang,Xuanzhao Huang,Wenyan Nong,Yao Zhou,Hui Tang,Hongxi Yang,Shijie Zhang,Juan Li,Xiaojun Cao,Lin Yang,Xia Gao,Kaishou Xu,Xiaoqiong Gu,Wen Zhang,Huimin Xia,Li Liu,Wenhao Zhou,Mulin Jun Li*

Main category: cs.CL

TL;DR: RareSeek R1 is a specialized clinical AI system that achieves state-of-the-art rare disease diagnosis accuracy by integrating narrative EHR data with knowledge graphs and retrieval mechanisms, performing on par with experienced physicians.


<details>
  <summary>Details</summary>
Motivation: Rare diseases affect millions worldwide with long diagnostic delays, and existing approaches face challenges including scarce real EHR data, outdated domain knowledge, and LLM hallucinations.

Method: Developed RareSeek R1 through staged instruction tuning, chain of thought learning, and graph-grounded retrieval using a large domain-specialized clinical corpus and clinician-validated reasoning set.

Result: Achieved state-of-the-art accuracy across multicenter EHR narratives and public benchmarks, with robust generalization and stability under noisy/overlapping phenotypes. Human studies showed performance comparable to experienced physicians.

Conclusion: This work advances a narrative-first, knowledge-integrated reasoning paradigm that shortens diagnostic odysseys and enables auditable, clinically translatable decision support.

Abstract: Rare diseases affect hundreds of millions worldwide, yet diagnosis often spans years. Convectional pipelines decouple noisy evidence extraction from downstream inferential diagnosis, and general/medical large language models (LLMs) face scarce real world electronic health records (EHRs), stale domain knowledge, and hallucinations. We assemble a large, domain specialized clinical corpus and a clinician validated reasoning set, and develop RareSeek R1 via staged instruction tuning, chain of thought learning, and graph grounded retrieval. Across multicenter EHR narratives and public benchmarks, RareSeek R1 attains state of the art accuracy, robust generalization, and stability under noisy or overlapping phenotypes. Augmented retrieval yields the largest gains when narratives pair with prioritized variants by resolving ambiguity and aligning candidates to mechanisms. Human studies show performance on par with experienced physicians and consistent gains in assistive use. Notably, transparent reasoning highlights decisive non phenotypic evidence (median 23.1%, such as imaging, interventions, functional tests) underpinning many correct diagnoses. This work advances a narrative first, knowledge integrated reasoning paradigm that shortens the diagnostic odyssey and enables auditable, clinically translatable decision support.

</details>


### [36] [Graded strength of comparative illusions is explained by Bayesian inference](https://arxiv.org/abs/2511.14642)
*Yuhan Zhang,Erxiao Wang,Cory Shain*

Main category: cs.CL

TL;DR: The study develops a quantitative model that explains the strength of comparative illusions in language processing using Bayesian noisy-channel inference, successfully predicting fine gradations in illusion strength and previously unexplained effects.


<details>
  <summary>Details</summary>
Motivation: To test and extend the noisy-channel theory of language comprehension by quantitatively modeling comparative illusions and explaining previously unexplained phenomena like the effect of pronominal vs. full noun phrase subjects.

Method: Developed a quantitative model of posterior probability for plausible interpretations by synthesizing statistical language models with human behavioral data, using Bayesian inference over a noisy channel framework.

Result: The model successfully explains fine gradations in comparative illusion strength and accounts for the previously unexplained effect of pronominal vs. full noun phrase than-clause subjects, with empirical validation.

Conclusion: The findings support noisy-channel inference as a unified computational-level theory of diverse language processing phenomena, including both illusory and non-illusory contexts.

Abstract: Like visual processing, language processing is susceptible to illusions in which people systematically misperceive stimuli. In one such case--the comparative illusion (CI), e.g., More students have been to Russia than I have--comprehenders tend to judge the sentence as acceptable despite its underlying nonsensical comparison. Prior research has argued that this phenomenon can be explained as Bayesian inference over a noisy channel: the posterior probability of an interpretation of a sentence is proportional to both the prior probability of that interpretation and the likelihood of corruption into the observed (CI) sentence. Initial behavioral work has supported this claim by evaluating a narrow set of alternative interpretations of CI sentences and showing that comprehenders favor interpretations that are more likely to have been corrupted into the illusory sentence. In this study, we replicate and go substantially beyond this earlier work by directly predicting the strength of illusion with a quantitative model of the posterior probability of plausible interpretations, which we derive through a novel synthesis of statistical language models with human behavioral data. Our model explains not only the fine gradations in the strength of CI effects, but also a previously unexplained effect caused by pronominal vs. full noun phrase than-clause subjects. These findings support a noisy-channel theory of sentence comprehension by demonstrating that the theory makes novel predictions about the comparative illusion that bear out empirically. This outcome joins related evidence of noisy channel processing in both illusory and non-illusory contexts to support noisy channel inference as a unified computational-level theory of diverse language processing phenomena.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [37] [Imagine in Space: Exploring the Frontier of Spatial Intelligence and Reasoning Efficiency in Vision Language Models](https://arxiv.org/abs/2511.13782)
*Xiaoxing Lian,Aidong Yang,Jun Zhu,Peng Wang,Yue Zhang*

Main category: cs.AI

TL;DR: SpatiaLite benchmark reveals VLMs struggle with visual spatial reasoning, relying heavily on linguistic representations and showing inefficiency in spatial transformations. An Imagery Driven Framework is proposed to improve internal world modeling.


<details>
  <summary>Details</summary>
Motivation: Current VLMs show remarkable reasoning capabilities but struggle with spatial reasoning tasks like mental rotation and navigation, which are fundamental to human cognition. The researchers hypothesize that imagination through internal spatial simulation is key.

Method: Introduced SpatiaLite, a fully synthetic benchmark to measure spatial reasoning accuracy and efficiency. Conducted comprehensive experiments on advanced VLMs and proposed an Imagery Driven Framework (IDF) for data synthesis and training.

Result: Three key findings: 1) VLMs rely on linguistic representations and fail at visual-centric spatial tasks, 2) VLMs show severe inefficiency with token usage growing rapidly with transformation complexity, 3) IDF can implicitly construct internal world models for spatial reasoning.

Conclusion: The work delineates spatial reasoning limits of advanced VLMs, identifies key shortcomings in their current mechanisms, and provides a framework (IDF) to inform future advances in building effective spatial reasoning capabilities.

Abstract: Large language models (LLMs) and vision language models (VLMs), such as DeepSeek R1,OpenAI o3, and Gemini 2.5 Pro, have demonstrated remarkable reasoning capabilities across logical inference, problem solving, and decision making. However, spatial reasoning:a fundamental component of human cognition that includes mental rotation, navigation, and spatial relationship comprehension remains a significant challenge for current advanced VLMs. We hypothesize that imagination, the internal simulation of spatial states, is the dominant reasoning mechanism within a spatial world model. To test this hypothesis and systematically probe current VLM spatial reasoning mechanisms, we introduce SpatiaLite, a fully synthetic benchmark that jointly measures spatial reasoning accuracy and reasoning efficiency. Comprehensive experiments reveal three key findings. First, advanced VLMs predominantly rely on linguistic representations for reasoning and imagination, resulting in significant deficiencies on visual centric tasks that demand perceptual spatial relations and 3D geometry transformations such as mental rotation or projection prediction. Second, advanced VLMs exhibit severe inefficiency in their current spatial reasoning mechanisms, with token usage growing rapidly as transformation complexity increases. Third, we propose an Imagery Driven Framework (IDF) for data synthesis and training, which can implicitly construct an internal world model that is critical for spatial reasoning in VLMs. Building on SpatiaLite, this work delineates the spatial reasoning limits and patterns of advanced VLMs, identifies key shortcomings, and informs future advances

</details>


### [38] [KANGURA: Kolmogorov-Arnold Network-Based Geometry-Aware Learning with Unified Representation Attention for 3D Modeling of Complex Structures](https://arxiv.org/abs/2511.13798)
*Mohammad Reza Shafie,Morteza Hajiabadi,Hamed Khosravi,Mobina Noori,Imtiaz Ahmed*

Main category: cs.AI

TL;DR: KANGURA is a novel 3D machine learning framework that uses Kolmogorov-Arnold Networks for geometry-aware learning to optimize microbial fuel cell anode structures, achieving state-of-the-art performance on both benchmarks and real-world applications.


<details>
  <summary>Details</summary>
Motivation: Existing predictive models struggle to capture complex geometric dependencies needed to optimize microbial fuel cell anode structures, which are crucial for sustainable energy generation.

Method: KANGURA formulates prediction as function decomposition using Kolmogorov-Arnold Networks, employs geometry-disentangled representation learning to separate structural variations, and uses unified attention mechanisms to enhance critical geometric regions.

Result: Achieved 92.7% accuracy on ModelNet40 benchmark (outperforming 15 SOTA models) and 97% accuracy on real-world MFC anode structure optimization problem.

Conclusion: KANGURA establishes a robust framework for 3D geometric modeling that unlocks new possibilities for optimizing complex structures in advanced manufacturing and quality-driven engineering applications.

Abstract: Microbial Fuel Cells (MFCs) offer a promising pathway for sustainable energy generation by converting organic matter into electricity through microbial processes. A key factor influencing MFC performance is the anode structure, where design and material properties play a crucial role. Existing predictive models struggle to capture the complex geometric dependencies necessary to optimize these structures. To solve this problem, we propose KANGURA: Kolmogorov-Arnold Network-Based Geometry-Aware Learning with Unified Representation Attention. KANGURA introduces a new approach to three-dimensional (3D) machine learning modeling. It formulates prediction as a function decomposition problem, where Kolmogorov-Arnold Network (KAN)- based representation learning reconstructs geometric relationships without a conventional multi- layer perceptron (MLP). To refine spatial understanding, geometry-disentangled representation learning separates structural variations into interpretable components, while unified attention mechanisms dynamically enhance critical geometric regions. Experimental results demonstrate that KANGURA outperforms over 15 state-of-the-art (SOTA) models on the ModelNet40 benchmark dataset, achieving 92.7% accuracy, and excels in a real-world MFC anode structure problem with 97% accuracy. This establishes KANGURA as a robust framework for 3D geometric modeling, unlocking new possibilities for optimizing complex structures in advanced manufacturing and quality-driven engineering applications.

</details>


### [39] [Causal computations in Semi Markovian Structural Causal Models using divide and conquer](https://arxiv.org/abs/2511.13852)
*Anna Rodum Bjru,Rafael Cabaas,Helge Langseth,Antonio Salmern*

Main category: cs.AI

TL;DR: Extension of Bjru et al.'s divide-and-conquer algorithm for bounding counterfactual probabilities from Markovian to semi-Markovian structural causal models, addressing challenges posed by confounding relationships.


<details>
  <summary>Details</summary>
Motivation: To extend counterfactual probability bounding methods to semi-Markovian SCMs, which can represent confounding relationships that Markovian models cannot handle, overcoming limitations of previous approaches.

Method: Investigated extension challenges using minimal examples, developed alternative solution strategies, and evaluated them both theoretically and through computational studies.

Result: Identified specific challenges in extending the methodology to semi-Markovian models and proposed alternative strategies to address these challenges.

Conclusion: The paper successfully explores the extension of counterfactual probability bounding methods to semi-Markovian SCMs, providing insights into the challenges and potential solution strategies for handling confounding relationships.

Abstract: Recently, Bjru et al. proposed a novel divide-and-conquer algorithm for bounding counterfactual probabilities in structural causal models (SCMs). They assumed that the SCMs were learned from purely observational data, leading to an imprecise characterization of the marginal distributions of exogenous variables. Their method leveraged the canonical representation of structural equations to decompose a general SCM with high-cardinality exogenous variables into a set of sub-models with low-cardinality exogenous variables. These sub-models had precise marginals over the exogenous variables and therefore admitted efficient exact inference. The aggregated results were used to bound counterfactual probabilities in the original model. The approach was developed for Markovian models, where each exogenous variable affects only a single endogenous variable. In this paper, we investigate extending the methodology to \textit{semi-Markovian} SCMs, where exogenous variables may influence multiple endogenous variables. Such models are capable of representing confounding relationships that Markovian models cannot. We illustrate the challenges of this extension using a minimal example, which motivates a set of alternative solution strategies. These strategies are evaluated both theoretically and through a computational study.

</details>


### [40] [Jailbreaking Large Vision Language Models in Intelligent Transportation Systems](https://arxiv.org/abs/2511.13892)
*Badhan Chandra Das,Md Tasnim Jawad,Md Jueal Mia,M. Hadi Amini,Yanzhao Wu*

Main category: cs.AI

TL;DR: This paper analyzes security vulnerabilities in Large Vision Language Models (LVLMs) used in Intelligent Transportation Systems, proposing a novel jailbreaking attack using image typography manipulation and multi-turn prompting, and developing a multi-layered defense mechanism.


<details>
  <summary>Details</summary>
Motivation: LVLMs are increasingly used in critical applications like Intelligent Transportation Systems but are highly vulnerable to jailbreaking attacks, posing serious security risks that need systematic analysis and mitigation.

Method: Constructed a transportation-specific harmful query dataset, developed a novel jailbreaking attack combining image typography manipulation with multi-turn prompting, and proposed a multi-layered response filtering defense technique.

Result: Extensive experiments on state-of-the-art LVLMs (both open-source and closed-source) showed the effectiveness of the proposed attack method, with evaluation using GPT-4 judgment and manual verification confirming severe security risks.

Conclusion: The study highlights critical security vulnerabilities in LVLMs integrated in ITS and demonstrates that image typography manipulation combined with multi-turn prompting poses significant jailbreaking risks, while the proposed multi-layered defense provides effective protection.

Abstract: Large Vision Language Models (LVLMs) demonstrate strong capabilities in multimodal reasoning and many real-world applications, such as visual question answering. However, LVLMs are highly vulnerable to jailbreaking attacks. This paper systematically analyzes the vulnerabilities of LVLMs integrated in Intelligent Transportation Systems (ITS) under carefully crafted jailbreaking attacks. First, we carefully construct a dataset with harmful queries relevant to transportation, following OpenAI's prohibited categories to which the LVLMs should not respond. Second, we introduce a novel jailbreaking attack that exploits the vulnerabilities of LVLMs through image typography manipulation and multi-turn prompting. Third, we propose a multi-layered response filtering defense technique to prevent the model from generating inappropriate responses. We perform extensive experiments with the proposed attack and defense on the state-of-the-art LVLMs (both open-source and closed-source). To evaluate the attack method and defense technique, we use GPT-4's judgment to determine the toxicity score of the generated responses, as well as manual verification. Further, we compare our proposed jailbreaking method with existing jailbreaking techniques and highlight severe security risks involved with jailbreaking attacks with image typography manipulation and multi-turn prompting in the LVLMs integrated in ITS.

</details>


### [41] [CORGI: Efficient Pattern Matching With Quadratic Guarantees](https://arxiv.org/abs/2511.13942)
*Daniel Weitekamp*

Main category: cs.AI

TL;DR: CORGI is a new pattern-matching algorithm that provides quadratic time/space guarantees for finding matches, addressing exponential complexity issues in rule-based systems by using a two-step graph approach instead of traditional RETE's -memory.


<details>
  <summary>Details</summary>
Motivation: Rule-based systems face exponential complexity when handling rules with many underconstrained variables or combinatorial partial matches, especially when AI systems automatically generate rules through induction or synthesis, leading to performance degradation and memory overflows.

Method: CORGI uses a two-step approach: building/maintaining a graph of grounded relations in a forward pass, then generating matches iteratively by working backward through the graph, eliminating the need for traditional -memory and full conflict sets.

Result: Performance evaluation shows CORGI significantly outperforms RETE implementations from SOAR and OPS5 on combinatorial matching tasks, avoiding high-latency delays and memory overflows.

Conclusion: CORGI provides a practical solution for real-time rule-based systems by offering predictable performance guarantees and iterative match streaming, making it suitable for AI planning, reactive control, and database applications without requiring hand-engineered constraints.

Abstract: Rule-based systems must solve complex matching problems within tight time constraints to be effective in real-time applications, such as planning and reactive control for AI agents, as well as low-latency relational database querying. Pattern-matching systems can encounter issues where exponential time and space are required to find matches for rules with many underconstrained variables, or which produce combinatorial intermediate partial matches (but are otherwise well-constrained). When online AI systems automatically generate rules from example-driven induction or code synthesis, they can easily produce worst-case matching patterns that slow or halt program execution by exceeding available memory. In our own work with cognitive systems that learn from example, we've found that aggressive forms of anti-unification-based generalization can easily produce these circumstances. To make these systems practical without hand-engineering constraints or succumbing to unpredictable failure modes, we introduce a new matching algorithm called CORGI (Collection-Oriented Relational Graph Iteration). Unlike RETE-based approaches, CORGI offers quadratic time and space guarantees for finding single satisficing matches, and the ability to iteratively stream subsequent matches without committing entire conflict sets to memory. CORGI differs from RETE in that it does not have a traditional $$-memory for collecting partial matches. Instead, CORGI takes a two-step approach: a graph of grounded relations is built/maintained in a forward pass, and an iterator generates matches as needed by working backward through the graph. This approach eliminates the high-latency delays and memory overflows that can result from populating full conflict sets. In a performance evaluation, we demonstrate that CORGI significantly outperforms RETE implementations from SOAR and OPS5 on a simple combinatorial matching task.

</details>


### [42] [Scene Graph-Guided Generative AI Framework for Synthesizing and Evaluating Industrial Hazard Scenarios](https://arxiv.org/abs/2511.13970)
*Sanjay Acharjee,Abir Khan Ratul,Diego Patino,Md Nazmus Sakib*

Main category: cs.AI

TL;DR: A generative AI framework that synthesizes photorealistic workplace hazard images using scene graphs derived from OSHA accident reports, with a VQA-based evaluation method that outperforms existing metrics.


<details>
  <summary>Details</summary>
Motivation: Training vision models for workplace hazard detection requires realistic images of unsafe conditions, but capturing actual accident-triggering scenarios is nearly impossible, creating a data acquisition challenge.

Method: OSHA accident reports are analyzed using GPT-4o to extract structured hazard reasoning, converted into object-level scene graphs that guide a text-to-image diffusion model to generate compositionally accurate hazard scenes, with evaluation using a VQA framework.

Result: The proposed VQA Graph Score outperforms CLIP and BLIP metrics across four state-of-the-art generative models based on entropy-based validation, confirming its higher discriminative sensitivity for evaluating realism and semantic fidelity.

Conclusion: The scene graph-guided generative AI framework successfully synthesizes realistic workplace hazard images from OSHA reports, with the VQA-based evaluation method providing superior assessment of generative model performance compared to existing metrics.

Abstract: Training vision models to detect workplace hazards accurately requires realistic images of unsafe conditions that could lead to accidents. However, acquiring such datasets is difficult because capturing accident-triggering scenarios as they occur is nearly impossible. To overcome this limitation, this study presents a novel scene graph-guided generative AI framework that synthesizes photorealistic images of hazardous scenarios grounded in historical Occupational Safety and Health Administration (OSHA) accident reports. OSHA narratives are analyzed using GPT-4o to extract structured hazard reasoning, which is converted into object-level scene graphs capturing spatial and contextual relationships essential for understanding risk. These graphs guide a text-to-image diffusion model to generate compositionally accurate hazard scenes. To evaluate the realism and semantic fidelity of the generated data, a visual question answering (VQA) framework is introduced. Across four state-of-the-art generative models, the proposed VQA Graph Score outperforms CLIP and BLIP metrics based on entropy-based validation, confirming its higher discriminative sensitivity.

</details>


### [43] [Artificial Intelligence Agents in Music Analysis: An Integrative Perspective Based on Two Use Cases](https://arxiv.org/abs/2511.13987)
*Antonio Manuel Martnez-Heredia,Dolores Godrid Rodrguez,Andrs Ortiz Garca*

Main category: cs.AI

TL;DR: This paper reviews AI agents in music analysis and education, showing they outperform traditional methods in pattern recognition and educational feedback, while highlighting challenges like transparency and bias.


<details>
  <summary>Details</summary>
Motivation: To synthesize the evolution of AI in music and evaluate its pedagogical implications through experimental validation, addressing the gap between technical capabilities and educational applications.

Method: Integrative review combined with dual-case experimental methodology: (1) generative AI platforms in secondary education, (2) multiagent system for symbolic music analysis with modular, scalable workflows.

Result: AI agents effectively enhance musical pattern recognition, compositional parameterization, and educational feedback, outperforming traditional automated methods in interpretability and adaptability.

Conclusion: The research provides a unified framework bridging technical, pedagogical, and ethical considerations, offering evidence-based guidance for responsible AI deployment in computational musicology and education.

Abstract: This paper presents an integrative review and experimental validation of artificial intelligence (AI) agents applied to music analysis and education. We synthesize the historical evolution from rule-based models to contemporary approaches involving deep learning, multi-agent architectures, and retrieval-augmented generation (RAG) frameworks. The pedagogical implications are evaluated through a dual-case methodology: (1) the use of generative AI platforms in secondary education to foster analytical and creative skills; (2) the design of a multiagent system for symbolic music analysis, enabling modular, scalable, and explainable workflows.
  Experimental results demonstrate that AI agents effectively enhance musical pattern recognition, compositional parameterization, and educational feedback, outperforming traditional automated methods in terms of interpretability and adaptability. The findings highlight key challenges concerning transparency, cultural bias, and the definition of hybrid evaluation metrics, emphasizing the need for responsible deployment of AI in educational environments.
  This research contributes to a unified framework that bridges technical, pedagogical, and ethical considerations, offering evidence-based guidance for the design and application of intelligent agents in computational musicology and music education.

</details>


### [44] [ALEX:A Light Editing-knowledge Extractor](https://arxiv.org/abs/2511.14018)
*Minghu Wang,Shuliang Zhao,Yuanyuan Zhao,Hongxia Xu*

Main category: cs.AI

TL;DR: ALEX is a lightweight knowledge editing framework that uses hierarchical memory architecture to efficiently handle multi-hop questions, reducing retrieval complexity from O(N) to O(K+N/C) while improving accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: Large Language Models struggle with adapting to evolving information due to their static knowledge, and existing knowledge editing methods face scalability and retrieval efficiency challenges, especially for complex multi-hop questions requiring multi-step reasoning.

Method: ALEX features a hierarchical memory architecture that organizes knowledge edits into semantic clusters, an Inferential Query Synthesis module to bridge semantic gaps between queries and facts, and a Dynamic Evidence Adjudication engine for efficient two-stage retrieval.

Result: Experiments on MQUAKE benchmark show significant improvements in multi-hop answer accuracy (MultiHop-ACC) and reasoning path reliability (HopWise-ACC), with over 80% reduction in required search space.

Conclusion: ALEX presents a promising approach for building scalable, efficient, and accurate knowledge editing systems by addressing fundamental challenges in retrieval complexity and multi-hop reasoning.

Abstract: The static nature of knowledge within Large Language Models (LLMs) makes it difficult for them to adapt to evolving information, rendering knowledge editing a critical task. However, existing methods struggle with challenges of scalability and retrieval efficiency, particularly when handling complex, multi-hop questions that require multi-step reasoning. To address these challenges, this paper introduces ALEX (A Light Editing-knowledge Extractor), a lightweight knowledge editing framework. The core innovation of ALEX is its hierarchical memory architecture, which organizes knowledge updates (edits) into semantic clusters. This design fundamentally reduces retrieval complexity from a linear O(N) to a highly scalable O(K+N/C). Furthermore, the framework integrates an Inferential Query Synthesis (IQS) module to bridge the semantic gap between queries and facts , and a Dynamic Evidence Adjudication (DEA) engine that executes an efficient two-stage retrieval process. Experiments on the MQUAKE benchmark demonstrate that ALEX significantly improves both the accuracy of multi-hop answers (MultiHop-ACC) and the reliability of reasoning paths (HopWise-ACC). It also reduces the required search space by over 80% , presenting a promising path toward building scalable, efficient, and accurate knowledge editing systems.

</details>


### [45] [Syn-STARTS: Synthesized START Triage Scenario Generation Framework for Scalable LLM Evaluation](https://arxiv.org/abs/2511.14023)
*Chiharu Hagiwara,Naoki Nonaka,Yuhta Hashimoto,Ryu Uchimido,Jun Seita*

Main category: cs.AI

TL;DR: Syn-STARTS is an LLM-based framework that generates synthetic triage cases for mass casualty incidents, showing comparable quality to manually curated datasets and enabling stable AI model development for emergency medical situations.


<details>
  <summary>Details</summary>
Motivation: Mass casualty incidents require effective triage decision-making but lack sufficient real-world data for AI development due to infrequent occurrences and difficulty in data collection at emergency scenes.

Method: Developed Syn-STARTS framework using Large Language Models (LLMs) to generate synthetic triage cases, then compared the quality against manually curated TRIAGE open dataset and evaluated stability across different triage categories (green, yellow, red, black) defined by START standard.

Result: Generated triage cases were qualitatively indistinguishable from manually curated datasets, and LLM accuracy evaluation showed high stability across hundreds of cases in all four triage categories.

Conclusion: Synthetic data generation using LLMs shows strong potential for developing high-performance AI models in critical medical situations where real-world data is scarce.

Abstract: Triage is a critically important decision-making process in mass casualty incidents (MCIs) to maximize victim survival rates. While the role of AI in such situations is gaining attention for making optimal decisions within limited resources and time, its development and performance evaluation require benchmark datasets of sufficient quantity and quality. However, MCIs occur infrequently, and sufficient records are difficult to accumulate at the scene, making it challenging to collect large-scale realworld data for research use. Therefore, we developed Syn-STARTS, a framework that uses LLMs to generate triage cases, and verified its effectiveness. The results showed that the triage cases generated by Syn-STARTS were qualitatively indistinguishable from the TRIAGE open dataset generated by manual curation from training materials. Furthermore, when evaluating the LLM accuracy using hundreds of cases each from the green, yellow, red, and black categories defined by the standard triage method START, the results were found to be highly stable. This strongly indicates the possibility of synthetic data in developing high-performance AI models for severe and critical medical situations.

</details>


### [46] [Making Evidence Actionable in Adaptive Learning](https://arxiv.org/abs/2511.14052)
*Amirreza Mehrabi,Jason W. Morphew,Breejha Quezada,N. Sanjay Rebello*

Main category: cs.AI

TL;DR: This paper presents an adaptive learning system with three safeguards (adequacy, attention, diversity) that converts concept-level assessments into vetted micro-interventions using binary integer programming and optimization methods.


<details>
  <summary>Details</summary>
Motivation: To address the problem in adaptive learning where precise diagnosis is followed by weak interventions that are mistimed or misaligned, creating a need for better intervention timing and alignment.

Method: Uses a binary integer program with constraints for coverage, time, difficulty windows, prerequisites, and anti-redundancy. Employs greedy selection for low-richness scenarios, gradient-based relaxation for rich repositories, and a hybrid method for transition.

Result: In simulations and deployment with 1,204 physics students, both solvers achieved full skill coverage within bounded watch time. Gradient-based method reduced redundancy by ~12% vs greedy while harmonizing difficulty, while greedy offered comparable adequacy with lower computational cost.

Conclusion: The system provides a tractable, auditable controller that closes the diagnostic-pedagogical loop and delivers equitable, load-aware personalization at classroom scale through localized missing content identification and targeted curation.

Abstract: Adaptive learning often diagnoses precisely yet intervenes weakly, yielding help that is mistimed or misaligned. This study presents evidence supporting an instructor-governed feedback loop that converts concept-level assessment evidence into vetted micro-interventions. The adaptive learning algorithm contains three safeguards: adequacy as a hard guarantee of gap closure, attention as a budgeted constraint for time and redundancy, and diversity as protection against overfitting to a single resource. We formalize intervention assignment as a binary integer program with constraints for coverage, time, difficulty windows informed by ability estimates, prerequisites encoded by a concept matrix, and anti-redundancy enforced through diversity. Greedy selection serves low-richness and tight-latency regimes, gradient-based relaxation serves rich repositories, and a hybrid method transitions along a richness-latency frontier. In simulation and in an introductory physics deployment with one thousand two hundred four students, both solvers achieved full skill coverage for essentially all learners within bounded watch time. The gradient-based method reduced redundant coverage by approximately twelve percentage points relative to greedy and harmonized difficulty across slates, while greedy delivered comparable adequacy with lower computational cost in scarce settings. Slack variables localized missing content and supported targeted curation, sustaining sufficiency across subgroups. The result is a tractable and auditable controller that closes the diagnostic-pedagogical loop and delivers equitable, load-aware personalization at classroom scale.

</details>


### [47] [Collaborative QA using Interacting LLMs. Impact of Network Structure, Node Capability and Distributed Data](https://arxiv.org/abs/2511.14098)
*Adit Jain,Vikram Krishnamurthy,Yiming Zhang*

Main category: cs.AI

TL;DR: This paper models how networks of LLMs perform collaborative question-answering and analyzes how hallucinations spread through such networks, using mean-field dynamics and randomized utility models.


<details>
  <summary>Details</summary>
Motivation: LLMs often hallucinate when direct evidence is lacking, and these effects become more pronounced in networks of interacting LLMs where hallucinations can spread and cause previously accurate LLMs to hallucinate.

Method: Combines mean-field dynamics from network science and randomized utility model from economics to construct a generative model. Models LLMs with latent states (truthful/not truthful) and analyzes diffusion of information in directed networks using MFD with probabilities governed by randomized utility model.

Result: For networks where each LLM has two possible latent states, the paper establishes sufficient conditions for existence and uniqueness of fixed points and analyzes how fixed points behave with respect to incentives given to individual LLMs. Experimental analysis with 100 open-source LLMs examines data heterogeneity, node capability, network structure, and sensitivity to framing.

Conclusion: The paper provides a theoretical framework for understanding how hallucinations propagate in networks of interacting LLMs and demonstrates experimental validation of the model's predictions regarding network behavior under various conditions.

Abstract: In this paper, we model and analyze how a network of interacting LLMs performs collaborative question-answering (CQA) in order to estimate a ground truth given a distributed set of documents. This problem is interesting because LLMs often hallucinate when direct evidence to answer a question is lacking, and these effects become more pronounced in a network of interacting LLMs. The hallucination spreads, causing previously accurate LLMs to hallucinate. We study interacting LLMs and their hallucination by combining novel ideas of mean-field dynamics (MFD) from network science and the randomized utility model from economics to construct a useful generative model. We model the LLM with a latent state that indicates if it is truthful or not with respect to the ground truth, and extend a tractable analytical model considering an MFD to model the diffusion of information in a directed network of LLMs. To specify the probabilities that govern the dynamics of the MFD, we propose a randomized utility model. For a network of LLMs, where each LLM has two possible latent states, we posit sufficient conditions for the existence and uniqueness of a fixed point and analyze the behavior of the fixed point in terms of the incentive (e.g., test-time compute) given to individual LLMs. We experimentally study and analyze the behavior of a network of $100$ open-source LLMs with respect to data heterogeneity, node capability, network structure, and sensitivity to framing on multiple semi-synthetic datasets.

</details>


### [48] [APD-Agents: A Large Language Model-Driven Multi-Agents Collaborative Framework for Automated Page Design](https://arxiv.org/abs/2511.14101)
*Xinpeng Chen,Xiaofeng Han,Kaihao Zhang,Guochao Ren,Yujie Wang,Wenhao Cao,Yang Zhou,Jianfeng Lu,Zhenbo Song*

Main category: cs.AI

TL;DR: APD-agents is an LLM-driven multi-agent framework that automates mobile app page layout design by dynamically coordinating specialized agents to parse user descriptions, generate layouts, retrieve templates, and recursively create fine-grained components.


<details>
  <summary>Details</summary>
Motivation: Mobile app layout design is time-consuming and requires extensive training with design software, while collaborative design across pages demands additional effort for consistency. The authors aim to automate this process using AI.

Method: A multi-agent framework with five specialized agents: OrchestratorAgent (coordinates tasks), SemanticParserAgent (converts descriptions to structured data), PrimaryLayoutAgent (generates coarse layouts), TemplateRetrievalAgent (fetches relevant examples), and RecursiveComponentAgent (handles fine-grained sub-elements).

Result: Experimental results on the RICO dataset demonstrate that APD-agents achieve state-of-the-art performance in automated mobile app page design.

Conclusion: The framework successfully leverages LLM-driven multi-agent collaboration to automate mobile app page design, reducing manual effort while maintaining high-quality layout generation.

Abstract: Layout design is a crucial step in developing mobile app pages. However, crafting satisfactory designs is time-intensive for designers: they need to consider which controls and content to present on the page, and then repeatedly adjust their size, position, and style for better aesthetics and structure. Although many design software can now help to perform these repetitive tasks, extensive training is needed to use them effectively. Moreover, collaborative design across app pages demands extra time to align standards and ensure consistent styling. In this work, we propose APD-agents, a large language model (LLM) driven multi-agent framework for automated page design in mobile applications. Our framework contains OrchestratorAgent, SemanticParserAgent, PrimaryLayoutAgent, TemplateRetrievalAgent, and RecursiveComponentAgent. Upon receiving the user's description of the page, the OrchestratorAgent can dynamically can direct other agents to accomplish users' design task. To be specific, the SemanticParserAgent is responsible for converting users' descriptions of page content into structured data. The PrimaryLayoutAgent can generate an initial coarse-grained layout of this page. The TemplateRetrievalAgent can fetch semantically relevant few-shot examples and enhance the quality of layout generation. Besides, a RecursiveComponentAgent can be used to decide how to recursively generate all the fine-grained sub-elements it contains for each element in the layout. Our work fully leverages the automatic collaboration capabilities of large-model-driven multi-agent systems. Experimental results on the RICO dataset show that our APD-agents achieve state-of-the-art performance.

</details>


### [49] [Run, Ruminate, and Regulate: A Dual-process Thinking System for Vision-and-Language Navigation](https://arxiv.org/abs/2511.14131)
*Yu Zhong,Zihao Zhang,Rui Zhang,Lingdong Huang,Haihan Gao,Shuo Wang,Da Li,Ruijian Han,Jiaming Guo,Shaohui Peng,Di Huang,Yunji Chen*

Main category: cs.AI

TL;DR: R3 is a dual-process framework that combines LLMs' generalization with VLN expertise using Runner, Ruminator, and Regulator modules to improve Vision-and-Language Navigation performance while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Address the performance gap between LLM-based approaches and domain experts in VLN tasks, while reducing the substantial computational cost and inference latency of using LLMs directly.

Method: Proposes R3 framework with three modules: Runner (lightweight transformer expert for efficient navigation), Ruminator (powerful multimodal LLM with chain-of-thought reasoning), and Regulator (monitors progress and controls thinking modes).

Result: Significantly outperforms state-of-the-art methods, achieving 3.28% and 3.30% improvements in SPL and RGSPL respectively on REVERIE benchmark.

Conclusion: R3 framework effectively handles challenging VLN tasks by harmoniously integrating LLMs' generalization capabilities with VLN-specific expertise in a zero-shot manner.

Abstract: Vision-and-Language Navigation (VLN) requires an agent to dynamically explore complex 3D environments following human instructions. Recent research underscores the potential of harnessing large language models (LLMs) for VLN, given their commonsense knowledge and general reasoning capabilities. Despite their strengths, a substantial gap in task completion performance persists between LLM-based approaches and domain experts, as LLMs inherently struggle to comprehend real-world spatial correlations precisely. Additionally, introducing LLMs is accompanied with substantial computational cost and inference latency. To address these issues, we propose a novel dual-process thinking framework dubbed R3, integrating LLMs' generalization capabilities with VLN-specific expertise in a zero-shot manner. The framework comprises three core modules: Runner, Ruminator, and Regulator. The Runner is a lightweight transformer-based expert model that ensures efficient and accurate navigation under regular circumstances. The Ruminator employs a powerful multimodal LLM as the backbone and adopts chain-of-thought (CoT) prompting to elicit structured reasoning. The Regulator monitors the navigation progress and controls the appropriate thinking mode according to three criteria, integrating Runner and Ruminator harmoniously. Experimental results illustrate that R3 significantly outperforms other state-of-the-art methods, exceeding 3.28% and 3.30% in SPL and RGSPL respectively on the REVERIE benchmark. This pronounced enhancement highlights the effectiveness of our method in handling challenging VLN tasks.

</details>


### [50] [Beyond Accuracy: A Multi-Dimensional Framework for Evaluating Enterprise Agentic AI Systems](https://arxiv.org/abs/2511.14136)
*Sushant Mehta*

Main category: cs.AI

TL;DR: The paper identifies limitations in current AI agent benchmarks that focus only on accuracy while ignoring enterprise needs like cost, reliability, and operational stability. It proposes CLEAR, a holistic evaluation framework addressing these gaps.


<details>
  <summary>Details</summary>
Motivation: Current agentic AI benchmarks overlook critical enterprise requirements such as cost-efficiency, reliability, and operational stability, leading to suboptimal deployment decisions.

Method: Systematic analysis of 12 benchmarks and empirical evaluation of state-of-the-art agents, followed by proposing the CLEAR framework (Cost, Latency, Efficacy, Assurance, Reliability) and evaluating six leading agents on 300 enterprise tasks.

Result: Found 50x cost variations for similar precision, agent performance dropping from 60% to 25% in consistency tests, and demonstrated that accuracy-optimized agents are 4.4-10.8x more expensive than cost-aware alternatives. CLEAR showed better production success prediction (=0.83) vs accuracy-only (=0.41).

Conclusion: The CLEAR framework provides a more comprehensive evaluation approach for enterprise AI agent deployment, addressing critical gaps in current benchmarking practices and enabling better cost-performance tradeoffs.

Abstract: Current agentic AI benchmarks predominantly evaluate task completion accuracy, while overlooking critical enterprise requirements such as cost-efficiency, reliability, and operational stability. Through systematic analysis of 12 main benchmarks and empirical evaluation of state-of-the-art agents, we identify three fundamental limitations: (1) absence of cost-controlled evaluation leading to 50x cost variations for similar precision, (2) inadequate reliability assessment where agent performance drops from 60\% (single run) to 25\% (8-run consistency), and (3) missing multidimensional metrics for security, latency, and policy compliance. We propose \textbf{CLEAR} (Cost, Latency, Efficacy, Assurance, Reliability), a holistic evaluation framework specifically designed for enterprise deployment. Evaluation of six leading agents on 300 enterprise tasks demonstrates that optimizing for accuracy alone yields agents 4.4-10.8x more expensive than cost-aware alternatives with comparable performance. Expert evaluation (N=15) confirms that CLEAR better predicts production success (correlation $=0.83$) compared to accuracy-only evaluation ($=0.41$).

</details>


### [51] [HFL-FlowLLM: Large Language Models for Network Traffic Flow Classification in Heterogeneous Federated Learning](https://arxiv.org/abs/2511.14199)
*Jiazhuo Tian,Yachao Yuan*

Main category: cs.AI

TL;DR: HFL-FlowLLM is the first framework applying large language models to network traffic flow classification in heterogeneous federated learning, achieving 13% F1 score improvement over state-of-the-art methods and 87% cost reduction.


<details>
  <summary>Details</summary>
Motivation: Traditional centralized ML struggles with distributed data and privacy in 5G/IoT networks, while existing federated learning approaches have high costs and poor generalization for network traffic classification.

Method: Proposed HFL-FlowLLM framework that applies large language models to network traffic flow classification in heterogeneous federated learning environments.

Result: 13% average F1 score improvement over state-of-the-art methods, up to 5% F1 improvement with more clients, and 87% training cost reduction compared to existing LLM federated learning frameworks.

Conclusion: HFL-FlowLLM demonstrates strong potential and practical value for modern communication network security with compelling performance and robustness.

Abstract: In modern communication networks driven by 5G and the Internet of Things (IoT), effective network traffic flow classification is crucial for Quality of Service (QoS) management and security. Traditional centralized machine learning struggles with the distributed data and privacy concerns in these heterogeneous environments, while existing federated learning approaches suffer from high costs and poor generalization. To address these challenges, we propose HFL-FlowLLM, which to our knowledge is the first framework to apply large language models to network traffic flow classification in heterogeneous federated learning. Compared to state-of-the-art heterogeneous federated learning methods for network traffic flow classification, the proposed approach improves the average F1 score by approximately 13%, demonstrating compelling performance and strong robustness. When compared to existing large language models federated learning frameworks, as the number of clients participating in each training round increases, the proposed method achieves up to a 5% improvement in average F1 score while reducing the training costs by about 87%. These findings prove the potential and practical value of HFL-FlowLLM in modern communication networks security.

</details>


### [52] [Do Large Language Models (LLMs) Understand Chronology?](https://arxiv.org/abs/2511.14214)
*Pattaraphon Kenny Wongchamcharoen,Paul Glasserman*

Main category: cs.AI

TL;DR: LLMs struggle with chronological ordering tasks, especially with longer sequences, but explicit reasoning budget significantly improves performance, with GPT-5 achieving flawless ordering at medium/high reasoning effort.


<details>
  <summary>Details</summary>
Motivation: To test whether LLMs understand chronology, which is crucial for their application in finance and economics where look-ahead bias is a concern.

Method: Evaluated multiple LLMs (GPT-4.1, Claude-3.7 Sonnet, GPT-5) on three chronological tasks: chronological ordering, conditional sorting, and anachronism detection, across different reasoning-effort settings.

Result: Exact match rates drop sharply with longer sequences, though rank correlations remain high. GPT-5 and Claude-3.7 Sonnet with Extended Thinking outperform normal models. Anachronism detection is the easiest task but performance declines with complex timelines.

Conclusion: Explicit reasoning budget helps with chronological tasks, with GPT-5 achieving perfect performance at medium/high effort. Current LLMs have limitations on chronological understanding, which is important for financial applications.

Abstract: Large language models (LLMs) are increasingly used in finance and economics, where prompt-based attempts against look-ahead bias implicitly assume that models understand chronology. We test this fundamental question with a series of chronological ordering tasks with increasing complexities over facts the model already knows from pre-training. Our tasks cover (1) chronological ordering, (2) conditional sorting (filter, then order), and (3) anachronism detection. We evaluate GPT-4.1, Claude-3.7 Sonnet, with and without Extended Thinking (ET), and GPT-5 across multiple reasoning-effort settings. Across models, Exact match rate drops sharply as sequences lengthen even while rank correlations stay high as LLMs largely preserve local order but struggle to maintain a single globally consistent timeline. In conditional sorting, most failures stem from the filtering step rather than the ordering step, but GPT-5 and Claude-3.7 Sonnet with Extended Thinking outshine normal models significantly. Lastly, anachronism detection is found to be the easiest task for the LLMs but performance still declines with increasingly overlapping timelines or entities. Overall, our main contribution is showing that allocating explicit reasoning budget helps with chronological ordering with GPT-5 at medium/high reasoning effort achieving flawless ordering at all lengths and perfect conditional sorting (both self-filtered and given-subset), whereas low/minimal effort degrades with longer lists, mirroring earlier models. Our findings delineate limits of current LLMs on chronological tasks, providing insights into task complexity, and demonstrate scenarios in which reasoning helps. These patterns are important for the real-time application of LLMs in finance. We release all code and evaluation templates to support full reproducibility.

</details>


### [53] [Listen Like a Teacher: Mitigating Whisper Hallucinations using Adaptive Layer Attention and Knowledge Distillation](https://arxiv.org/abs/2511.14219)
*Kumud Tripathi,Aditya Srinivas Menon,Aman Gaurav,Raj Prakash Gohil,Pankaj Wasnik*

Main category: cs.AI

TL;DR: This paper presents a two-stage architecture to reduce hallucinations in Whisper ASR systems under noisy conditions, using Adaptive Layer Attention for robust encoding and multi-objective knowledge distillation to suppress hallucinations.


<details>
  <summary>Details</summary>
Motivation: Whisper model suffers from hallucination errors especially under noisy acoustic conditions, and previous approaches focused on audio preprocessing or post-processing rather than modifying the model itself to directly mitigate hallucinations.

Method: Two-stage approach: 1) Adaptive Layer Attention groups encoder layers into semantically coherent blocks and fuses them via multi-head attention; 2) Multi-objective knowledge distillation trains student on noisy audio to align with teacher processing clean inputs.

Result: Experiments on noisy speech benchmarks show notable reductions in hallucinations and word error rates while preserving performance on clean speech.

Conclusion: The combination of ALA and KD provides a principled strategy to improve Whisper's reliability under real-world noisy conditions by directly addressing hallucination issues through model modifications.

Abstract: The Whisper model, an open-source automatic speech recognition system, is widely adopted for its strong performance across multilingual and zero-shot settings. However, it frequently suffers from hallucination errors, especially under noisy acoustic conditions. Previous works to reduce hallucinations in Whisper-style ASR systems have primarily focused on audio preprocessing or post-processing of transcriptions to filter out erroneous content. However, modifications to the Whisper model itself remain largely unexplored to mitigate hallucinations directly. To address this challenge, we present a two-stage architecture that first enhances encoder robustness through Adaptive Layer Attention (ALA) and further suppresses hallucinations using a multi-objective knowledge distillation (KD) framework. In the first stage, ALA groups encoder layers into semantically coherent blocks via inter-layer correlation analysis. A learnable multi-head attention module then fuses these block representations, enabling the model to jointly exploit low- and high-level features for more robust encoding. In the second stage, our KD framework trains the student model on noisy audio to align its semantic and attention distributions with a teacher model processing clean inputs. Our experiments on noisy speech benchmarks show notable reductions in hallucinations and word error rates, while preserving performance on clean speech. Together, ALA and KD offer a principled strategy to improve Whisper's reliability under real-world noisy conditions.

</details>


### [54] [DevPiolt: Operation Recommendation for IoT Devices at Xiaomi Home](https://arxiv.org/abs/2511.14227)
*Yuxiang Wang,Siwen Wang,Haowei Han,Ao Wang,Boya Liu,Yong Zhao,Chengbo Wu,Bin Zhu,Bin Qin,Xiaokai Zhou,Xiao Yan,Jiawei Jiang,Bo Du*

Main category: cs.AI

TL;DR: DevPiolt is an LLM-based recommendation model for IoT device operations that uses continual pre-training, multi-task fine-tuning, direct preference optimization, and confidence-based exposure control to provide personalized device operation suggestions.


<details>
  <summary>Details</summary>
Motivation: Existing recommendation models struggle with complex IoT operation logic, diverse user preferences, and sensitivity to suboptimal suggestions, limiting their applicability to IoT device operations.

Method: The approach involves: 1) equipping LLM with IoT domain knowledge via continual pre-training and multi-task fine-tuning, 2) using direct preference optimization to align with user preferences, and 3) implementing confidence-based exposure control to avoid low-quality recommendations.

Result: DevPiolt significantly outperforms baselines with 69.5% average improvement across all metrics. In real-world deployment serving 255,000 users, it achieved 21.6% increase in unique visitor device coverage and 29.1% increase in page view acceptance rates.

Conclusion: DevPiolt effectively addresses the challenges of IoT operation recommendation through LLM-based approach with domain adaptation and quality control mechanisms, demonstrating significant improvements in both offline and online performance metrics.

Abstract: Operation recommendation for IoT devices refers to generating personalized device operations for users based on their context, such as historical operations, environment information, and device status. This task is crucial for enhancing user satisfaction and corporate profits. Existing recommendation models struggle with complex operation logic, diverse user preferences, and sensitive to suboptimal suggestions, limiting their applicability to IoT device operations. To address these issues, we propose DevPiolt, a LLM-based recommendation model for IoT device operations. Specifically, we first equip the LLM with fundamental domain knowledge of IoT operations via continual pre-training and multi-task fine-tuning. Then, we employ direct preference optimization to align the fine-tuned LLM with specific user preferences. Finally, we design a confidence-based exposure control mechanism to avoid negative user experiences from low-quality recommendations. Extensive experiments show that DevPiolt significantly outperforms baselines on all datasets, with an average improvement of 69.5% across all metrics. DevPiolt has been practically deployed in Xiaomi Home app for one quarter, providing daily operation recommendations to 255,000 users. Online experiment results indicate a 21.6% increase in unique visitor device coverage and a 29.1% increase in page view acceptance rates.

</details>


### [55] [Enhancing Regional Airbnb Trend Forecasting Using LLM-Based Embeddings of Accessibility and Human Mobility](https://arxiv.org/abs/2511.14248)
*Hongju Lee,Youngjun Park,Jisun An,Dongman Lee*

Main category: cs.AI

TL;DR: A novel time-series forecasting framework using LLM-generated regional embeddings to predict Airbnb market trends (Revenue, Reservation Days, Number of Reservations) 1-3 months ahead, achieving 48% reduction in RMSE and MAE compared to conventional methods.


<details>
  <summary>Details</summary>
Motivation: Airbnb expansion disrupts local housing markets, increasing rental prices and affordability issues. Accurate regional forecasting provides critical insights for policymakers and urban planners to mitigate these impacts.

Method: Sliding-window approach integrating listing features with external contextual factors (urban accessibility, human mobility). Structured tabular data converted to prompt-based LLM inputs to generate regional embeddings, then fed into time-series models (RNN, LSTM, Transformer).

Result: Experiments on Seoul's Airbnb dataset show 48% reduction in average RMSE and MAE compared to traditional statistical and machine learning baselines.

Conclusion: The framework improves forecasting accuracy and offers practical insights for detecting oversupplied regions and supporting data-driven urban policy decisions.

Abstract: The expansion of short-term rental platforms, such as Airbnb, has significantly disrupted local housing markets, often leading to increased rental prices and housing affordability issues. Accurately forecasting regional Airbnb market trends can thus offer critical insights for policymakers and urban planners aiming to mitigate these impacts. This study proposes a novel time-series forecasting framework to predict three key Airbnb indicators -- Revenue, Reservation Days, and Number of Reservations -- at the regional level. Using a sliding-window approach, the model forecasts trends 1 to 3 months ahead. Unlike prior studies that focus on individual listings at fixed time points, our approach constructs regional representations by integrating listing features with external contextual factors such as urban accessibility and human mobility. We convert structured tabular data into prompt-based inputs for a Large Language Model (LLM), producing comprehensive regional embeddings. These embeddings are then fed into advanced time-series models (RNN, LSTM, Transformer) to better capture complex spatio-temporal dynamics. Experiments on Seoul's Airbnb dataset show that our method reduces both average RMSE and MAE by approximately 48% compared to conventional baselines, including traditional statistical and machine learning models. Our framework not only improves forecasting accuracy but also offers practical insights for detecting oversupplied regions and supporting data-driven urban policy decisions.

</details>


### [56] [PathMind: A Retrieve-Prioritize-Reason Framework for Knowledge Graph Reasoning with Large Language Models](https://arxiv.org/abs/2511.14256)
*Yu Liu,Xixun Lin,Yanmin Shang,Yangxi Li,Shi Wang,Yanan Cao*

Main category: cs.AI

TL;DR: PathMind is a novel framework for knowledge graph reasoning that selectively guides LLMs with important reasoning paths using a "Retrieve-Prioritize-Reason" paradigm, outperforming existing methods on complex reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based KGR methods face two limitations: indiscriminate path extraction that introduces irrelevant noise, and high computational demands from frequent LLM calls during dynamic path exploration.

Method: PathMind follows a "Retrieve-Prioritize-Reason" paradigm: retrieves query subgraphs, prioritizes paths using a semantic-aware function considering both accumulative and estimated future costs, and generates responses via dual-phase training (instruction tuning and path-wise preference alignment).

Result: Extensive experiments show PathMind consistently outperforms competitive baselines, particularly on complex reasoning tasks with fewer input tokens, by identifying essential reasoning paths.

Conclusion: PathMind enhances faithful and interpretable reasoning by selectively guiding LLMs with important reasoning paths, addressing limitations of existing methods through its prioritization mechanism and dual-phase training strategy.

Abstract: Knowledge graph reasoning (KGR) is the task of inferring new knowledge by performing logical deductions on knowledge graphs. Recently, large language models (LLMs) have demonstrated remarkable performance in complex reasoning tasks. Despite promising success, current LLM-based KGR methods still face two critical limitations. First, existing methods often extract reasoning paths indiscriminately, without assessing their different importance, which may introduce irrelevant noise that misleads LLMs. Second, while many methods leverage LLMs to dynamically explore potential reasoning paths, they require high retrieval demands and frequent LLM calls. To address these limitations, we propose PathMind, a novel framework designed to enhance faithful and interpretable reasoning by selectively guiding LLMs with important reasoning paths. Specifically, PathMind follows a "Retrieve-Prioritize-Reason" paradigm. First, it retrieves a query subgraph from KG through the retrieval module. Next, it introduces a path prioritization mechanism that identifies important reasoning paths using a semantic-aware path priority function, which simultaneously considers the accumulative cost and the estimated future cost for reaching the target. Finally, PathMind generates accurate and logically consistent responses via a dual-phase training strategy, including task-specific instruction tuning and path-wise preference alignment. Extensive experiments on benchmark datasets demonstrate that PathMind consistently outperforms competitive baselines, particularly on complex reasoning tasks with fewer input tokens, by identifying essential reasoning paths.

</details>


### [57] [DataSage: Multi-agent Collaboration for Insight Discovery with External Knowledge Retrieval, Multi-role Debating, and Multi-path Reasoning](https://arxiv.org/abs/2511.14299)
*Xiaochuan Liu,Yuanfeng Song,Xiaoming Yin,Xing Chen*

Main category: cs.AI

TL;DR: DataSage is a multi-agent framework that addresses limitations in automated data insight discovery by incorporating external knowledge retrieval, multi-role debating, and multi-path reasoning to improve analytical depth and accuracy.


<details>
  <summary>Details</summary>
Motivation: Current LLM-driven data insight agents have three key limitations: insufficient domain knowledge utilization, shallow analytical depth, and error-prone code generation, which prevent them from delivering satisfactory automated insight discovery.

Method: Proposes DataSage with three innovative features: external knowledge retrieval to enrich analytical context, multi-role debating mechanism to simulate diverse analytical perspectives and deepen analysis, and multi-path reasoning to improve code and insight accuracy.

Result: Extensive experiments on InsightBench show that DataSage consistently outperforms existing data insight agents across all difficulty levels, demonstrating superior performance in automated data insight discovery.

Conclusion: DataSage provides an effective solution for automated data insight discovery by addressing key limitations of existing approaches through its multi-agent framework with knowledge enrichment, diverse perspective simulation, and improved reasoning capabilities.

Abstract: In today's data-driven era, fully automated end-to-end data analytics, particularly insight discovery, is critical for discovering actionable insights that assist organizations in making effective decisions. With the rapid advancement of large language models (LLMs), LLM-driven agents have emerged as a promising paradigm for automating data analysis and insight discovery. However, existing data insight agents remain limited in several key aspects, often failing to deliver satisfactory results due to: (1) insufficient utilization of domain knowledge, (2) shallow analytical depth, and (3) error-prone code generation during insight generation. To address these issues, we propose DataSage, a novel multi-agent framework that incorporates three innovative features including external knowledge retrieval to enrich the analytical context, a multi-role debating mechanism to simulate diverse analytical perspectives and deepen analytical depth, and multi-path reasoning to improve the accuracy of the generated code and insights. Extensive experiments on InsightBench demonstrate that DataSage consistently outperforms existing data insight agents across all difficulty levels, offering an effective solution for automated data insight discovery.

</details>


### [58] [When Words Change the Model: Sensitivity of LLMs for Constraint Programming Modelling](https://arxiv.org/abs/2511.14334)
*Alessio Pellegrino,Jacopo Mauro*

Main category: cs.AI

TL;DR: LLMs can generate executable models for standard constraint problems but struggle with rephrased versions, suggesting their success may come from training data contamination rather than genuine reasoning.


<details>
  <summary>Details</summary>
Motivation: To test whether LLMs' apparent success in automatically generating constraint programming models comes from genuine reasoning or data contamination from training on standard benchmark problems.

Method: Systematically rephrased and perturbed well-known CSPLib problems while preserving structure, then compared model generation performance across original and modified descriptions using three representative LLMs.

Result: LLMs produce syntactically valid models for original problems but performance drops sharply with contextual and linguistic variations, revealing shallow understanding and sensitivity to wording.

Conclusion: LLMs' model generation capabilities appear to derive more from training data exposure than genuine reasoning, as they show limited robustness to problem rephrasing and contextual changes.

Abstract: One of the long-standing goals in optimisation and constraint programming is to describe a problem in natural language and automatically obtain an executable, efficient model. Large language models appear to bring this vision closer, showing impressive results in automatically generating models for classical benchmarks. However, much of this apparent success may derive from data contamination rather than genuine reasoning: many standard CP problems are likely included in the training data of these models. To examine this hypothesis, we systematically rephrased and perturbed a set of well-known CSPLib problems to preserve their structure while modifying their context and introducing misleading elements. We then compared the models produced by three representative LLMs across original and modified descriptions. Our qualitative analysis shows that while LLMs can produce syntactically valid and semantically plausible models, their performance drops sharply under contextual and linguistic variation, revealing shallow understanding and sensitivity to wording.

</details>


### [59] [Operationalizing Pluralistic Values in Large Language Model Alignment Reveals Trade-offs in Safety, Inclusivity, and Model Behavior](https://arxiv.org/abs/2511.14476)
*Dalia Ali,Dora Zhao,Allison Koenecke,Orestis Papakyriakopoulos*

Main category: cs.AI

TL;DR: This study examines how incorporating pluralistic human values affects LLM alignment, revealing systematic demographic variations in preference ratings and showing that technical design choices significantly impact model behavior.


<details>
  <summary>Details</summary>
Motivation: Current LLM alignment approaches often overlook human social diversity, focusing on homogeneous values rather than incorporating pluralistic perspectives from different demographic groups.

Method: Collected alignment data from 1,095 US and German participants (27,375 ratings) across five dimensions, fine-tuned multiple LLMs using group-specific preferences while varying rating scales, disagreement handling methods, and optimization techniques.

Result: Found systematic demographic effects: male participants rated responses 18% less toxic than females; conservative and Black participants rated responses 27.9% and 44% more emotionally aware than liberal and White participants. Technical choices showed strong effects: preserving disagreement achieved 53% greater toxicity reduction than majority voting, 5-point scales yielded 22% more reduction than binary formats, and DPO outperformed GRPO.

Conclusion: The findings represent a preliminary step in addressing how alignment should balance expert-driven and user-driven signals to ensure both safety and fair representation of diverse human values.

Abstract: Although large language models (LLMs) are increasingly trained using human feedback for safety and alignment with human values, alignment decisions often overlook human social diversity. This study examines how incorporating pluralistic values affects LLM behavior by systematically evaluating demographic variation and design parameters in the alignment pipeline. We collected alignment data from US and German participants (N = 1,095, 27,375 ratings) who rated LLM responses across five dimensions: Toxicity, Emotional Awareness (EA), Sensitivity, Stereotypical Bias, and Helpfulness. We fine-tuned multiple Large Language Models and Large Reasoning Models using preferences from different social groups while varying rating scales, disagreement handling methods, and optimization techniques. The results revealed systematic demographic effects: male participants rated responses 18% less toxic than female participants; conservative and Black participants rated responses 27.9% and 44% more emotionally aware than liberal and White participants, respectively. Models fine-tuned on group-specific preferences exhibited distinct behaviors. Technical design choices showed strong effects: the preservation of rater disagreement achieved roughly 53% greater toxicity reduction than majority voting, and 5-point scales yielded about 22% more reduction than binary formats; and Direct Preference Optimization (DPO) consistently outperformed Group Relative Policy Optimization (GRPO) in multi-value optimization. These findings represent a preliminary step in answering a critical question: How should alignment balance expert-driven and user-driven signals to ensure both safety and fair representation?

</details>


### [60] [A Neuro-Symbolic Framework for Reasoning under Perceptual Uncertainty: Bridging Continuous Perception and Discrete Symbolic Planning](https://arxiv.org/abs/2511.14533)
*Jiahao Wu,Shengwen Yu*

Main category: cs.AI

TL;DR: A neuro-symbolic framework that bridges perception and planning by modeling uncertainty propagation, achieving high success rates in robotic manipulation tasks while providing theoretical guarantees on planning convergence.


<details>
  <summary>Details</summary>
Motivation: To address the fundamental challenge of bridging continuous perceptual signals with discrete symbolic reasoning under uncertainty in AI systems, particularly for domains requiring uncertainty-aware reasoning from perception to planning.

Method: Couples transformer-based perceptual front-end with GNN relational reasoning to extract probabilistic symbolic states from visual observations, combined with an uncertainty-aware symbolic planner that actively gathers information when confidence is low.

Result: Processed 10,047 PyBullet scenes (3-10 objects) with calibrated predicate confidences (F1=0.68). Achieved 94%/90%/88% success on robotic manipulation benchmarks (90.7% average), exceeding POMDP baselines by 10-14 points with 15ms planning time.

Conclusion: The framework successfully bridges perception and planning with principled uncertainty modeling, provides theoretical guarantees validated empirically, and is general-purpose for domains requiring uncertainty-aware reasoning from perceptual input to symbolic planning.

Abstract: Bridging continuous perceptual signals and discrete symbolic reasoning is a fundamental challenge in AI systems that must operate under uncertainty. We present a neuro-symbolic framework that explicitly models and propagates uncertainty from perception to planning, providing a principled connection between these two abstraction levels. Our approach couples a transformer-based perceptual front-end with graph neural network (GNN) relational reasoning to extract probabilistic symbolic states from visual observations, and an uncertainty-aware symbolic planner that actively gathers information when confidence is low. We demonstrate the framework's effectiveness on tabletop robotic manipulation as a concrete application: the translator processes 10,047 PyBullet-generated scenes (3--10 objects) and outputs probabilistic predicates with calibrated confidences (overall F1=0.68). When embedded in the planner, the system achieves 94\%/90\%/88\% success on Simple Stack, Deep Stack, and Clear+Stack benchmarks (90.7\% average), exceeding the strongest POMDP baseline by 10--14 points while planning within 15\,ms. A probabilistic graphical-model analysis establishes a quantitative link between calibrated uncertainty and planning convergence, providing theoretical guarantees that are validated empirically. The framework is general-purpose and can be applied to any domain requiring uncertainty-aware reasoning from perceptual input to symbolic planning.

</details>


### [61] [Rate-Distortion Guided Knowledge Graph Construction from Lecture Notes Using Gromov-Wasserstein Optimal Transport](https://arxiv.org/abs/2511.14595)
*Yuan An,Ruhma Hashmi,Michelle Rogers,Jane Greenberg,Brian K. Smith*

Main category: cs.AI

TL;DR: A framework using rate-distortion theory and optimal transport geometry to automatically construct and refine knowledge graphs from educational materials, enabling generation of high-quality multiple-choice questions.


<details>
  <summary>Details</summary>
Motivation: Converting unstructured educational materials like lecture notes into task-oriented knowledge graphs that capture key pedagogical content remains challenging, limiting the effectiveness of AI-powered learning systems.

Method: Models lecture content as metric-measure space, aligns candidate KGs using Fused Gromov-Wasserstein couplings to quantify semantic distortion, and applies refinement operators (add, merge, split, remove, rewire) to minimize rate-distortion Lagrangian.

Result: Applied to data science lectures, yields interpretable rate-distortion curves and shows MCQs generated from refined KGs consistently surpass those from raw notes on fifteen quality criteria.

Conclusion: Establishes a principled foundation for information-theoretic KG optimization in personalized and AI-assisted education.

Abstract: Task-oriented knowledge graphs (KGs) enable AI-powered learning assistant systems to automatically generate high-quality multiple-choice questions (MCQs). Yet converting unstructured educational materials, such as lecture notes and slides, into KGs that capture key pedagogical content remains difficult. We propose a framework for knowledge graph construction and refinement grounded in rate-distortion (RD) theory and optimal transport geometry. In the framework, lecture content is modeled as a metric-measure space, capturing semantic and relational structure, while candidate KGs are aligned using Fused Gromov-Wasserstein (FGW) couplings to quantify semantic distortion. The rate term, expressed via the size of KG, reflects complexity and compactness. Refinement operators (add, merge, split, remove, rewire) minimize the rate-distortion Lagrangian, yielding compact, information-preserving KGs. Our prototype applied to data science lectures yields interpretable RD curves and shows that MCQs generated from refined KGs consistently surpass those from raw notes on fifteen quality criteria. This study establishes a principled foundation for information-theoretic KG optimization in personalized and AI-assisted education.

</details>


### [62] [AutoTool: Efficient Tool Selection for Large Language Model Agents](https://arxiv.org/abs/2511.14650)
*Jingyi Jia,Qinbin Li*

Main category: cs.AI

TL;DR: AutoTool is a graph-based framework that reduces LLM agent inference costs by leveraging tool usage inertia patterns from historical data, achieving up to 30% cost reduction while maintaining competitive performance.


<details>
  <summary>Details</summary>
Motivation: Current LLM agent frameworks suffer from high inference costs in tool selection, particularly in approaches like ReAct that repeatedly invoke LLMs for each step's tool choice decision.

Method: Constructs a directed graph from historical agent trajectories where nodes represent tools and edges capture transition probabilities, modeling tool usage inertia. Integrates parameter-level information for refined tool input generation.

Result: Extensive experiments show AutoTool reduces inference costs by up to 30% while maintaining competitive task completion rates across diverse agent tasks.

Conclusion: AutoTool demonstrates the promise of integrating statistical structure into LLM agent design for greater efficiency without sacrificing performance, offering a practical and scalable enhancement for inference-heavy frameworks.

Abstract: Large Language Model (LLM) agents have emerged as powerful tools for automating complex tasks by leveraging the reasoning and decision-making abilities of LLMs. However, a major bottleneck in current agent frameworks lies in the high inference cost of tool selection, especially in approaches like ReAct that repeatedly invoke the LLM to determine which tool to use at each step. In this work, we propose AutoTool, a novel graph-based framework that bypasses repeated LLM inference by exploiting a key empirical observation: tool usage inertia - the tendency of tool invocations to follow predictable sequential patterns. AutoTool constructs a directed graph from historical agent trajectories, where nodes represent tools and edges capture transition probabilities, effectively modeling the inertia in tool selection. It further integrates parameter-level information to refine tool input generation. By traversing this structured representation, AutoTool efficiently selects tools and their parameters with minimal reliance on LLM inference. Extensive experiments across diverse agent tasks demonstrate that AutoTool reduces inference costs by up to 30% while maintaining competitive task completion rates, offering a practical and scalable enhancement for inference-heavy frameworks. Our work highlights the promise of integrating statistical structure into LLM agent design for greater efficiency without sacrificing performance.

</details>
