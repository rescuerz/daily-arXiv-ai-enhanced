<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 5]
- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Know Your Limits: Entropy Estimation Modeling for Compression and Generalization](https://arxiv.org/abs/2511.10618)
*Benjamin L. Badger,Matthew Neligeorge*

Main category: cs.CL

TL;DR: The paper introduces encoder-augmented causal decoder models that achieve better compression than causal transformers and shows that training models to approach estimated per-token entropies improves generalization compared to standard training approaches.


<details>
  <summary>Details</summary>
Motivation: Current causal language models face computational infeasibility in accurately estimating language entropy due to intrinsic informational entropy constraints, and there's a need for more efficient architectures that can approach entropy limits while improving generalization.

Method: Developed encoder-augmented causal decoder model architectures that offer superior training efficiency and higher compression than causal transformers. Introduced per-token entropy estimation and trained models to approach but not exceed estimated per-token entropies.

Result: The proposed architectures achieve higher compression than causal transformers even on modest hardware. Models trained to approach estimated per-token entropies exhibit greater generalization than models trained without entropy considerations.

Conclusion: Training language models to approach but not exceed estimated per-token entropies leads to better generalization, and encoder-augmented causal decoder architectures provide a computationally feasible path to approaching language entropy limits.

Abstract: Language prediction is constrained by informational entropy intrinsic to language, such that there exists a limit to how accurate any language model can become and equivalently a lower bound to language compression. The most efficient language compression algorithms today are causal (next token prediction) large language models, but the use of these models to form accurate estimates of language entropy is currently computationally infeasible. We introduce encoder-augmented causal decoder model architectures that exhibit superior training efficiency characteristics and achieve higher compression than causal transformers even when trained on modest hardware. We demonstrate how entropy estimates can be obtained on a per-token basis, and show that the generalization of models trained to approach the entropy of their training data necessarily exceeds the generalization of models trained to minimize loss beyond this value. We show empirically that causal models trained to approach but not exceed estimated per-token entropies exhibit greater generalization than models trained without taking entropy into account.

</details>


### [2] [SSR: Socratic Self-Refine for Large Language Model Reasoning](https://arxiv.org/abs/2511.10621)
*Haizhou Shi,Ye Liu,Bo Pang,Zeyu Leo Liu,Hao Wang,Silvio Savarese,Caiming Xiong,Yingbo Zhou,Semih Yavuz*

Main category: cs.CL

TL;DR: SSR is a novel framework that decomposes LLM reasoning into verifiable sub-question/answer pairs for step-level confidence estimation and iterative refinement, outperforming existing self-refinement methods across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing test-time frameworks for LLMs rely on coarse self-verification and self-correction, limiting effectiveness on complex reasoning tasks that require fine-grained evaluation and precise refinement.

Method: SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks, then iteratively refines unreliable steps.

Result: Empirical results across five reasoning benchmarks and three LLMs show SSR consistently outperforms state-of-the-art iterative self-refinement baselines.

Conclusion: SSR provides a principled black-box approach for evaluating and understanding LLM reasoning processes while producing more accurate and interpretable reasoning chains.

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning.

</details>


### [3] [Instella: Fully Open Language Models with Stellar Performance](https://arxiv.org/abs/2511.10628)
*Jiang Liu,Jialian Wu,Xiaodong Yu,Yusheng Su,Prakamya Mishra,Gowtham Ramesh,Sudhanshu Ranjan,Chaitanya Manem,Ximeng Sun,Ze Wang,Pratik Prabhanjan Brahma,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

TL;DR: Instella is a family of fully open 3B parameter language models trained on open data using AMD GPUs, achieving SOTA performance among fully open models and competitive with open-weight models of similar size, with specialized variants for long contexts (128K tokens) and mathematical reasoning.


<details>
  <summary>Details</summary>
Motivation: Most high-performing LLMs are closed-source or partially open, limiting transparency and reproducibility. The goal is to create fully open models using only openly available data and codebase to advance open and reproducible language modeling research.

Method: Large-scale pre-training on openly available data using AMD Instinct MI300X GPUs, followed by general-purpose instruction tuning and alignment with human preferences. Specialized variants created through supervised fine-tuning and reinforcement learning on mathematical tasks (Instella-Math) and extended context handling (Instella-Long).

Result: Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size, despite using fewer pre-training tokens than many contemporaries. Instella-Long handles 128K token contexts, and Instella-Math shows enhanced reasoning capabilities.

Conclusion: Instella establishes a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research through fully open models that compete with proprietary alternatives.

Abstract: Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.

</details>


### [4] [Black-Box On-Policy Distillation of Large Language Models](https://arxiv.org/abs/2511.10643)
*Tianzhu Ye,Li Dong,Zewen Chi,Xun Wu,Shaohan Huang,Furu Wei*

Main category: cs.CL

TL;DR: Generative Adversarial Distillation (GAD) enables black-box LLM distillation by framing the student as a generator and training a discriminator to distinguish student responses from teacher responses, creating a minimax game.


<details>
  <summary>Details</summary>
Motivation: To enable effective black-box distillation of LLMs where only the teacher's text outputs are available, without access to internal logits or parameters.

Method: GAD frames student LLM as generator and trains a discriminator to distinguish student responses from teacher responses, creating a minimax game where the discriminator co-evolves with the student as an on-policy reward model.

Result: GAD consistently outperforms sequence-level knowledge distillation, with Qwen2.5-14B-Instruct student becoming comparable to its teacher GPT-5-Chat on LMSYS-Chat automatic evaluation.

Conclusion: GAD establishes a promising and effective paradigm for black-box LLM distillation through its adversarial training approach.

Abstract: Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.

</details>


### [5] [ParoQuant: Pairwise Rotation Quantization for Efficient Reasoning LLM Inference](https://arxiv.org/abs/2511.10645)
*Yesheng Liang,Haisheng Chen,Song Han,Zhijian Liu*

Main category: cs.CL

TL;DR: ParoQuant is a weight-only post-training quantization method that uses pairwise Givens rotations and channel-wise scaling to handle outliers in LLMs, achieving better accuracy than AWQ with minimal inference overhead.


<details>
  <summary>Details</summary>
Motivation: Existing PTQ methods struggle with outlier suppression in LLM weights and activations, leading to accuracy degradation in reasoning tasks where errors accumulate across long chains of thought.

Method: Combines hardware-efficient independent Givens rotations with channel-wise scaling to normalize magnitudes across channels and reduce dynamic range within quantization groups, with co-designed inference kernels for GPU parallelism.

Result: Achieves average 2.4% accuracy improvement over AWQ on reasoning tasks with less than 10% overhead.

Conclusion: ParoQuant enables more efficient and accurate deployment of reasoning LLMs through effective outlier suppression and lightweight runtime operations.

Abstract: Weight-only post-training quantization (PTQ) compresses the weights of Large Language Models (LLMs) into low-precision representations to reduce memory footprint and accelerate inference. However, the presence of outliers in weights and activations often leads to large quantization errors and severe accuracy degradation, especially in recent reasoning LLMs where errors accumulate across long chains of thought. Existing PTQ methods either fail to sufficiently suppress outliers or introduce significant overhead during inference. In this paper, we propose Pairwise Rotation Quantization (ParoQuant), a weight-only PTQ method that combines hardware-efficient and optimizable independent Givens rotations with channel-wise scaling to even out the magnitude across channels and narrow the dynamic range within each quantization group. We further co-design the inference kernel to fully exploit GPU parallelism and keep the rotations and scaling lightweight at runtime. ParoQuant achieves an average 2.4% accuracy improvement over AWQ on reasoning tasks with less than 10% overhead. This paves the way for more efficient and accurate deployment of reasoning LLMs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [Querying Labeled Time Series Data with Scenario Programs](https://arxiv.org/abs/2511.10627)
*Edward Kim,Devan Shanker,Varun Bharadwaj,Hongbeen Park,Jinkyu Kim,Hazem Torfah,Daniel J Fremont,Sanjit A Seshia*

Main category: cs.AI

TL;DR: The paper addresses the sim-to-real gap in cyber physical systems testing by developing a method to validate simulated failure scenarios using real-world sensor data, with a formal definition and querying algorithm that outperforms existing commercial vision models.


<details>
  <summary>Details</summary>
Motivation: To bridge the sim-to-real gap in CPS testing by verifying whether failure scenarios discovered in simulation environments are reproducible in real-world systems using actual sensor data.

Method: Introduces a formal definition for matching labeled time series sensor data to abstract scenarios using Scenic probabilistic programming language, and presents a querying algorithm to identify matching data subsets in real-world datasets.

Result: The algorithm is more accurate and orders of magnitude faster than state-of-the-art commercial vision large language models, and scales effectively with the duration of queried time series data.

Conclusion: The proposed approach provides an effective method for validating simulated failure scenarios against real-world data, addressing the critical sim-to-real gap in cyber physical systems testing.

Abstract: Simulation-based testing has become a crucial complement to road testing for ensuring the safety of cyber physical systems (CPS). As a result, significant research efforts have been directed toward identifying failure scenarios within simulation environments. However, a critical question remains. Are the AV failure scenarios discovered in simulation reproducible on actual systems in the real world? The sim-to-real gap caused by differences between simulated and real sensor data means that failure scenarios identified in simulation might either be artifacts of synthetic sensor data or actual issues that also occur with real sensor data. To address this, an effective approach to validating simulated failure scenarios is to locate occurrences of these scenarios within real-world datasets and verify whether the failure persists on the datasets. To this end, we introduce a formal definition of how labeled time series sensor data can match an abstract scenario, represented as a scenario program using the Scenic probabilistic programming language. We present a querying algorithm that, given a scenario program and a labeled dataset, identifies the subset of data that matches the specified scenario. Our experiment shows that our algorithm is more accurate and orders of magnitude faster in querying scenarios than the state-of-the-art commercial vision large language models, and can scale with the duration of queried time series data.

</details>
