<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 6]
- [cs.AI](#cs.AI) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Auxiliary Metrics Help Decoding Skill Neurons in the Wild](https://arxiv.org/abs/2511.21610)
*Yixiu Zhao,Xiaozhi Wang,Zijun Yao,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: The paper presents a lightweight method to identify "skill neurons" in large language models that encode specific capabilities, extending prior work to complex multi-skill scenarios by correlating neuron activations with auxiliary metrics.


<details>
  <summary>Details</summary>
Motivation: While LLMs demonstrate impressive capabilities across diverse tasks, their internal mechanisms remain largely opaque, creating a need for interpretability methods that can uncover how specific skills are encoded within the model.

Method: The approach extends prior work on skill neuron identification by correlating neuron activations with auxiliary metrics (external labels and model confidence scores) to uncover interpretable, task-specific behaviors without manual token aggregation. It's validated on open-ended text generation and natural language inference tasks.

Result: The method successfully detects neurons that drive known skills and reveals previously unidentified shortcuts in arithmetic reasoning on BigBench, demonstrating its effectiveness in complex scenarios involving multiple skills.

Conclusion: The proposed lightweight method provides a broadly applicable approach for isolating skill-encoding neurons in LLMs, offering insights into model internals and uncovering shortcuts in reasoning tasks.

Abstract: Large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, yet their internal mechanisms remain largely opaque. In this paper, we introduce a simple, lightweight, and broadly applicable method with a focus on isolating neurons that encode specific skills. Building upon prior work that identified "skill neurons" via soft prompt training on classification tasks, our approach extends the analysis to complex scenarios involving multiple skills. We correlate neuron activations with auxiliary metrics -- such as external labels and the model's own confidence score -- thereby uncovering interpretable and task-specific behaviors without the need for manual token aggregation. We empirically validate our method on tasks spanning open-ended text generation and natural language inference, demonstrating its ability to detect neurons that not only drive known skills but also reveal previously unidentified shortcuts in arithmetic reasoning on BigBench.

</details>


### [2] [Beyond URLs: Metadata Diversity and Position for Efficient LLM Pretraining](https://arxiv.org/abs/2511.21613)
*Dongyang Fan,Diba Hashemi,Sai Praneeth Karimireddy,Martin Jaggi*

Main category: cs.CL

TL;DR: This paper investigates using various metadata types beyond URLs to accelerate LLM pretraining, finding that fine-grained quality indicators and metadata appending as auxiliary tasks can improve training efficiency through quality-aware latent structures.


<details>
  <summary>Details</summary>
Motivation: Prior work only explored URLs as useful metadata for accelerating LLM pretraining, leaving open whether other metadata types could provide greater benefits and how different metadata integration approaches affect training efficiency.

Method: The study investigates multiple metadata types, introduces metadata appending as an auxiliary task, uses learnable meta-tokens trained with masked loss, and employs probing to analyze latent representations to understand how metadata shapes learning.

Result: The research identifies that effective metadata encodes information at finer granularity, metadata appending as auxiliary tasks speeds up pretraining, and learnable meta-tokens can recover part of the speedup by creating quality-aware latent structures.

Conclusion: The findings provide practical guidelines for integrating metadata to improve both the efficiency and effectiveness of LLM pretraining, demonstrating that various metadata types beyond URLs can accelerate training through different integration approaches.

Abstract: Incorporating metadata in Large Language Models (LLMs) pretraining has recently emerged as a promising approach to accelerate training. However prior work highlighted only one useful signal-URLs, leaving open the question of whether other forms of metadata could yield greater benefits. In this study, we investigate a wider range of metadata types and find other types of metadata, such as fine-grained indicators of document quality that can also accelerate pretraining when prepended. We identify a common feature among effective metadata: they encode information at a finer granularity. We further introduce metadata appending as a means of improving training efficiency, where predicting an appropriate metadata as auxiliary task can help speed up pretraining. In addition, learnable meta-tokens trained with masked loss can recover part of the speedup by inducing quality-aware latent structure. Using probing, we analyze latent representations to understand how metadata shapes learning. Together, these results yield practical guidelines for integrating metadata to improve both the efficiency and effectiveness of LLM pretraining.

</details>


### [3] [The author is dead, but what if they never lived? A reception experiment on Czech AI- and human-authored poetry](https://arxiv.org/abs/2511.21629)
*Anna Marklová,Ondřej Vinš,Martina Vokáčová,Jiří Milička*

Main category: cs.CL

TL;DR: Czech native speakers cannot reliably distinguish between AI-generated and human-written Czech poetry, performing at chance level (45.8% accuracy). There's a strong authorship bias where poems believed to be AI-generated are rated less favorably, even though AI poems actually received equal or better ratings.


<details>
  <summary>Details</summary>
Motivation: To examine how Czech native speakers perceive and identify AI-generated poetry in their native language, particularly in a morphologically complex, low-resource Slavic language like Czech, given that most AI poetry research focuses on English.

Method: Conducted a study where Czech native speakers were asked to identify authorship (AI vs human) of poems and provide aesthetic evaluations. Used logistic regression to analyze factors affecting recognition accuracy.

Result: Participants performed at chance level for authorship identification (45.8% correct). AI poems were rated equally or more favorably than human ones, but poems believed to be AI-generated received lower ratings due to authorship bias. Liking a poem made accurate authorship assignment less probable. Poetry familiarity had no effect on recognition.

Conclusion: AI can convincingly produce poetry in Czech, a morphologically complex language. Readers' beliefs about authorship strongly influence aesthetic evaluation, showing interconnectedness between perceived authorship and aesthetic judgment.

Abstract: Large language models are increasingly capable of producing creative texts, yet most studies on AI-generated poetry focus on English -- a language that dominates training data. In this paper, we examine the perception of AI- and human-written Czech poetry. We ask if Czech native speakers are able to identify it and how they aesthetically judge it. Participants performed at chance level when guessing authorship (45.8\% correct on average), indicating that Czech AI-generated poems were largely indistinguishable from human-written ones. Aesthetic evaluations revealed a strong authorship bias: when participants believed a poem was AI-generated, they rated it as less favorably, even though AI poems were in fact rated equally or more favorably than human ones on average. The logistic regression model uncovered that the more the people liked a poem, the less probable was that they accurately assign the authorship. Familiarity with poetry or literary background had no effect on recognition accuracy. Our findings show that AI can convincingly produce poetry even in a morphologically complex, low-resource (with respect of the training data of AI models) Slavic language such as Czech. The results suggest that readers' beliefs about authorship and the aesthetic evaluation of the poem are interconnected.

</details>


### [4] [Matrix: Peer-to-Peer Multi-Agent Synthetic Data Generation Framework](https://arxiv.org/abs/2511.21686)
*Dong Wang,Yang Li,Ansong Ni,Ching-Feng Yeh,Youssef Emad,Xinjie Lei,Liam Robbins,Karthik Padthe,Hu Xu,Xian Li,Asli Celikyilmaz,Ramya Raghavendra,Lifei Huang,Carole-Jean Wu,Shang-Wen Li*

Main category: cs.CL

TL;DR: Matrix is a decentralized framework for multi-agent synthetic data generation that eliminates central orchestrators, uses distributed queues for message passing, and achieves 2-15x higher throughput than existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent synthesis frameworks have scalability bottlenecks due to centralized orchestrators and are hardcoded for specific domains, limiting flexibility for diverse data generation tasks.

Method: Matrix uses a decentralized peer-to-peer design where control and data flow are represented as serialized messages passed through distributed queues. It leverages Ray for scalability, with lightweight agents handling tasks and distributed services managing compute-intensive operations like LLM inference.

Result: Matrix scales to tens of thousands of concurrent agentic workflows and achieves 2-15x higher data generation throughput under identical hardware resources across various synthesis scenarios including collaborative dialogue, web reasoning, and tool-use trajectory generation.

Conclusion: Matrix provides a scalable, flexible framework for multi-agent synthetic data generation that significantly outperforms existing approaches in throughput while maintaining output quality across diverse applications.

Abstract: Synthetic data has become increasingly important for training large language models, especially when real data is scarce, expensive, or privacy-sensitive. Many such generation tasks require coordinated multi-agent workflows, where specialized agents collaborate to produce data that is higher quality, more diverse, and structurally richer. However, existing frameworks for multi-agent synthesis often depend on a centralized orchestrator, creating scalability bottlenecks, or are hardcoded for specific domains, limiting flexibility. We present \textbf{Matrix}, a decentralized framework that represents both control and data flow as serialized messages passed through distributed queues. This peer-to-peer design eliminates the central orchestrator. Each task progresses independently through lightweight agents, while compute-intensive operations, such as LLM inference or containerized environments, are handled by distributed services. Built on Ray, Matrix scales to tens of thousands of concurrent agentic workflows and provides a modular, configurable design that enables easy adaptation to a wide range of data generation workflows. We evaluate Matrix across diverse synthesis scenarios, such as multi-agent collaborative dialogue, web-based reasoning data extraction, and tool-use trajectory generation in customer service environments. In all cases, Matrix achieves $2$--$15\times$ higher data generation throughput under identical hardware resources, without compromising output quality.

</details>


### [5] [ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration](https://arxiv.org/abs/2511.21689)
*Hongjin Su,Shizhe Diao,Ximing Lu,Mingjie Liu,Jiacheng Xu,Xin Dong,Yonggan Fu,Peter Belcak,Hanrong Ye,Hongxu Yin,Yi Dong,Evelina Bakhturina,Tao Yu,Yejin Choi,Jan Kautz,Pavlo Molchanov*

Main category: cs.CL

TL;DR: ToolOrchestra trains small orchestrator models that coordinate multiple tools to solve complex problems more efficiently and effectively than large language models like GPT-5, achieving better performance at lower cost on benchmarks like Humanity's Last Exam.


<details>
  <summary>Details</summary>
Motivation: Large language models are computationally expensive for solving deep complex problems, and there's a need for more efficient methods that can coordinate multiple intelligent tools while aligning with user preferences.

Method: ToolOrchestra uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards to train small orchestrators (8B parameters) that manage other models and tools.

Result: The Orchestrator model achieves 37.1% on HLE (outperforming GPT-5's 35.1%) with 2.5x efficiency, and surpasses GPT-5 on tau2-Bench and FRAMES using only 30% of the cost while maintaining user preference alignment.

Conclusion: Composing diverse tools with lightweight orchestration models is more efficient and effective than existing methods, enabling practical and scalable tool-augmented reasoning systems.

Abstract: Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.

</details>


### [6] [Revisiting Generalization Across Difficulty Levels: It's Not So Easy](https://arxiv.org/abs/2511.21692)
*Yeganeh Kordi,Nihal V. Nayak,Max Zuo,Ilana Nguyen,Stephen H. Bach*

Main category: cs.CL

TL;DR: LLMs show limited cross-difficulty generalization - training on easy or hard data alone doesn't consistently improve performance across all difficulty levels, highlighting the need for diverse difficulty ranges in training and evaluation data.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs generalize across different task difficulties, addressing conflicting findings about whether training on easier or harder data leads to better results and where those gains occur.

Method: Systematic evaluation using Item Response Theory (IRT) to rank example difficulty based solely on outputs from thousands of different LLMs, excluding human opinions, across six datasets and fine-grained difficulty groups.

Result: Cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties.

Conclusion: Having a range of difficulties in both training and evaluation data is crucial for LLMs, and taking shortcuts with respect to difficulty selection is risky.

Abstract: We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [On the Limits of Innate Planning in Large Language Models](https://arxiv.org/abs/2511.21591)
*Charles Schepanowski,Charles Ling*

Main category: cs.AI

TL;DR: LLMs struggle with planning and stateful reasoning in the 8-puzzle task, showing brittle state representations and weak heuristic planning even with corrective feedback and move validation assistance.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLMs' capacity for planning and state tracking without external tools, using the 8-puzzle as a precise testbed that requires goal-directed planning and state management.

Method: Tested four LLMs on 8-puzzle using Zero-Shot, Chain-of-Thought, and Algorithm-of-Thought prompting with tiered corrective feedback, and later with external move validator providing only valid moves.

Result: Feedback improved some success rates but runs were long and indirect. With move validation, no models solved any puzzles. Models showed brittle state representations and weak heuristic planning.

Conclusion: Current LLMs have substantial limitations in planning without external tools, requiring mechanisms for explicit state maintenance and structured search for progress.

Abstract: Large language models (LLMs) achieve impressive results on many benchmarks, yet their capacity for planning and stateful reasoning remains unclear. We study these abilities directly, without code execution or other tools, using the 8-puzzle: a classic task that requires state tracking and goal-directed planning while allowing precise, step-by-step evaluation. Four models are tested under common prompting conditions (Zero-Shot, Chain-of-Thought, Algorithm-of-Thought) and with tiered corrective feedback. Feedback improves success rates for some model-prompt combinations, but many successful runs are long, computationally expensive, and indirect. We then examine the models with an external move validator that provides only valid moves. Despite this level of assistance, none of the models solve any puzzles in this setting. Qualitative analysis reveals two dominant deficits across all models: (1) brittle internal state representations, leading to frequent invalid moves, and (2) weak heuristic planning, with models entering loops or selecting actions that do not reduce the distance to the goal state. These findings indicate that, in the absence of external tools such as code interpreters, current LLMs have substantial limitations in planning and that further progress may require mechanisms for maintaining explicit state and performing structured search.

</details>


### [8] [Bridging the Unavoidable A Priori: A Framework for Comparative Causal Modeling](https://arxiv.org/abs/2511.21636)
*Peter S. Hovmand,Kari O'Donnell,Callie Ogland-Hand,Brian Biroscak,Douglas D. Gunzler*

Main category: cs.AI

TL;DR: This paper integrates system dynamics and structural equation modeling into a unified mathematical framework to address biases in AI/ML systems and advance responsible AI development.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenge of combining methods with different underlying assumptions (Dana Meadow's 'the unavoidable a priori') and enable richer causal modeling for responsible AI/ML development.

Method: Develops a common mathematical framework that brings together system dynamics and structural equation modeling, allowing generation of systems from distributions and method development.

Result: Creates a unified approach that can generate systems, develop methods, and compare results to inform system dynamics epistemology for data science and AI/ML applications.

Conclusion: The proposed framework enables bridging system dynamics and structural equation modeling to better address human bias amplification in AI/ML systems and advance responsible AI practices.

Abstract: AI/ML models have rapidly gained prominence as innovations for solving previously unsolved problems and their unintended consequences from amplifying human biases. Advocates for responsible AI/ML have sought ways to draw on the richer causal models of system dynamics to better inform the development of responsible AI/ML. However, a major barrier to advancing this work is the difficulty of bringing together methods rooted in different underlying assumptions (i.e., Dana Meadow's "the unavoidable a priori"). This paper brings system dynamics and structural equation modeling together into a common mathematical framework that can be used to generate systems from distributions, develop methods, and compare results to inform the underlying epistemology of system dynamics for data science and AI/ML applications.

</details>


### [9] [Agentic Learner with Grow-and-Refine Multimodal Semantic Memory](https://arxiv.org/abs/2511.21678)
*Weihao Bo,Shan Zhang,Yanpeng Sun,Jingjing Wu,Qunyi Xie,Xiao Tan,Kunbin Chen,Wei He,Xiaofan Li,Na Zhao,Jingdong Wang,Zechao Li*

Main category: cs.AI

TL;DR: ViLoMem introduces a dual-stream memory framework that separately encodes visual distraction patterns and logical reasoning errors to help MLLMs learn from both successful and failed experiences, improving performance across multimodal benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing memory-augmented agents mainly store single-modality trajectories, which suffer from brevity bias and fail to preserve how visual attention and logical reasoning jointly contributed to solutions, misaligned with human cognition's multimodal semantic memory.

Method: A dual-stream memory framework that constructs compact, schema-based memory, separately encoding visual distraction patterns and logical reasoning errors, following a grow-and-refine principle to incrementally accumulate and update multimodal semantic knowledge.

Result: Across six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction-hallucination separation.

Conclusion: The framework demonstrates the value of error-aware multimodal memory for lifelong and cross-domain agentic learning, enabling MLLMs to learn from their successful and failed experiences while avoiding catastrophic forgetting.

Abstract: MLLMs exhibit strong reasoning on isolated queries, yet they operate de novo -- solving each problem independently and often repeating the same mistakes. Existing memory-augmented agents mainly store past trajectories for reuse. However, trajectory-based memory suffers from brevity bias, gradually losing essential domain knowledge. More critically, even in truly multimodal problem-solving settings, it records only a single-modality trace of past behavior, failing to preserve how visual attention and logical reasoning jointly contributed to the solution. This is fundamentally misaligned with human cognition: semantic memory is both multimodal and integrated, preserving visual and abstract knowledge through coordinated but distinct representational streams. We thus introduce ViLoMem, a dual-stream memory framework that constructs compact, schema-based memory. It separately encodes visual distraction patterns and logical reasoning errors, enabling MLLMs to learn from their successful and failed experiences. Following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge -- preserving stable, generalizable strategies while avoiding catastrophic forgetting. Across six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction--hallucination separation, demonstrating the value of error-aware multimodal memory for lifelong and cross-domain agentic learning. Our project page will be available at https://weihao-bo.github.io/ViLoMeo-page.

</details>
