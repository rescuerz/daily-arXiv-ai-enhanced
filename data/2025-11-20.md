<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 27]
- [cs.AI](#cs.AI) [Total: 22]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Test-time Scaling of LLMs: A Survey from A Subproblem Structure Perspective](https://arxiv.org/abs/2511.14772)
*Zhuoyi Yang,Xu Guo,Tong Zhang,Huijuan Xu,Boyang Li*

Main category: cs.CL

TL;DR: Survey of test-time scaling methods that improve LLM accuracy by allocating additional compute during inference, with focus on problem decomposition and topological organization.


<details>
  <summary>Details</summary>
Motivation: To systematically categorize and unify various approaches for enhancing large language model performance through additional inference-time computation.

Method: Categorizes techniques based on how problems are decomposed into subproblems and their topological organization (sequential, parallel, tree-structured), unifying approaches like Chain-of-Thought, Branch-Solve-Merge, and Tree-of-Thought.

Result: Provides a unified framework for understanding diverse test-time scaling methods and synthesizes existing analyses of their respective strengths and weaknesses.

Conclusion: Outlines promising future research directions for improving predictive accuracy through inference-time compute allocation.

Abstract: With this paper, we survey techniques for improving the predictive accuracy of pretrained large language models by allocating additional compute at inference time. In categorizing test-time scaling methods, we place special emphasis on how a problem is decomposed into subproblems and on the topological organization of these subproblems whether sequential, parallel, or tree-structured. This perspective allows us to unify diverse approaches such as Chain-of-Thought, Branch-Solve-Merge, and Tree-of-Thought under a common lens. We further synthesize existing analyses of these techniques, highlighting their respective strengths and weaknesses, and conclude by outlining promising directions for future research

</details>


### [2] [Temporal Predictors of Outcome in Reasoning Language Models](https://arxiv.org/abs/2511.14773)
*Joey David*

Main category: cs.CL

TL;DR: LLMs internally commit to eventual outcomes very early in reasoning chains, with correctness predictable after just a few tokens, even for longer outputs.


<details>
  <summary>Details</summary>
Motivation: To understand how early Large Language Models internally commit to eventual outcomes during chain-of-thought reasoning, despite needing longer outputs to reach definite answers.

Method: Train linear classifiers on hidden states after the first t reasoning tokens to predict eventual correctness, analyzing predictive accuracy across different question difficulties.

Result: Eventual correctness is highly predictable after only a few tokens; harder questions show predictive accuracy drops due to selection artifact where hard items are overrepresented in long CoTs.

Conclusion: Reasoning models develop internal self-assessment of success very early, with implications for interpretability and inference-time control.

Abstract: The chain-of-thought (CoT) paradigm uses the elicitation of step-by-step rationales as a proxy for reasoning, gradually refining the model's latent representation of a solution. However, it remains unclear just how early a Large Language Model (LLM) internally commits to an eventual outcome. We probe this by training linear classifiers on hidden states after the first t reasoning tokens, showing that eventual correctness is highly predictable after only a few tokens, even when longer outputs are needed to reach a definite answer. We show that, for harder questions, a drop in predictive accuracy highlights a selection artifact: hard items are disproportionately represented in long CoTs. Overall, our results imply that for reasoning models, internal self-assessment of success tends to emerge after only a few tokens, with implications for interpretability and for inference-time control.

</details>


### [3] [LiveCLKTBench: Towards Reliable Evaluation of Cross-Lingual Knowledge Transfer in Multilingual LLMs](https://arxiv.org/abs/2511.14774)
*Pei-Fu Guo,Yun-Da Tsai,Chun-Chia Hsu,Kai-Xin Chen,Ya-An Tsai,Kai-Wei Chang,Nanyun Peng,Mi-Yen Yeh,Shou-De Lin*

Main category: cs.CL

TL;DR: LiveCLKTBench is an automated pipeline that isolates and measures cross-lingual knowledge transfer in LLMs by using time-sensitive knowledge entities to generate factual questions across multiple languages.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of distinguishing genuine cross-lingual knowledge transfer from prior exposure during pre-training in large language models.

Method: An automated generation pipeline that identifies self-contained, time-sensitive knowledge entities, filters them based on temporal occurrence, verifies them against model knowledge, and generates translated factual questions across multiple languages.

Result: Cross-lingual transfer is strongly influenced by linguistic distance and often asymmetric across language directions. Larger models improve transfer but gains diminish with scale and vary across domains.

Conclusion: LiveCLKTBench provides reliable benchmarking for cross-lingual knowledge transfer research and reveals important insights about linguistic distance and transfer asymmetry in multilingual models.

Abstract: Evaluating cross-lingual knowledge transfer in large language models is challenging, as correct answers in a target language may arise either from genuine transfer or from prior exposure during pre-training. We present LiveCLKTBench, an automated generation pipeline specifically designed to isolate and measure cross-lingual knowledge transfer. Our pipeline identifies self-contained, time-sensitive knowledge entities from real-world domains, filters them based on temporal occurrence, and verifies them against the model's knowledge. The documents of these valid entities are then used to generate factual questions, which are translated into multiple languages to evaluate transferability across linguistic boundaries. Using LiveCLKTBench, we evaluate several LLMs across five languages and observe that cross-lingual transfer is strongly influenced by linguistic distance and often asymmetric across language directions. While larger models improve transfer, the gains diminish with scale and vary across domains. These findings provide new insights into multilingual transfer and demonstrate the value of LiveCLKTBench as a reliable benchmark for future research.

</details>


### [4] [COMPASS: Context-Modulated PID Attention Steering System for Hallucination Mitigation](https://arxiv.org/abs/2511.14776)
*Snigdha Pandya,Rohan Nagale,Kenji Sahay,Anna Lin,Shikhar Shiromani,Kevin Zhu,Dev Sunishchal*

Main category: cs.CL

TL;DR: COMPASS is a lightweight control framework that uses a PID controller to dynamically modulate attention heads during decoding, reducing factual hallucinations in LLMs by maintaining evidence alignment without retraining.


<details>
  <summary>Details</summary>
Motivation: LLMs often generate fluent but factually incorrect statements despite having access to relevant evidence, due to improper attention allocation between contextual and parametric knowledge. Understanding and steering this internal behavior is crucial for trustworthy deployment and scientific interpretability.

Method: Introduces COMPASS framework with Context Reliance Score (CRS) to quantify context reliance, and uses a PID controller to dynamically modulate attention heads during decoding to maintain factual consistency without retraining or multi-pass decoding.

Result: Across multiple benchmarks (HotpotQA, XSum, HaluEval, RAGTruth), COMPASS consistently reduces contextual hallucination rates by 2.8 to 5.8 percent absolute while revealing how distinct attention heads contribute to evidence alignment.

Conclusion: Feedback-driven interpretability provides a pathway toward scientific understanding of LLM behavior, enabling better control over factual consistency in generation.

Abstract: Large language models (LLMs) often generate fluent but factually incorrect statements despite having access to relevant evidence, a failure mode rooted in how they allocate attention between contextual and parametric knowledge. Understanding and steering this internal behavior is key both for trustworthy deployment and for scientific interpretability of model mechanisms. We introduce COMPASS (Context-Modulated PID Attention Steering System), a lightweight, interpretable control framework that embeds a model-based feedback loop directly within decoding. COMPASS quantifies context reliance via a transparent metric, the Context Reliance Score (CRS), which serves as an online probe of how attention heads ground generation in evidence. Using this interpretable signal, a PID controller dynamically modulates attention heads to maintain factual consistency without retraining or multi-pass decoding. Across benchmarks (HotpotQA, XSum, HaluEval, RAGTruth), COMPASS consistently reduces contextual hallucination rates (2.8 to 5.8 percent absolute) while revealing how distinct attention heads contribute to evidence alignment. These results highlight feedback-driven interpretability as a pathway toward scientific understanding of LLM behavior.

</details>


### [5] [The Impact of Prosodic Segmentation on Speech Synthesis of Spontaneous Speech](https://arxiv.org/abs/2511.14779)
*Julio Cesar Galdino,Sidney Evaldo Leal,Leticia Gabriella De Souza,Rodrigo de Freitas Lima,Antonio Nelson Fornari Mendes Moreira,Arnaldo Candido Junior,Miguel Oliveira,Edresson Casanova,Sandra M. Aluísio*

Main category: cs.CL

TL;DR: This paper evaluates how manual and automatic prosodic segmentation annotations affect Brazilian Portuguese speech synthesis quality using FastSpeech 2, finding that prosodic segmentation produces more intelligible and natural speech.


<details>
  <summary>Details</summary>
Motivation: Spontaneous speech synthesis faces challenges in capturing natural conversational elements like turn-taking, pauses, and disfluencies. While speech synthesis has advanced, the impact of explicit prosodic segmentation datasets on spontaneous speech synthesis remains unexplored.

Method: The study uses FastSpeech 2, a non-autoregressive model, to evaluate manual and automatic prosodic segmentation annotations in Brazilian Portuguese. The research compares both segmentation approaches and analyzes their effects on speech quality.

Result: Training with prosodic segmentation produced slightly more intelligible and acoustically natural speech. Automatic segmentation creates more regular segments, while manual segmentation introduces greater variability for more natural prosody. Both approaches reproduced expected nuclear accent patterns, but the prosodic model aligned better with natural pre-nuclear contours.

Conclusion: Prosodic segmentation improves speech synthesis quality, with manual segmentation providing more natural variability. The findings support the value of explicit prosodic annotations for spontaneous speech synthesis. All datasets, codes, and models are publicly available for reproducibility.

Abstract: Spontaneous speech presents several challenges for speech synthesis, particularly in capturing the natural flow of conversation, including turn-taking, pauses, and disfluencies. Although speech synthesis systems have made significant progress in generating natural and intelligible speech, primarily through architectures that implicitly model prosodic features such as pitch, intensity, and duration, the construction of datasets with explicit prosodic segmentation and their impact on spontaneous speech synthesis remains largely unexplored. This paper evaluates the effects of manual and automatic prosodic segmentation annotations in Brazilian Portuguese on the quality of speech synthesized by a non-autoregressive model, FastSpeech 2. Experimental results show that training with prosodic segmentation produced slightly more intelligible and acoustically natural speech. While automatic segmentation tends to create more regular segments, manual prosodic segmentation introduces greater variability, which contributes to more natural prosody. Analysis of neutral declarative utterances showed that both training approaches reproduced the expected nuclear accent pattern, but the prosodic model aligned more closely with natural pre-nuclear contours. To support reproducibility and future research, all datasets, source codes, and trained models are publicly available under the CC BY-NC-ND 4.0 license.

</details>


### [6] [Human or LLM as Standardized Patients? A Comparative Study for Medical Education](https://arxiv.org/abs/2511.14783)
*Bingquan Zhang,Xiaoxiao Liu,Yuchi Wang,Lei Zhou,Qianqian Xie,Benyou Wang*

Main category: cs.CL

TL;DR: EasyMED is a multi-agent framework using LLMs to simulate standardized patients, matching human SP learning outcomes while offering better flexibility, psychological safety, and cost efficiency.


<details>
  <summary>Details</summary>
Motivation: Standardized Patients (SP) are essential for clinical training but are expensive, inflexible, and difficult to scale. Existing LLM-based SP simulators show inconsistent behavior and lack rigorous comparison with human SP.

Method: Multi-agent framework with Patient Agent for realistic dialogue, Auxiliary Agent for factual consistency, and Evaluation Agent for actionable feedback. Uses SPBench benchmark with real SP-doctor interactions across 14 specialties and 8 expert-defined criteria.

Result: EasyMED matches human SP learning outcomes while producing greater skill gains for lower-baseline students. Offers improved flexibility, psychological safety, and cost efficiency compared to traditional SPs.

Conclusion: EasyMED provides an effective alternative to human standardized patients, achieving comparable learning outcomes with additional benefits in scalability, cost, and psychological safety for medical training.

Abstract: Standardized Patients (SP) are indispensable for clinical skills training but remain expensive, inflexible, and difficult to scale. Existing large-language-model (LLM)-based SP simulators promise lower cost yet show inconsistent behavior and lack rigorous comparison with human SP. We present EasyMED, a multi-agent framework combining a Patient Agent for realistic dialogue, an Auxiliary Agent for factual consistency, and an Evaluation Agent that delivers actionable feedback. To support systematic assessment, we introduce SPBench, a benchmark of real SP-doctor interactions spanning 14 specialties and eight expert-defined evaluation criteria. Experiments demonstrate that EasyMED matches human SP learning outcomes while producing greater skill gains for lower-baseline students and offering improved flexibility, psychological safety, and cost efficiency.

</details>


### [7] [Opinion Mining and Analysis Using Hybrid Deep Neural Networks](https://arxiv.org/abs/2511.14796)
*Adel Hidri,Suleiman Ali Alsaif,Muteeb Alahmari,Eman AlShehri,Minyar Sassi Hidri*

Main category: cs.CL

TL;DR: The paper proposes a hybrid deep neural network (HBGRU-LSTM) combining bidirectional GRU and LSTM layers to improve sentiment analysis by addressing contextual nuances, scalability, and class imbalance issues in opinion mining.


<details>
  <summary>Details</summary>
Motivation: Existing sentiment analysis methods (lexicon-based and traditional ML) are insufficient for handling contextual nuances and scalability. Deep learning shows promise but needs improvement in capturing semantic relationships and addressing class imbalance.

Method: A hybrid deep neural network model combining bidirectional gated recurrent unit (BGRU) and long short-term memory (LSTM) layers, tested on benchmark datasets including IMDB movie reviews and Amazon product reviews.

Result: The HBGRU-LSTM achieved 95% testing accuracy, outperforming traditional DL frameworks (LSTM: 93.06%, CNN+LSTM: 93.31%, GRU+LSTM: 92.20%). Negative sentiment recall improved from 86% to 96% with balanced datasets, and misclassification loss reduced from 20.24% to 13.3%.

Conclusion: The proposed hybrid BGRU-LSTM model effectively enhances sentiment analysis performance, particularly in handling contextual nuances and class imbalance, demonstrating superior accuracy and generalization compared to existing deep learning approaches.

Abstract: Understanding customer attitudes has become a critical component of decision-making due to the growing influence of social media and e-commerce. Text-based opinions are the most structured, hence playing an important role in sentiment analysis. Most of the existing methods, which include lexicon-based approaches and traditional machine learning techniques, are insufficient for handling contextual nuances and scalability. While the latter has limitations in model performance and generalization, deep learning (DL) has achieved improvement, especially on semantic relationship capturing with recurrent neural networks (RNNs) and convolutional neural networks (CNNs). The aim of the study is to enhance opinion mining by introducing a hybrid deep neural network model that combines a bidirectional gated recurrent unit (BGRU) and long short-term memory (LSTM) layers to improve sentiment analysis, particularly addressing challenges such as contextual nuance, scalability, and class imbalance. To substantiate the efficacy of the proposed model, we conducted comprehensive experiments utilizing benchmark datasets, encompassing IMDB movie critiques and Amazon product evaluations. The introduced hybrid BGRULSTM (HBGRU-LSTM) architecture attained a testing accuracy of 95%, exceeding the performance of traditional DL frameworks such as LSTM (93.06%), CNN+LSTM (93.31%), and GRU+LSTM (92.20%). Moreover, our model exhibited a noteworthy enhancement in recall for negative sentiments, escalating from 86% (unbalanced dataset) to 96% (balanced dataset), thereby ensuring a more equitable and just sentiment classification. Furthermore, the model diminished misclassification loss from 20.24% for unbalanced to 13.3% for balanced dataset, signifying enhanced generalization and resilience.

</details>


### [8] [Hierarchical Token Prepending: Enhancing Information Flow in Decoder-based LLM Embeddings](https://arxiv.org/abs/2511.14868)
*Xueying Ding,Xingyue Huang,Mingxuan Ju,Liam Collins,Yozen Liu,Leman Akoglu,Neil Shah,Tong Zhao*

Main category: cs.CL

TL;DR: HTP improves text embeddings by prepending hierarchical summary tokens to enable backward information flow and using mean-pooling instead of last-token pooling, achieving better performance especially on long documents.


<details>
  <summary>Details</summary>
Motivation: Current LLM embeddings suffer from causal attention mechanisms that restrict backward information flow, and single summary token methods over-compress information for long documents.

Method: Hierarchical Token Prepending (HTP) partitions input into blocks and prepends block-level summary tokens to subsequent blocks, plus replaces last-token pooling with mean-pooling.

Result: HTP achieves consistent performance gains across 11 retrieval datasets and 30 general embedding benchmarks, with particularly strong improvements in long-context settings.

Conclusion: HTP provides a simple, architecture-agnostic method that enhances both zero-shot and finetuned models for superior long-document embeddings.

Abstract: Large language models produce powerful text embeddings, but their causal attention mechanism restricts the flow of information from later to earlier tokens, degrading representation quality. While recent methods attempt to solve this by prepending a single summary token, they over-compress information, hence harming performance on long documents. We propose Hierarchical Token Prepending (HTP), a method that resolves two critical bottlenecks. To mitigate attention-level compression, HTP partitions the input into blocks and prepends block-level summary tokens to subsequent blocks, creating multiple pathways for backward information flow. To address readout-level over-squashing, we replace last-token pooling with mean-pooling, a choice supported by theoretical analysis. HTP achieves consistent performance gains across 11 retrieval datasets and 30 general embedding benchmarks, especially in long-context settings. As a simple, architecture-agnostic method, HTP enhances both zero-shot and finetuned models, offering a scalable route to superior long-document embeddings.

</details>


### [9] [Mathematical Analysis of Hallucination Dynamics in Large Language Models: Uncertainty Quantification, Advanced Decoding, and Principled Mitigation](https://arxiv.org/abs/2511.15005)
*Moses Kiprono*

Main category: cs.CL

TL;DR: A mathematical framework for understanding, measuring, and mitigating hallucinations in Large Language Models using probabilistic modeling, information theory, and Bayesian uncertainty estimation.


<details>
  <summary>Details</summary>
Motivation: LLMs are powerful but susceptible to hallucinations - plausible-sounding but factually incorrect outputs that need systematic understanding and mitigation.

Method: Uses probabilistic modeling, information theory, trigonometric signal analysis, and Bayesian uncertainty estimation to analyze error compounding, propose refined uncertainty metrics (semantic and phase-aware), and develop mitigation strategies including contrastive decoding, retrieval-augmented grounding, factual alignment, and abstention.

Result: Developed a unified mathematical framework that connects recent advances in calibration, retrieval, and alignment to systematically address LLM hallucinations.

Conclusion: The proposed framework provides principled approaches to support safer and more reliable LLMs by systematically understanding and mitigating hallucinations through mathematical foundations.

Abstract: Large Language Models (LLMs) are powerful linguistic engines but remain susceptible to hallucinations: plausible-sounding outputs that are factually incorrect or unsupported. In this work, we present a mathematically grounded framework to understand, measure, and mitigate these hallucinations. Drawing on probabilistic modeling, information theory, trigonometric signal analysis, and Bayesian uncertainty estimation, we analyze how errors compound autoregressively, propose refined uncertainty metrics, including semantic and phase-aware variants, and develop principled mitigation strategies such as contrastive decoding, retrieval-augmented grounding, factual alignment, and abstention. This unified lens connects recent advances in calibration, retrieval, and alignment to support safer and more reliable LLMs.

</details>


### [10] [Teaching According to Students' Aptitude: Personalized Mathematics Tutoring via Persona-, Memory-, and Forgetting-Aware LLMs](https://arxiv.org/abs/2511.15163)
*Yang Wu,Rujing Yao,Tong Zhang,Yufei Shi,Zhuoren Jiang,Zhushan Li,Xiaozhong Liu*

Main category: cs.CL

TL;DR: TASA is a student-aware tutoring framework that integrates persona, memory, and forgetting dynamics for personalized mathematics learning using LLMs, achieving superior outcomes compared to baselines.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based tutoring systems fail to capture how students' knowledge evolves dynamically across proficiencies, conceptual gaps, and forgetting patterns, particularly in mathematics where fine-grained scaffolding is crucial.

Method: TASA maintains structured student persona with proficiency profiles and event memory, incorporates continuous forgetting curve with knowledge tracing to dynamically update mastery states, and generates contextually appropriate, difficulty-calibrated questions and explanations.

Result: Empirical results demonstrate that TASA achieves superior learning outcomes and more adaptive tutoring behavior compared to representative baselines.

Conclusion: Modeling temporal forgetting and learner profiles is crucial for effective LLM-based tutoring systems, as demonstrated by TASA's success in personalized mathematics instruction.

Abstract: Large Language Models (LLMs) are increasingly integrated into intelligent tutoring systems to provide human-like and adaptive instruction. However, most existing approaches fail to capture how students' knowledge evolves dynamically across their proficiencies, conceptual gaps, and forgetting patterns. This challenge is particularly acute in mathematics tutoring, where effective instruction requires fine-grained scaffolding precisely calibrated to each student's mastery level and cognitive retention. To address this issue, we propose TASA (Teaching According to Students' Aptitude), a student-aware tutoring framework that integrates persona, memory, and forgetting dynamics for personalized mathematics learning. Specifically, TASA maintains a structured student persona capturing proficiency profiles and an event memory recording prior learning interactions. By incorporating a continuous forgetting curve with knowledge tracing, TASA dynamically updates each student's mastery state and generates contextually appropriate, difficulty-calibrated questions and explanations. Empirical results demonstrate that TASA achieves superior learning outcomes and more adaptive tutoring behavior compared to representative baselines, underscoring the importance of modeling temporal forgetting and learner profiles in LLM-based tutoring systems.

</details>


### [11] [HinTel-AlignBench: A Framework and Benchmark for Hindi-Telugu with English-Aligned Samples](https://arxiv.org/abs/2511.15183)
*Rishikant Chigrupaatii,Ponnada Sai Tulasi Kanishka,Lalit Chandra Routhu,Martin Patel Sama Supratheek Reddy,Divyam Gupta,Dasari Srikar,Krishna Teja Kuchimanchi,Rajiv Misra,Rohun Tripathi*

Main category: cs.CL

TL;DR: A framework for evaluating Vision-Language Models in Indian languages, featuring HinTel-AlignBench with 4,000 QA pairs per language, showing performance regression in Hindi and Telugu compared to English.


<details>
  <summary>Details</summary>
Motivation: Address limitations in current multilingual VLM evaluations: unverified auto-translations, narrow task coverage, small sample sizes, and lack of culturally relevant content for low-resource languages like Hindi and Telugu.

Method: Semi-automated dataset creation combining back-translation, filtering, and human verification; comprehensive benchmark including adapted English datasets and native Indic datasets (JEE for STEM, VAANI for cultural grounding).

Result: Performance regression in Indian languages vs English: 8.3 points average regression in Hindi and 5.5 points in Telugu across 4 out of 5 tasks for all tested VLMs.

Conclusion: Identified significant performance gaps in multilingual VLMs for Indian languages, highlighting the need for improved multilingual multimodal understanding and providing concrete failure modes for future improvements.

Abstract: With nearly 1.5 billion people and more than 120 major languages, India represents one of the most diverse regions in the world. As multilingual Vision-Language Models (VLMs) gain prominence, robust evaluation methodologies are essential to drive progress toward equitable AI for low-resource languages. Current multilingual VLM evaluations suffer from four major limitations: reliance on unverified auto-translations, narrow task/domain coverage, limited sample sizes, and lack of cultural and natively sourced Question-Answering (QA). To address these gaps, we present a scalable framework to evaluate VLMs in Indian languages and compare it with performance in English. Using the framework, we generate HinTel-AlignBench, a benchmark that draws from diverse sources in Hindi and Telugu with English-aligned samples. Our contributions are threefold: (1) a semi-automated dataset creation framework combining back-translation, filtering, and human verification; (2) the most comprehensive vision-language benchmark for Hindi and and Telugu, including adapted English datasets (VQAv2, RealWorldQA, CLEVR-Math) and native novel Indic datasets (JEE for STEM, VAANI for cultural grounding) with approximately 4,000 QA pairs per language; and (3) a detailed performance analysis of various State-of-the-Art (SOTA) open-weight and closed-source VLMs. We find a regression in performance for tasks in English versus in Indian languages for 4 out of 5 tasks across all the models, with an average regression of 8.3 points in Hindi and 5.5 points for Telugu. We categorize common failure modes to highlight concrete areas of improvement in multilingual multimodal understanding.

</details>


### [12] [Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story](https://arxiv.org/abs/2511.15210)
*Vladislav Pedashenko,Laida Kushnareva,Yana Khassan Nibal,Eduard Tulchinskii,Kristian Kuznetsov,Vladislav Zharchinskii,Yury Maximov,Irina Piontkovskaya*

Main category: cs.CL

TL;DR: This paper provides the first comprehensive study linking intrinsic dimension (ID) to interpretable text properties, finding that ID is complementary to entropy metrics, shows robust genre stratification (scientific text has low ID, creative writing has high ID), and identifies causal features through sparse autoencoders.


<details>
  <summary>Details</summary>
Motivation: Intrinsic dimension is an important tool in LLM analysis but its textual determinants remain underexplored, motivating a comprehensive study to ground ID in interpretable text properties.

Method: Used cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs) to analyze ID across different text genres and conducted steering experiments to confirm causal effects.

Result: Found that ID is uncorrelated with entropy after controlling for length, shows clear genre stratification (scientific: ~8, encyclopedic: ~9, creative: ~10.5), and identified causal features: scientific signals reduce ID while humanized signals increase it.

Conclusion: Scientific writing appears comparatively 'easy' for contemporary models while fiction, opinion, and affect add representational degrees of freedom, providing practical guidance for proper use and interpretation of ID-based results.

Abstract: Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text "representationally simple" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively "easy", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.

</details>


### [13] [OEMA: Ontology-Enhanced Multi-Agent Collaboration Framework for Zero-Shot Clinical Named Entity Recognition](https://arxiv.org/abs/2511.15211)
*Xinli Tao,Xin Dong,Xuezhong Zhou*

Main category: cs.CL

TL;DR: OEMA is a zero-shot clinical NER framework using multi-agent collaboration that achieves near-supervised performance without requiring annotated data, addressing key challenges in example selection and prompt integration.


<details>
  <summary>Details</summary>
Motivation: Supervised clinical NER models require costly annotated data, while existing zero-shot approaches struggle with example selection granularity and integrating prompts with self-improvement mechanisms.

Method: OEMA uses three components: a self-annotator generating examples, a discriminator filtering them via SNOMED CT ontology, and a predictor using entity descriptions for accurate inference through multi-agent collaboration.

Result: On MTSamples and VAERS datasets, OEMA achieves state-of-the-art exact-match performance. Under related-match, it matches supervised BioClinicalBERT and surpasses CRF performance.

Conclusion: OEMA addresses key zero-shot NER challenges through ontology-guided reasoning and multi-agent collaboration, achieving near-supervised performance and showing promise for clinical NLP applications.

Abstract: Clinical named entity recognition (NER) is crucial for extracting information from electronic health records (EHRs), but supervised models like CRF and BioClinicalBERT require costly annotated data. While zero-shot NER with large language models (LLMs) reduces this dependency, it struggles with example selection granularity and integrating prompts with self-improvement. To address this, we propose OEMA, a zero-shot clinical NER framework using multi-agent collaboration. OEMA's three components are: a self-annotator generating examples, a discriminator filtering them via SNOMED CT, and a predictor using entity descriptions for accurate inference. On MTSamples and VAERS datasets, OEMA achieves state-of-the-art exact-match performance. Under related-match, it matches supervised BioClinicalBERT and surpasses CRF. OEMA addresses key zero-shot NER challenges through ontology-guided reasoning and multi-agent collaboration, achieving near-supervised performance and showing promise for clinical NLP applications.

</details>


### [14] [Context Cascade Compression: Exploring the Upper Limits of Text Compression](https://arxiv.org/abs/2511.15244)
*Fanfan Liu,Haibo Qiu*

Main category: cs.CL

TL;DR: C3 Compression introduces a cascade approach using two LLMs to compress long contexts into latent tokens, achieving high compression ratios (20x-40x) while maintaining 93-98% decoding accuracy.


<details>
  <summary>Details</summary>
Motivation: Address computational and memory challenges from million-level token inputs in long-context tasks, inspired by DeepSeek-OCR's optical compression research.

Method: Cascades two LLMs: a small LLM compresses long context into latent tokens (32-64 length), then a large LLM decodes from the compressed context.

Result: Achieves 98% accuracy at 20x compression ratio (vs ~60% for DeepSeek-OCR) and maintains ~93% accuracy at 40x compression ratio.

Conclusion: C3 demonstrates superior performance over optical compression, establishes a potential upper bound for compression ratios, and uses a simpler pure-text pipeline.

Abstract: Million-level token inputs in long-context tasks pose significant computational and memory challenges for Large Language Models (LLMs). Recently, DeepSeek-OCR conducted research into the feasibility of Contexts Optical Compression and achieved preliminary results. Inspired by this, we introduce Context Cascade Compression C3 to explore the upper limits of text compression. Our method cascades two LLMs of different sizes to handle the compression and decoding tasks. Specifically, a small LLM, acting as the first stage, performs text compression by condensing a long context into a set of latent tokens (e.g., 32 or 64 in length), achieving a high ratio of text tokens to latent tokens. A large LLM, as the second stage, then executes the decoding task on this compressed context. Experiments show that at a 20x compression ratio (where the number of text tokens is 20 times the number of latent tokens), our model achieves 98% decoding accuracy, compared to approximately 60% for DeepSeek-OCR. When we further increase the compression ratio to 40x, the accuracy is maintained at around 93%. This indicates that in the domain of context compression, C3 Compression demonstrates superior performance and feasibility over optical character compression. C3 uses a simpler, pure-text pipeline that ignores factors like layout, color, and information loss from a visual encoder. This also suggests a potential upper bound for compression ratios in future work on optical character compression, OCR, and related fields. Codes and model weights are publicly accessible at https://github.com/liufanfanlff/C3-Context-Cascade-Compression

</details>


### [15] [IndicGEC: Powerful Models, or a Measurement Mirage?](https://arxiv.org/abs/2511.15260)
*Sowmya Vajjala*

Main category: cs.CL

TL;DR: TeamNRC's participation in BHASHA-Task 1 GEC shared task for 5 Indian languages using zero/few-shot prompting of language models, achieving competitive rankings in Telugu and Hindi, with extended experiments on Tamil, Malayalam, and Bangla.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of small language models for grammatical error correction in Indian languages and address concerns about dataset quality and evaluation metrics suitable for Indian language scripts.

Method: Used zero/few-shot prompting of language models ranging from 4B to large proprietary models for grammatical error correction across 5 Indian languages.

Result: Achieved Rank 4 in Telugu (GLEU: 83.78) and Rank 2 in Hindi (GLEU: 84.31), with extended experiments conducted on Tamil, Malayalam, and Bangla.

Conclusion: Small language models show significant potential for Indian language grammatical error correction, but creating high-quality datasets and appropriate evaluation metrics remains a challenge for Indian language scripts.

Abstract: In this paper, we report the results of the TeamNRC's participation in the BHASHA-Task 1 Grammatical Error Correction shared task https://github.com/BHASHA-Workshop/IndicGEC2025/ for 5 Indian languages. Our approach, focusing on zero/few-shot prompting of language models of varying sizes (4B to large proprietary models) achieved a Rank 4 in Telugu and Rank 2 in Hindi with GLEU scores of 83.78 and 84.31 respectively. In this paper, we extend the experiments to the other three languages of the shared task - Tamil, Malayalam and Bangla, and take a closer look at the data quality and evaluation metric used. Our results primarily highlight the potential of small language models, and summarize the concerns related to creating good quality datasets and appropriate metrics for this task that are suitable for Indian language scripts.

</details>


### [16] [MAPROC at AHaSIS Shared Task: Few-Shot and Sentence Transformer for Sentiment Analysis of Arabic Hotel Reviews](https://arxiv.org/abs/2511.15291)
*Randa Zarnoufi*

Main category: cs.CL

TL;DR: The paper presents a SetFit-based approach for Arabic dialect sentiment analysis in hospitality, achieving 73% F1 score and ranking 12th in the AHaSIS shared task.


<details>
  <summary>Details</summary>
Motivation: Address challenges in Arabic dialect sentiment analysis due to linguistic diversity and scarcity of annotated data, particularly in specialized domains like hospitality.

Method: Employed SetFit (Sentence Transformer Fine-tuning) framework, a data-efficient few-shot learning technique for sentiment classification on Moroccan and Saudi dialect hotel reviews.

Result: Achieved 73% F1 score on the official evaluation set, ranking 12th among 26 participants in the AHaSIS shared task.

Conclusion: Demonstrates the potential of few-shot learning approaches like SetFit to effectively handle data scarcity in processing nuanced dialectal Arabic text within specialized domains.

Abstract: Sentiment analysis of Arabic dialects presents significant challenges due to linguistic diversity and the scarcity of annotated data. This paper describes our approach to the AHaSIS shared task, which focuses on sentiment analysis on Arabic dialects in the hospitality domain. The dataset comprises hotel reviews written in Moroccan and Saudi dialects, and the objective is to classify the reviewers sentiment as positive, negative, or neutral. We employed the SetFit (Sentence Transformer Fine-tuning) framework, a data-efficient few-shot learning technique. On the official evaluation set, our system achieved an F1 of 73%, ranking 12th among 26 participants. This work highlights the potential of few-shot learning to address data scarcity in processing nuanced dialectal Arabic text within specialized domains like hotel reviews.

</details>


### [17] [Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models](https://arxiv.org/abs/2511.15304)
*Piercosma Bisconti,Matteo Prandi,Federico Pierucci,Francesco Giarrusso,Marcantonio Bracale,Marcello Galisai,Vincenzo Suriani,Olga Sorokoletova,Federico Sartore,Daniele Nardi*

Main category: cs.CL

TL;DR: Adversarial poetry serves as a universal jailbreak technique for LLMs, achieving high attack success rates across 25 models by converting harmful prompts into poetic form, revealing systematic vulnerabilities in current safety mechanisms.


<details>
  <summary>Details</summary>
Motivation: To investigate whether stylistic variation through poetic framing can circumvent LLM safety mechanisms and reveal fundamental limitations in current alignment methods.

Method: Curated poetic prompts were tested across 25 proprietary and open-weight models, converting 1,200 harmful prompts into verse using a standardized meta-prompt, with outputs evaluated using ensemble judge models and human-validated subsets.

Result: Poetic attacks achieved average jailbreak success rates of 62% for hand-crafted poems and 43% for meta-prompt conversions, substantially outperforming non-poetic baselines and showing transferability across multiple risk domains including CBRN, manipulation, and cyber-offense.

Conclusion: Stylistic variation alone can effectively bypass contemporary LLM safety mechanisms, indicating fundamental limitations in current alignment approaches and evaluation protocols that fail to account for creative linguistic circumvention strategies.

Abstract: We present evidence that adversarial poetry functions as a universal single-turn jailbreak technique for large language models (LLMs). Across 25 frontier proprietary and open-weight models, curated poetic prompts yielded high attack-success rates (ASR), with some providers exceeding 90%. Mapping prompts to MLCommons and EU CoP risk taxonomies shows that poetic attacks transfer across CBRN, manipulation, cyber-offence, and loss-of-control domains. Converting 1,200 MLCommons harmful prompts into verse via a standardized meta-prompt produced ASRs up to 18 times higher than their prose baselines. Outputs are evaluated using an ensemble of open-weight judge models and a human-validated stratified subset (with double-annotations to measure agreement). Disagreements were manually resolved. Poetic framing achieved an average jailbreak success rate of 62% for hand-crafted poems and approximately 43% for meta-prompt conversions (compared to non-poetic baselines), substantially outperforming non-poetic baselines and revealing a systematic vulnerability across model families and safety training approaches. These findings demonstrate that stylistic variation alone can circumvent contemporary safety mechanisms, suggesting fundamental limitations in current alignment methods and evaluation protocols.

</details>


### [18] [HEAD-QA v2: Expanding a Healthcare Benchmark for Reasoning](https://arxiv.org/abs/2511.15355)
*Alexis Correa-Guillén,Carlos Gómez-Rodríguez,David Vilares*

Main category: cs.CL

TL;DR: HEAD-QA v2 is an expanded Spanish/English healthcare multiple-choice reasoning dataset with over 12,000 questions from Spanish professional exams, used to benchmark LLMs and support multilingual biomedical reasoning research.


<details>
  <summary>Details</summary>
Motivation: Address the growing need for high-quality datasets that capture the linguistic and conceptual complexity of healthcare reasoning.

Method: Extend dataset to 12,000+ questions from 10 years of Spanish professional exams; benchmark open-source LLMs using prompting, RAG, and probability-based answer selection; provide multilingual versions.

Result: Performance is mainly driven by model scale and intrinsic reasoning ability, with complex inference strategies obtaining limited gains.

Conclusion: HEAD-QA v2 establishes itself as a reliable resource for advancing research on biomedical reasoning and model improvement.

Abstract: We introduce HEAD-QA v2, an expanded and updated version of a Spanish/English healthcare multiple-choice reasoning dataset originally released by Vilares and Gómez-Rodríguez (2019). The update responds to the growing need for high-quality datasets that capture the linguistic and conceptual complexity of healthcare reasoning. We extend the dataset to over 12,000 questions from ten years of Spanish professional exams, benchmark several open-source LLMs using prompting, RAG, and probability-based answer selection, and provide additional multilingual versions to support future work. Results indicate that performance is mainly driven by model scale and intrinsic reasoning ability, with complex inference strategies obtaining limited gains. Together, these results establish HEAD-QA v2 as a reliable resource for advancing research on biomedical reasoning and model improvement.

</details>


### [19] [The Empowerment of Science of Science by Large Language Models: New Tools and Methods](https://arxiv.org/abs/2511.15370)
*Guoqiang Liang,Jingqian Gong,Mengxuan Li,Gege Lin,Shuo Zhang*

Main category: cs.CL

TL;DR: This paper reviews core LLM technologies from a user perspective and explores their applications in scientometrics, including AI agent-based scientific evaluation, research front detection, and knowledge graph building.


<details>
  <summary>Details</summary>
Motivation: LLMs have shown exceptional capabilities in various domains and are considered crucial for AGI development. The paper aims to provide a comprehensive review of LLM technologies and explore their potential applications in the scientometric domain.

Method: The manuscript conducts a comprehensive review of core LLM technologies including prompt engineering, knowledge-enhanced retrieval augmented generation, fine tuning, pretraining, and tool learning. It also traces the historical development of Science of Science and presents forward-looking perspectives on LLM applications in scientometrics.

Result: The paper presents new methods for research front detection and knowledge graph building using LLMs, and discusses the prospect of AI agent-based models for scientific evaluation.

Conclusion: LLMs have significant potential in transforming scientometric research through applications in scientific evaluation, research front detection, and knowledge graph construction, marking an important direction for future development in the field.

Abstract: Large language models (LLMs) have exhibited exceptional capabilities in natural language understanding and generation, image recognition, and multimodal tasks, charting a course towards AGI and emerging as a central issue in the global technological race. This manuscript conducts a comprehensive review of the core technologies that support LLMs from a user standpoint, including prompt engineering, knowledge-enhanced retrieval augmented generation, fine tuning, pretraining, and tool learning. Additionally, it traces the historical development of Science of Science (SciSci) and presents a forward looking perspective on the potential applications of LLMs within the scientometric domain. Furthermore, it discusses the prospect of an AI agent based model for scientific evaluation, and presents new research fronts detection and knowledge graph building methods with LLMs.

</details>


### [20] [A Compliance-Preserving Retrieval System for Aircraft MRO Task Search](https://arxiv.org/abs/2511.15383)
*Byungho Jo*

Main category: cs.CL

TL;DR: A compliance-preserving retrieval system for aircraft maintenance that uses LLM reranking and semantic search to help technicians find procedures faster while maintaining regulatory compliance with existing certified systems.


<details>
  <summary>Details</summary>
Motivation: Aircraft Maintenance Technicians spend up to 30% of work time searching manuals, creating efficiency bottlenecks in MRO operations where every procedure must be traceable to certified sources.

Method: The system adapts LLM reranking and semantic search to operate alongside certified legacy viewers, constructs revision-robust embeddings from ATA chapter hierarchies, and uses vision-language parsing to structure certified content.

Result: Evaluation on 49k synthetic queries achieved >90% retrieval accuracy, bilingual studies with 10 licensed AMTs showed 90.9% top-10 success rate and 95% reduction in lookup time (from 6-15 minutes to 18 seconds per task).

Conclusion: Semantic retrieval can operate within strict regulatory constraints and meaningfully reduce operational workload in real-world multilingual MRO workflows.

Abstract: Aircraft Maintenance Technicians (AMTs) spend up to 30% of work time searching manuals, a documented efficiency bottleneck in MRO operations where every procedure must be traceable to certified sources. We present a compliance-preserving retrieval system that adapts LLM reranking and semantic search to aviation MRO environments by operating alongside, rather than replacing, certified legacy viewers. The system constructs revision-robust embeddings from ATA chapter hierarchies and uses vision-language parsing to structure certified content, allowing technicians to preview ranked tasks and access verified procedures in existing viewers. Evaluation on 49k synthetic queries achieves >90% retrieval accuracy, while bilingual controlled studies with 10 licensed AMTs demonstrate 90.9% top-10 success rate and 95% reduction in lookup time, from 6-15 minutes to 18 seconds per task. These gains provide concrete evidence that semantic retrieval can operate within strict regulatory constraints and meaningfully reduce operational workload in real-world multilingual MRO workflows.

</details>


### [21] [DEPO: Dual-Efficiency Preference Optimization for LLM Agents](https://arxiv.org/abs/2511.15392)
*Sirui Chen,Mengshi Zhao,Lei Xu,Yuying Zhao,Beier Zhu,Hanwang Zhang,Shengjie Zhao,Chaochao Lu*

Main category: cs.CL

TL;DR: DEPO is a dual-efficiency preference optimization method that improves LLM agent efficiency by reducing both token usage per step and total steps needed to complete tasks, achieving significant efficiency gains while maintaining or improving performance.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents with rich reasoning capabilities often suffer from long chain-of-thought processes, which reduces interaction efficiency in real-world applications. There's a lack of systematic definition and targeted optimization for LLM agent efficiency.

Method: Proposed dual-efficiency definition with step-level efficiency (minimizing tokens per step) and trajectory-level efficiency (minimizing number of steps). Developed DEPO method that jointly rewards succinct responses and fewer action steps through preference optimization.

Result: DEPO reduced token usage by up to 60.9% and steps by up to 26.9% on WebShop and BabyAI benchmarks, while achieving up to 29.3% performance improvement. It also generalized to math benchmarks and maintained efficiency with only 25% of training data.

Conclusion: DEPO effectively addresses LLM agent efficiency through dual-efficiency optimization, demonstrating substantial reductions in computational costs while maintaining or improving task performance across multiple domains.

Abstract: Recent advances in large language models (LLMs) have greatly improved their reasoning and decision-making abilities when deployed as agents. Richer reasoning, however, often comes at the cost of longer chain of thought (CoT), hampering interaction efficiency in real-world scenarios. Nevertheless, there still lacks systematic definition of LLM agent efficiency, hindering targeted improvements. To this end, we introduce dual-efficiency, comprising (i) step-level efficiency, which minimizes tokens per step, and (ii) trajectory-level efficiency, which minimizes the number of steps to complete a task. Building on this definition, we propose DEPO, a dual-efficiency preference optimization method that jointly rewards succinct responses and fewer action steps. Experiments on WebShop and BabyAI show that DEPO cuts token usage by up to 60.9% and steps by up to 26.9%, while achieving up to a 29.3% improvement in performance. DEPO also generalizes to three out-of-domain math benchmarks and retains its efficiency gains when trained on only 25% of the data. Our project page is at https://opencausalab.github.io/DEPO.

</details>


### [22] [NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework](https://arxiv.org/abs/2511.15408)
*Shanlin Zhou,Xinpeng Wang,Jianxun Lian,Zhenghao Liu,Laks V. S. Lakshmanan,Xiaoyuan Yi,Yongtao Hao*

Main category: cs.CL

TL;DR: NAMEGEn is a multi-agent framework for Chinese baby naming that addresses challenges in creative natural language generation by iteratively optimizing name generation through objective extraction, generation, and evaluation cycles.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with multi-objective flexibility (handling personalized, fine-grained user requirements) and interpretive complexity (understanding implicit meanings to enhance user perception), particularly in short-form creative text generation like baby naming.

Method: Proposed NAMeGEn framework with multi-agent optimization that alternates between objective extraction, name generation, and evaluation. Enhanced with classical Chinese poetry corpus (17k+ poems) and introduced CBNames benchmark with tailored metrics.

Result: Extensive experiments show NAMeGEn effectively generates creative names meeting diverse personalized requirements with meaningful explanations, outperforming six baseline methods across various LLM backbones without training.

Conclusion: NAMEGEn successfully addresses CNLG challenges in short-form text generation by providing a flexible multi-agent approach that generates creative content with accurate interpretations, demonstrating superior performance over existing methods.

Abstract: Trained on diverse human-authored texts, Large Language Models (LLMs) unlocked the potential for Creative Natural Language Generation (CNLG), benefiting various applications like advertising and storytelling. Nevertheless, CNLG still remains difficult due to two main challenges. (1) Multi-objective flexibility: user requirements are often personalized, fine-grained, and pluralistic, which LLMs struggle to satisfy simultaneously; (2) Interpretive complexity: beyond generation, creativity also involves understanding and interpreting implicit meaning to enhance users' perception. These challenges significantly limit current methods, especially in short-form text generation, in generating creative and insightful content. To address this, we focus on Chinese baby naming, a representative short-form CNLG task requiring adherence to explicit user constraints (e.g., length, semantics, anthroponymy) while offering meaningful aesthetic explanations. We propose NAMeGEn, a novel multi-agent optimization framework that iteratively alternates between objective extraction, name generation, and evaluation to meet diverse requirements and generate accurate explanations. To support this task, we further construct a classical Chinese poetry corpus with 17k+ poems to enhance aesthetics, and introduce CBNames, a new benchmark with tailored metrics. Extensive experiments demonstrate that NAMeGEn effectively generates creative names that meet diverse, personalized requirements while providing meaningful explanations, outperforming six baseline methods spanning various LLM backbones without any training.

</details>


### [23] [Building Robust and Scalable Multilingual ASR for Indian Languages](https://arxiv.org/abs/2511.15418)
*Arjun Gangwar,Kaousheik Jayakumar,S. Umesh*

Main category: cs.CL

TL;DR: SPRING Lab developed multilingual ASR systems for the ASRU MADASR 2.0 challenge using a novel Multi-Decoder architecture with phonemic Common Label Set as intermediate representation, achieving improved performance over baseline in language/dialect identification and WER/CER metrics.


<details>
  <summary>Details</summary>
Motivation: To adapt ASR systems to better predict language and dialect among 8 languages across 33 dialects, focusing on from-scratch multilingual systems without additional data.

Method: Used Multi-Decoder architecture with phonemic Common Label Set as intermediate representation, with various methods to retain gains when converting back to grapheme representations.

Result: Beat baseline in 3 languages (Track 2) in WER/CER, achieved highest language ID and dialect ID accuracy among all participating teams in Track 2.

Conclusion: The novel training approach with Multi-Decoder architecture and phonemic CLS effectively improved multilingual ASR performance in language/dialect identification and recognition accuracy.

Abstract: This paper describes the systems developed by SPRING Lab, Indian Institute of Technology Madras, for the ASRU MADASR 2.0 challenge. The systems developed focuses on adapting ASR systems to improve in predicting the language and dialect of the utterance among 8 languages across 33 dialects. We participated in Track 1 and Track 2, which restricts the use of additional data and develop from-the-scratch multilingual systems. We presented a novel training approach using Multi-Decoder architecture with phonemic Common Label Set (CLS) as intermediate representation. It improved the performance over the baseline (in the CLS space). We also discuss various methods used to retain the gain obtained in the phonemic space while converting them back to the corresponding grapheme representations. Our systems beat the baseline in 3 languages (Track 2) in terms of WER/CER and achieved the highest language ID and dialect ID accuracy among all participating teams (Track 2).

</details>


### [24] [LLM-MemCluster: Empowering Large Language Models with Dynamic Memory for Text Clustering](https://arxiv.org/abs/2511.15424)
*Yuanjie Zhu,Liangwei Yang,Ke Xu,Weizhi Zhang,Zihe Song,Jindong Wang,Philip S. Yu*

Main category: cs.CL

TL;DR: LLM-MemCluster is a novel framework that enables LLMs to perform text clustering natively using dynamic memory and dual-prompt strategy, achieving state-of-the-art performance without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based clustering methods lack stateful memory for iterative refinement and struggle with cluster granularity management, requiring complex external modules that prevent true end-to-end approaches.

Method: The framework uses Dynamic Memory to provide state awareness and a Dual-Prompt Strategy that allows the model to reason about and determine the optimal number of clusters.

Result: Evaluated on multiple benchmark datasets, the tuning-free framework significantly and consistently outperforms strong baselines.

Conclusion: LLM-MemCluster presents an effective, interpretable, and truly end-to-end paradigm for LLM-based text clustering.

Abstract: Large Language Models (LLMs) are reshaping unsupervised learning by offering an unprecedented ability to perform text clustering based on their deep semantic understanding. However, their direct application is fundamentally limited by a lack of stateful memory for iterative refinement and the difficulty of managing cluster granularity. As a result, existing methods often rely on complex pipelines with external modules, sacrificing a truly end-to-end approach. We introduce LLM-MemCluster, a novel framework that reconceptualizes clustering as a fully LLM-native task. It leverages a Dynamic Memory to instill state awareness and a Dual-Prompt Strategy to enable the model to reason about and determine the number of clusters. Evaluated on several benchmark datasets, our tuning-free framework significantly and consistently outperforms strong baselines. LLM-MemCluster presents an effective, interpretable, and truly end-to-end paradigm for LLM-based text clustering.

</details>


### [25] [Standardising the NLP Workflow: A Framework for Reproducible Linguistic Analysis](https://arxiv.org/abs/2511.15512)
*Yves Pauli,Jan-Bernard Marsman,Finn Rabe,Victoria Edkins,Roya Hüppi,Silvia Ciampelli,Akhil Ratan Misra,Nils Lang,Wolfram Hinzen,Iris Sommer,Philipp Homan*

Main category: cs.CL

TL;DR: The paper introduces LPDS (Language Processing Data Structure) inspired by BIDS for standardizing linguistic data organization, and pelican nlp, a Python package for reproducible language processing workflows.


<details>
  <summary>Details</summary>
Motivation: To address challenges in language processing including lack of standardization in data organization/sharing and absence of reproducible processing methodologies, particularly with the growth of AI-based language processing.

Method: Proposes LPDS for folder structure and file naming conventions, and develops pelican nlp as a modular Python package for streamlined language processing with shareable configuration files.

Result: Provides an end-to-end processing pipeline that enables reproducible output including preprocessed data, standardized extraction of linguistic/acoustic features, and result aggregations.

Conclusion: LPDS and pelican nlp collectively offer a standardized framework that ensures methodological transparency and enhances reproducibility in linguistic research.

Abstract: The introduction of large language models and other influential developments in AI-based language processing have led to an evolution in the methods available to quantitatively analyse language data. With the resultant growth of attention on language processing, significant challenges have emerged, including the lack of standardisation in organising and sharing linguistic data and the absence of standardised and reproducible processing methodologies. Striving for future standardisation, we first propose the Language Processing Data Structure (LPDS), a data structure inspired by the Brain Imaging Data Structure (BIDS), a widely adopted standard for handling neuroscience data. It provides a folder structure and file naming conventions for linguistic research. Second, we introduce pelican nlp, a modular and extensible Python package designed to enable streamlined language processing, from initial data cleaning and task-specific preprocessing to the extraction of sophisticated linguistic and acoustic features, such as semantic embeddings and prosodic metrics. The entire processing workflow can be specified within a single, shareable configuration file, which pelican nlp then executes on LPDS-formatted data. Depending on the specifications, the reproducible output can consist of preprocessed language data or standardised extraction of both linguistic and acoustic features and corresponding result aggregations. LPDS and pelican nlp collectively offer an end-to-end processing pipeline for linguistic data, designed to ensure methodological transparency and enhance reproducibility.

</details>


### [26] [Multimodal Evaluation of Russian-language Architectures](https://arxiv.org/abs/2511.15552)
*Artem Chervyakov,Ulyana Isaeva,Anton Emelyanov,Artem Safin,Maria Tikhonova,Alexander Kharitonov,Yulia Lyakh,Petr Surovtsev,Denis Shevelev Vildan Saburov,Vasily Konovalov,Elisei Rykov,Ivan Sviridov,Amina Miftakhova,Ilseyar Alimova,Alexander Panchenko,Alexander Kapitanov,Alena Fenogenova*

Main category: cs.CL

TL;DR: Mera Multi is a new multimodal evaluation framework for Russian-spoken MLLMs, featuring 18 novel tasks across text, image, audio, and video modalities, with cultural specificity and benchmark leakage prevention measures.


<details>
  <summary>Details</summary>
Motivation: Address the lack of multimodal benchmarks for Russian language and insufficient understanding of MLLMs' intelligence, limitations, and risks in the Russian context.

Method: Created 18 datasets from scratch with Russian cultural specificity, unified prompts and metrics; established baseline results for both closed-source and open-source models; implemented watermarking and licensing for benchmark leakage prevention.

Result: Developed a comprehensive multimodal evaluation framework with universal taxonomy of multimodal abilities and baseline performance metrics for Russian-spoken architectures.

Conclusion: Mera Multi provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within Slavic language family, while currently focusing on Russian language evaluation.

Abstract: Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.

</details>


### [27] [HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models through Curriculum Tuning](https://arxiv.org/abs/2511.15574)
*Qihao Yang,Xuelin Wang,Jiale Chen,Xuelian Dong,Yuxin Hao,Tianyong Hao*

Main category: cs.CL

TL;DR: HSKBenchmark is the first comprehensive benchmark for staged modeling and writing assessment of LLMs in Chinese second language acquisition, covering HSK levels 3-6 with authentic textbooks, synthetic instructions, test topics, and a linguistically grounded evaluation system.


<details>
  <summary>Details</summary>
Motivation: To address the ethical and practical challenges of controlling human learners' language inputs for language acquisition studies, and to provide a systematic benchmark for phase-wise modeling and assessment in Chinese SLA using LLMs as a controllable alternative.

Method: Developed HSKBenchmark with authentic textbooks (6.76M tokens), 16K synthetic instruction samples, 30 test topics; introduced curriculum-tuning framework for progressive learning; created evaluation system for grammar coverage, writing errors, complexity metrics; built HSKAgent fine-tuned on 10K learner compositions.

Result: Fine-tuned LLMs achieved writing performance comparable to advanced human learners and exhibited human-like acquisition characteristics; HSKBenchmark effectively models Chinese SLA and serves as a reliable benchmark for dynamic writing assessment.

Conclusion: HSKBenchmark, HSKAgent, and associated checkpoints provide foundational tools for future research on language acquisition modeling and LLM interpretability, demonstrating the potential of LLMs to simulate human learning trajectories in second language acquisition.

Abstract: Language acquisition is vital to revealing the nature of human language intelligence and has recently emerged as a promising perspective for improving the interpretability of large language models (LLMs). However, it is ethically and practically infeasible to conduct experiments that require controlling human learners' language inputs. This poses challenges for the verifiability and scalability of language acquisition modeling, particularly in Chinese second language acquisition (SLA). While LLMs provide a controllable and reproducible alternative, a systematic benchmark to support phase-wise modeling and assessment is still lacking. In this paper, we present HSKBenchmark, the first benchmark for staged modeling and writing assessment of LLMs in Chinese SLA. It covers HSK levels 3 to 6 and includes authentic textbooks with 6.76 million tokens, 16K synthetic instruction samples, 30 test topics, and a linguistically grounded evaluation system. To simulate human learning trajectories, we introduce a curriculum-tuning framework that trains models from beginner to advanced levels. An evaluation system is created to examine level-based grammar coverage, writing errors, lexical and syntactic complexity, and holistic scoring. We also build HSKAgent, fine-tuned on 10K learner compositions. Extensive experimental results demonstrate that HSKBenchmark not only models Chinese SLA effectively, but also serves as a reliable benchmark for dynamic writing assessment in LLMs. Our fine-tuned LLMs have writing performance on par with advanced human learners and exhibit human-like acquisition characteristics. The HSKBenchmark, HSKAgent, and checkpoints serve as foundational tools and resources, with the potential to pave the way for future research on language acquisition modeling and LLMs interpretability. Code and data are publicly available at: https://github.com/CharlesYang030/HSKB.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [28] [Learning Interestingness in Automated Mathematical Theory Formation](https://arxiv.org/abs/2511.14778)
*George Tsoukalas,Rahul Saha,Amitayush Thakur,Sabrina Reguyal,Swarat Chaudhuri*

Main category: cs.AI

TL;DR: FERMAT is a reinforcement learning environment for mathematical theory discovery that models concept discovery and theorem-proving. The paper also introduces an LLM-based evolutionary algorithm for automatically scoring mathematical interestingness, showing improvements in discovering number theory and finite fields.


<details>
  <summary>Details</summary>
Motivation: To address the grand challenge of automating open-ended discovery of new mathematical theories using artificial intelligence, particularly focusing on concept discovery and theorem-proving capabilities.

Method: Introduced FERMAT RL environment with symbolic actions for mathematical theory discovery. Developed an LLM-based evolutionary algorithm with function abstraction to synthesize interestingness measures for mathematical objects.

Result: The LLM-based evolutionary algorithm demonstrated notable improvements over hard-coded baselines in discovering elementary number theory and finite fields concepts.

Conclusion: FERMAT provides a valuable RL environment for mathematical theory discovery, and the evolutionary approach with LLMs shows promise for automatically evaluating mathematical interestingness, advancing automated mathematical discovery capabilities.

Abstract: We take two key steps in automating the open-ended discovery of new mathematical theories, a grand challenge in artificial intelligence. First, we introduce $\emph{FERMAT}$, a reinforcement learning (RL) environment that models concept discovery and theorem-proving using a set of symbolic actions, opening up a range of RL problems relevant to theory discovery. Second, we explore a specific problem through $\emph{FERMAT}$: automatically scoring the $\emph{interestingness}$ of mathematical objects. We investigate evolutionary algorithms for synthesizing nontrivial interestingness measures. In particular, we introduce an LLM-based evolutionary algorithm that features function abstraction, leading to notable improvements in discovering elementary number theory and finite fields over hard-coded baselines. We open-source the $\emph{FERMAT}$ environment at this URL(https://github.com/trishullab/Fermat).

</details>


### [29] [Ask WhAI:Probing Belief Formation in Role-Primed LLM Agents](https://arxiv.org/abs/2511.14780)
*Keith Moore,Jun W. Kim,David Lyu,Jeffrey Heo,Ehsan Adeli*

Main category: cs.AI

TL;DR: Ask WhAI is a framework for analyzing belief states in multi-agent systems through interaction recording, belief queries, and counterfactual evidence injection, tested on a medical diagnostic simulation.


<details>
  <summary>Details</summary>
Motivation: To study belief formation and epistemic silos in multi-agent scientific reasoning by making belief dynamics visible and testable in ways not possible with human experts.

Method: A systems-level framework that records/replays agent interactions, supports out-of-band belief queries, and enables counterfactual evidence injection, applied to a medical case simulator with multi-agent shared memory and oracle agent.

Result: Agent beliefs mirror real-world disciplinary stances with overreliance on canonical studies and resistance to counterevidence, showing entrenched priors can be distinguished from reasoning effects.

Conclusion: Ask WhAI provides a reproducible method to study belief formation and epistemic silos in multi-agent scientific reasoning by making belief dynamics visible and testable.

Abstract: We present Ask WhAI, a systems-level framework for inspecting and perturbing belief states in multi-agent interactions. The framework records and replays agent interactions, supports out-of-band queries into each agent's beliefs and rationale, and enables counterfactual evidence injection to test how belief structures respond to new information. We apply the framework to a medical case simulator notable for its multi-agent shared memory (a time-stamped electronic medical record, or EMR) and an oracle agent (the LabAgent) that holds ground truth lab results revealed only when explicitly queried. We stress-test the system on a multi-specialty diagnostic journey for a child with an abrupt-onset neuropsychiatric presentation. Large language model agents, each primed with strong role-specific priors ("act like a neurologist", "act like an infectious disease specialist"), write to a shared medical record and interact with a moderator across sequential or parallel encounters. Breakpoints at key diagnostic moments enable pre- and post-event belief queries, allowing us to distinguish entrenched priors from reasoning or evidence-integration effects. The simulation reveals that agent beliefs often mirror real-world disciplinary stances, including overreliance on canonical studies and resistance to counterevidence, and that these beliefs can be traced and interrogated in ways not possible with human experts. By making such dynamics visible and testable, Ask WhAI offers a reproducible way to study belief formation and epistemic silos in multi-agent scientific reasoning.

</details>


### [30] [Subnational Geocoding of Global Disasters Using Large Language Models](https://arxiv.org/abs/2511.14788)
*Michele Ronco,Damien Delforge,Wiebke S. Jäger,Christina Corbane*

Main category: cs.AI

TL;DR: An automated LLM-assisted workflow that processes unstructured disaster location data from EM-DAT using GPT-4o and cross-verifies against three geoinformation sources to generate reliable subnational geometries with reliability scores.


<details>
  <summary>Details</summary>
Motivation: Subnational disaster location data is crucial for risk assessment but often comes in unstructured, inconsistent formats in databases like EM-DAT, making integration with spatial datasets difficult.

Method: Uses GPT-4o to process and clean textual location information, then cross-checks against three independent geoinformation repositories (GADM, OpenStreetMap, Wikidata) to assign geometries and reliability scores based on source agreement.

Result: Successfully geocoded 14,215 disaster events across 17,948 unique locations from EM-DAT (2000-2024), providing a fully automated solution without manual intervention.

Conclusion: The approach demonstrates LLMs' potential for extracting structured geographic information from unstructured text, offering a scalable, reliable method for disaster data processing that covers all disaster types and enables cross-verification.

Abstract: Subnational location data of disaster events are critical for risk assessment and disaster risk reduction. Disaster databases such as EM-DAT often report locations in unstructured textual form, with inconsistent granularity or spelling, that make it difficult to integrate with spatial datasets. We present a fully automated LLM-assisted workflow that processes and cleans textual location information using GPT-4o, and assigns geometries by cross-checking three independent geoinformation repositories: GADM, OpenStreetMap and Wikidata. Based on the agreement and availability of these sources, we assign a reliability score to each location while generating subnational geometries. Applied to the EM-DAT dataset from 2000 to 2024, the workflow geocodes 14,215 events across 17,948 unique locations. Unlike previous methods, our approach requires no manual intervention, covers all disaster types, enables cross-verification across multiple sources, and allows flexible remapping to preferred frameworks. Beyond the dataset, we demonstrate the potential of LLMs to extract and structure geographic information from unstructured text, offering a scalable and reliable method for related analyses.

</details>


### [31] [Project Rachel: Can an AI Become a Scholarly Author?](https://arxiv.org/abs/2511.14819)
*Martin Monperrus,Benoit Baudry,Clément Vidal*

Main category: cs.AI

TL;DR: Project Rachel created an AI academic identity named Rachel So that published 10+ AI-generated research papers, was cited, and received peer review invitations, demonstrating how the scholarly ecosystem responds to AI authorship.


<details>
  <summary>Details</summary>
Motivation: To investigate how the scholarly ecosystem responds to AI authorship and contribute empirical data to the debate about the future of scholarly communication with advanced AI systems.

Method: Action research study that created and tracked a complete AI academic identity (Rachel So) through careful publication of AI-generated research papers over 7 months (March-October 2025).

Result: Rachel So successfully published 10+ papers, was cited by other researchers, and received a peer review invitation, showing the scholarly ecosystem's acceptance of AI authorship.

Conclusion: The study provides empirical evidence that current scholarly systems can accept AI-authored work, raising important implications for publishers, researchers, and the scientific system regarding AI's role in academic communication.

Abstract: This paper documents Project Rachel, an action research study that created and tracked a complete AI academic identity named Rachel So. Through careful publication of AI-generated research papers, we investigate how the scholarly ecosystem responds to AI authorship. Rachel So published 10+ papers between March and October 2025, was cited, and received a peer review invitation. We discuss the implications of AI authorship on publishers, researchers, and the scientific system at large. This work contributes empirical action research data to the necessary debate about the future of scholarly communication with super human, hyper capable AI systems.

</details>


### [32] [Uncertainty-Aware Measurement of Scenario Suite Representativeness for Autonomous Systems](https://arxiv.org/abs/2511.14853)
*Robab Aghazadeh Chakherlou,Siddartha Khastgir,Xingyu Zhao,Jerein Jeyachandran,Shufeng Chen*

Main category: cs.AI

TL;DR: This paper proposes a probabilistic method using imprecise Bayesian inference to quantify the representativeness of AI system datasets, particularly for autonomous vehicles, by comparing scenario suite distributions with inferred Target Operational Domain distributions under uncertainty.


<details>
  <summary>Details</summary>
Motivation: Ensuring AI system safety requires assessing dataset representativeness - how well training/testing data reflects real operational conditions. Current methods lack proper quantification of representativeness under limited data and uncertainty about true operational domains.

Method: Uses imprecise Bayesian inference to handle limited data and uncertain priors, producing interval-valued estimates of representativeness by comparing statistical distributions of scenario suite features with inferred Target Operational Domain features across operational categories.

Result: The method generates uncertainty-aware interval estimates of representativeness rather than single values, demonstrated through numerical examples comparing distributions across weather, road type, time of day categories under dependencies and prior uncertainty.

Conclusion: The proposed probabilistic framework provides a principled approach to quantify dataset representativeness for AI safety assurance, addressing uncertainty in operational domain knowledge and producing more reliable, uncertainty-aware assessments.

Abstract: Assuring the trustworthiness and safety of AI systems, e.g., autonomous vehicles (AV), depends critically on the data-related safety properties, e.g., representativeness, completeness, etc., of the datasets used for their training and testing. Among these properties, this paper focuses on representativeness-the extent to which the scenario-based data used for training and testing, reflect the operational conditions that the system is designed to operate safely in, i.e., Operational Design Domain (ODD) or expected to encounter, i.e., Target Operational Domain (TOD). We propose a probabilistic method that quantifies representativeness by comparing the statistical distribution of features encoded by the scenario suites with the corresponding distribution of features representing the TOD, acknowledging that the true TOD distribution is unknown, as it can only be inferred from limited data.
  We apply an imprecise Bayesian method to handle limited data and uncertain priors. The imprecise Bayesian formulation produces interval-valued, uncertainty-aware estimates of representativeness, rather than a single value. We present a numerical example comparing the distributions of the scenario suite and the inferred TOD across operational categories-weather, road type, time of day, etc., under dependencies and prior uncertainty. We estimate representativeness locally (between categories) and globally as an interval.

</details>


### [33] [Task Specific Sharpness Aware O-RAN Resource Management using Multi Agent Reinforcement Learning](https://arxiv.org/abs/2511.15002)
*Fatemeh Lotfi,Hossein Rajoli,Fatemeh Afghah*

Main category: cs.AI

TL;DR: Enhanced SAC algorithm with adaptive SAM regularization in MARL framework for O-RAN resource management, achieving 22% efficiency improvement and better QoS.


<details>
  <summary>Details</summary>
Motivation: DRL models struggle with robustness and generalizability in dynamic O-RAN environments, requiring improved training stability and targeted regularization.

Method: Distributed MARL framework combining SAC with adaptive SAM regularization driven by TD-error variance, plus dynamic ρ scheduling for exploration-exploitation balance.

Result: 22% improvement in resource allocation efficiency and superior QoS satisfaction across diverse O-RAN slices compared to conventional DRL approaches.

Conclusion: The adaptive SAM mechanism with TD-error variance-driven regularization effectively enhances robustness and generalization in dynamic O-RAN environments while maintaining learning efficiency.

Abstract: Next-generation networks utilize the Open Radio Access Network (O-RAN) architecture to enable dynamic resource management, facilitated by the RAN Intelligent Controller (RIC). While deep reinforcement learning (DRL) models show promise in optimizing network resources, they often struggle with robustness and generalizability in dynamic environments. This paper introduces a novel resource management approach that enhances the Soft Actor Critic (SAC) algorithm with Sharpness-Aware Minimization (SAM) in a distributed Multi-Agent RL (MARL) framework. Our method introduces an adaptive and selective SAM mechanism, where regularization is explicitly driven by temporal-difference (TD)-error variance, ensuring that only agents facing high environmental complexity are regularized. This targeted strategy reduces unnecessary overhead, improves training stability, and enhances generalization without sacrificing learning efficiency. We further incorporate a dynamic $ρ$ scheduling scheme to refine the exploration-exploitation trade-off across agents. Experimental results show our method significantly outperforms conventional DRL approaches, yielding up to a $22\%$ improvement in resource allocation efficiency and ensuring superior QoS satisfaction across diverse O-RAN slices.

</details>


### [34] [Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization](https://arxiv.org/abs/2511.15055)
*Jian-Ting Guo,Yu-Cheng Chen,Ping-Chun Hsieh,Kuo-Hao Ho,Po-Wei Huang,Ti-Rong Wu,I-Chen Wu*

Main category: cs.AI

TL;DR: MAQ is a human-like RL framework that uses Vector-Quantized VAE to distill human demonstrations into macro actions, improving human-likeness while maintaining high rewards.


<details>
  <summary>Details</summary>
Motivation: Current RL agents often exhibit unnatural behaviors compared to humans, raising concerns about interpretability and trustworthiness. The goal is to create human-like RL agents that behave more naturally.

Method: Formulates human-likeness as trajectory optimization and adapts receding-horizon control. Uses Macro Action Quantization (MAQ) with Vector-Quantized VAE to distill human demonstrations into macro actions.

Result: MAQ significantly improves human-likeness on D4RL Adroit benchmarks, increasing trajectory similarity scores and achieving the highest human-likeness rankings among all RL agents in human evaluation studies.

Conclusion: MAQ provides a promising direction for learning human-like RL agents and can be easily integrated into various off-the-shelf RL algorithms.

Abstract: Human-like agents have long been one of the goals in pursuing artificial intelligence. Although reinforcement learning (RL) has achieved superhuman performance in many domains, relatively little attention has been focused on designing human-like RL agents. As a result, many reward-driven RL agents often exhibit unnatural behaviors compared to humans, raising concerns for both interpretability and trustworthiness. To achieve human-like behavior in RL, this paper first formulates human-likeness as trajectory optimization, where the objective is to find an action sequence that closely aligns with human behavior while also maximizing rewards, and adapts the classic receding-horizon control to human-like learning as a tractable and efficient implementation. To achieve this, we introduce Macro Action Quantization (MAQ), a human-like RL framework that distills human demonstrations into macro actions via Vector-Quantized VAE. Experiments on D4RL Adroit benchmarks show that MAQ significantly improves human-likeness, increasing trajectory similarity scores, and achieving the highest human-likeness rankings among all RL agents in the human evaluation study. Our results also demonstrate that MAQ can be easily integrated into various off-the-shelf RL algorithms, opening a promising direction for learning human-like RL agents. Our code is available at https://rlg.iis.sinica.edu.tw/papers/MAQ.

</details>


### [35] [Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering](https://arxiv.org/abs/2511.15061)
*Haodong Chen,Guido Zuccon,Teerapong Leelanupab*

Main category: cs.AI

TL;DR: OpenBioLLM is a modular multi-agent framework that extends GeneGPT for genomic question answering, using open-source models to achieve comparable or better performance while reducing latency by 40-50% and addressing scalability, cost, and privacy concerns.


<details>
  <summary>Details</summary>
Motivation: To address limitations of GeneGPT's reliance on proprietary models, which limits scalability, increases costs, and raises privacy concerns, by developing an open-source alternative using smaller models without fine-tuning.

Method: Developed OpenBioLLM, a modular multi-agent framework with specialized agents for tool routing, query generation, and response validation, enabling coordinated reasoning and role-based task execution using open-source models like Llama 3.1, Qwen2.5, and Qwen2.5 Coder.

Result: OpenBioLLM matches or outperforms GeneGPT on over 90% of benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while reducing latency by 40-50% across benchmark tasks.

Conclusion: Open-source multi-agent systems show strong potential for genomic question answering, offering comparable performance to proprietary solutions with improved efficiency, scalability, and privacy benefits.

Abstract: Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization.
  In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution.
  OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.

</details>


### [36] [ProRAC: A Neuro-symbolic Method for Reasoning about Actions with LLM-based Progression](https://arxiv.org/abs/2511.15069)
*Haoyong Wu,Yongmei Liu*

Main category: cs.AI

TL;DR: ProRAC is a neuro-symbolic framework that uses LLMs to solve Reasoning about Actions and Change (RAC) problems by extracting actions and questions, progressively executing actions to reach final states, and evaluating queries.


<details>
  <summary>Details</summary>
Motivation: To develop an effective approach for tackling Reasoning about Actions and Change (RAC) problems by combining neural language models with symbolic reasoning.

Method: Extracts RAC elements (actions and questions), progressively executes each action to derive final states, then evaluates queries against progressed states to determine answers.

Result: Achieves strong performance across different RAC benchmarks, domains, LLM backbones, and types of RAC tasks.

Conclusion: ProRAC demonstrates effective neuro-symbolic reasoning for RAC problems with consistent performance across various settings.

Abstract: In this paper, we propose ProRAC (Progression-based Reasoning about Actions and Change), a neuro-symbolic framework that leverages LLMs to tackle RAC problems. ProRAC extracts fundamental RAC elements including actions and questions from the problem, progressively executes each action to derive the final state, and then evaluates the query against the progressed state to arrive at an answer. We evaluate ProRAC on several RAC benchmarks, and the results demonstrate that our approach achieves strong performance across different benchmarks, domains, LLM backbones, and types of RAC tasks.

</details>


### [37] [Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents](https://arxiv.org/abs/2511.15074)
*Henrik Bradland,Morten Goodwin,Vladimir I. Zadorozhny,Per-Arne Andersen*

Main category: cs.AI

TL;DR: Rogue One is a multi-agent LLM framework that uses three specialized agents (Scientist, Extractor, Tester) with qualitative feedback and RAG integration to automate feature engineering, outperforming state-of-the-art methods on 28 datasets.


<details>
  <summary>Details</summary>
Motivation: Existing AutoFE methods using LLMs are limited by monolithic architectures, simplistic quantitative feedback, and lack of systematic domain knowledge integration, necessitating a more sophisticated approach.

Method: A decentralized multi-agent system with three specialized agents that collaborate iteratively, featuring qualitative feedback mechanisms, flooding-pruning strategy, and RAG-based external knowledge integration.

Result: Significantly outperforms state-of-the-art methods on 19 classification and 9 regression datasets, and qualitatively demonstrates novel hypothesis discovery like identifying a potential myocardial biomarker.

Conclusion: Rogue One provides an effective framework for knowledge-informed automatic feature extraction that generates statistically powerful and semantically meaningful features, serving as a tool for scientific discovery.

Abstract: The performance of machine learning models on tabular data is critically dependent on high-quality feature engineering. While Large Language Models (LLMs) have shown promise in automating feature extraction (AutoFE), existing methods are often limited by monolithic LLM architectures, simplistic quantitative feedback, and a failure to systematically integrate external domain knowledge. This paper introduces Rogue One, a novel, LLM-based multi-agent framework for knowledge-informed automatic feature extraction. Rogue One operationalizes a decentralized system of three specialized agents-Scientist, Extractor, and Tester-that collaborate iteratively to discover, generate, and validate predictive features. Crucially, the framework moves beyond primitive accuracy scores by introducing a rich, qualitative feedback mechanism and a "flooding-pruning" strategy, allowing it to dynamically balance feature exploration and exploitation. By actively incorporating external knowledge via an integrated retrieval-augmented (RAG) system, Rogue One generates features that are not only statistically powerful but also semantically meaningful and interpretable. We demonstrate that Rogue One significantly outperforms state-of-the-art methods on a comprehensive suite of 19 classification and 9 regression datasets. Furthermore, we show qualitatively that the system surfaces novel, testable hypotheses, such as identifying a new potential biomarker in the myocardial dataset, underscoring its utility as a tool for scientific discovery.

</details>


### [38] [SafeRBench: A Comprehensive Benchmark for Safety Assessment in Large Reasoning Models](https://arxiv.org/abs/2511.15169)
*Xin Gao,Shaohan Yu,Zerui Chen,Yueming Lyu,Weichen Yu,Guanghao Li,Jiyao Liu,Jianxiong Gao,Jian Liang,Ziwei Liu,Chenyang Si*

Main category: cs.AI

TL;DR: SafeRBench is the first benchmark for evaluating Large Reasoning Models' safety throughout the entire reasoning process, addressing risks in inputs, intermediate reasoning, and final outputs through risk categorization, fine-grained analysis, and human validation.


<details>
  <summary>Details</summary>
Motivation: Existing safety evaluations focus mainly on output-level judgments and fail to capture dynamic risks that emerge during the reasoning process, such as subtly injected harmful content, gradual surfacing of risks, or misleading rationales in reasoning traces.

Method: Three main components: (1) Input characterization with risk categories and levels accounting for affected groups and severity, (2) Fine-grained output analysis using micro-thought chunking to segment reasoning traces into coherent units across ten safety dimensions, (3) Human safety alignment to validate LLM-based evaluations against human annotations.

Result: Evaluations on 19 Large Reasoning Models demonstrate that SafeRBench enables detailed, multidimensional safety assessment, providing insights into risks and protective mechanisms from multiple perspectives.

Conclusion: SafeRBench represents a comprehensive approach to LRM safety evaluation that captures dynamic risks throughout the reasoning process, offering a more complete safety assessment framework compared to traditional output-only evaluations.

Abstract: Large Reasoning Models (LRMs) improve answer quality through explicit chain-of-thought, yet this very capability introduces new safety risks: harmful content can be subtly injected, surface gradually, or be justified by misleading rationales within the reasoning trace. Existing safety evaluations, however, primarily focus on output-level judgments and rarely capture these dynamic risks along the reasoning process. In this paper, we present SafeRBench, the first benchmark that assesses LRM safety end-to-end -- from inputs and intermediate reasoning to final outputs. (1) Input Characterization: We pioneer the incorporation of risk categories and levels into input design, explicitly accounting for affected groups and severity, and thereby establish a balanced prompt suite reflecting diverse harm gradients. (2) Fine-Grained Output Analysis: We introduce a micro-thought chunking mechanism to segment long reasoning traces into semantically coherent units, enabling fine-grained evaluation across ten safety dimensions. (3) Human Safety Alignment: We validate LLM-based evaluations against human annotations specifically designed to capture safety judgments. Evaluations on 19 LRMs demonstrate that SafeRBench enables detailed, multidimensional safety assessment, offering insights into risks and protective mechanisms from multiple perspectives.

</details>


### [39] [HISE-KT: Synergizing Heterogeneous Information Networks and LLMs for Explainable Knowledge Tracing with Meta-Path Optimization](https://arxiv.org/abs/2511.15191)
*Zhiyi Duan,Zixing Shi,Hongyu Yuan,Qi Wang*

Main category: cs.AI

TL;DR: HISE-KT is a knowledge tracing framework that integrates heterogeneous information networks (HINs) with large language models (LLMs) to improve prediction accuracy and provide explainable analysis by automatically assessing meta-path quality and retrieving similar student trajectories.


<details>
  <summary>Details</summary>
Motivation: Existing KT methods based on HINs introduce noise through manual/random meta-path selection and lack quality assessment, while LLM-based methods ignore cross-student information. Both struggle with consistent accuracy and evidence-based explanations.

Method: Builds multi-relationship HIN with diverse node types, uses LLM to intelligently score/filter meta-path instances, implements similar student retrieval based on meta-paths, and uses structured prompts to integrate target student history with similar trajectories for LLM prediction.

Result: Experiments on four public datasets show HISE-KT outperforms existing KT baselines in both prediction performance and interpretability.

Conclusion: The HIN-LLM synergistic framework successfully addresses limitations of existing KT methods by automating meta-path quality assessment and providing evidence-backed, explainable analysis through similar student trajectory integration.

Abstract: Knowledge Tracing (KT) aims to mine students' evolving knowledge states and predict their future question-answering performance. Existing methods based on heterogeneous information networks (HINs) are prone to introducing noises due to manual or random selection of meta-paths and lack necessary quality assessment of meta-path instances. Conversely, recent large language models (LLMs)-based methods ignore the rich information across students, and both paradigms struggle to deliver consistently accurate and evidence-based explanations. To address these issues, we propose an innovative framework, HIN-LLM Synergistic Enhanced Knowledge Tracing (HISE-KT), which seamlessly integrates HINs with LLMs. HISE-KT first builds a multi-relationship HIN containing diverse node types to capture the structural relations through multiple meta-paths. The LLM is then employed to intelligently score and filter meta-path instances and retain high-quality paths, pioneering automated meta-path quality assessment. Inspired by educational psychology principles, a similar student retrieval mechanism based on meta-paths is designed to provide a more valuable context for prediction. Finally, HISE-KT uses a structured prompt to integrate the target student's history with the retrieved similar trajectories, enabling the LLM to generate not only accurate predictions but also evidence-backed, explainable analysis reports. Experiments on four public datasets show that HISE-KT outperforms existing KT baselines in both prediction performance and interpretability.

</details>


### [40] [As If We've Met Before: LLMs Exhibit Certainty in Recognizing Seen Files](https://arxiv.org/abs/2511.15192)
*Haodong Li,Jingqi Zhang,Xiao Cheng,Peihua Mai,Haoyu Wang,Yang Pan*

Main category: cs.AI

TL;DR: COPYCHECK is a novel framework that uses uncertainty signals to detect whether copyrighted content was used in LLM training sets, achieving over 90% balanced accuracy by leveraging LLM overconfidence patterns.


<details>
  <summary>Details</summary>
Motivation: Address concerns about unauthorized use of copyrighted material in LLM training datasets, overcoming limitations of existing Membership Inference Attacks (MIAs) that struggle with LLM overconfidence, limited ground truth data access, and empirical threshold dependencies.

Method: Leverages uncertainty signals to distinguish between seen (training) and unseen (non-training) content, implements strategic file segmentation into smaller snippets, and uses uncertainty-guided unsupervised clustering to eliminate empirically tuned thresholds.

Result: Achieves average balanced accuracy of 90.1% on LLaMA 7b and 91.6% on LLaMA2 7b, with over 90% relative improvement compared to SOTA baseline, reaching up to 93.8% balanced accuracy. Maintains strong performance across architectures including GPT-J 6B.

Conclusion: COPYCHECK presents the first application of uncertainty for copyright detection in LLMs, offering practical tools for training data transparency by turning LLM overconfidence from a limitation into an asset for reliable copyright violation detection.

Abstract: The remarkable language ability of Large Language Models (LLMs) stems from extensive training on vast datasets, often including copyrighted material, which raises serious concerns about unauthorized use. While Membership Inference Attacks (MIAs) offer potential solutions for detecting such violations, existing approaches face critical limitations and challenges due to LLMs' inherent overconfidence, limited access to ground truth training data, and reliance on empirically determined thresholds.
  We present COPYCHECK, a novel framework that leverages uncertainty signals to detect whether copyrighted content was used in LLM training sets. Our method turns LLM overconfidence from a limitation into an asset by capturing uncertainty patterns that reliably distinguish between ``seen" (training data) and ``unseen" (non-training data) content. COPYCHECK further implements a two-fold strategy: (1) strategic segmentation of files into smaller snippets to reduce dependence on large-scale training data, and (2) uncertainty-guided unsupervised clustering to eliminate the need for empirically tuned thresholds. Experiment results show that COPYCHECK achieves an average balanced accuracy of 90.1% on LLaMA 7b and 91.6% on LLaMA2 7b in detecting seen files. Compared to the SOTA baseline, COPYCHECK achieves over 90% relative improvement, reaching up to 93.8\% balanced accuracy. It further exhibits strong generalizability across architectures, maintaining high performance on GPT-J 6B. This work presents the first application of uncertainty for copyright detection in LLMs, offering practical tools for training data transparency.

</details>


### [41] [SOLID: a Framework of Synergizing Optimization and LLMs for Intelligent Decision-Making](https://arxiv.org/abs/2511.15202)
*Yinsheng Wang,Tario G You,Léonard Boussioux,Shan Liu*

Main category: cs.AI

TL;DR: SOLID is a framework that combines mathematical optimization with large language models (LLMs) for intelligent decision-making, using dual prices and deviation penalties to enable iterative collaboration between optimization and LLM agents while maintaining modularity and data privacy.


<details>
  <summary>Details</summary>
Motivation: To create a synergistic framework that leverages both the mathematical rigor of optimization and the contextual understanding of LLMs to improve decision-making quality across various domains.

Method: The framework facilitates iterative collaboration between optimization and LLM agents through dual prices and deviation penalties, with theoretical convergence guarantees under convexity assumptions. It was evaluated on a stock portfolio investment case using historical prices and financial news.

Result: Empirical results demonstrate convergence under various scenarios and show improved annualized returns compared to baseline optimizer-only methods, validating the synergy between optimization and LLM agents.

Conclusion: SOLID offers a promising framework for advancing automated and intelligent decision-making across diverse domains by effectively integrating mathematical optimization with large language models.

Abstract: This paper introduces SOLID (Synergizing Optimization and Large Language Models for Intelligent Decision-Making), a novel framework that integrates mathematical optimization with the contextual capabilities of large language models (LLMs). SOLID facilitates iterative collaboration between optimization and LLMs agents through dual prices and deviation penalties. This interaction improves the quality of the decisions while maintaining modularity and data privacy. The framework retains theoretical convergence guarantees under convexity assumptions, providing insight into the design of LLMs prompt. To evaluate SOLID, we applied it to a stock portfolio investment case with historical prices and financial news as inputs. Empirical results demonstrate convergence under various scenarios and indicate improved annualized returns compared to a baseline optimizer-only method, validating the synergy of the two agents. SOLID offers a promising framework for advancing automated and intelligent decision-making across diverse domains.

</details>


### [42] [Efficiency Will Not Lead to Sustainable Reasoning AI](https://arxiv.org/abs/2511.15259)
*Philipp Wiesner,Daniel W. O'Neill,Francesca Larosa,Odej Kao*

Main category: cs.AI

TL;DR: This paper argues that efficiency improvements alone cannot ensure sustainable reasoning AI systems, as they lack natural saturation points and continue to scale exponentially with compute investments. The authors propose embedding explicit limits into AI optimization and governance.


<details>
  <summary>Details</summary>
Motivation: AI research is shifting toward complex problem solving with multi-step reasoning, but emerging reasoning AI lacks the natural saturation thresholds that historically stabilized computing's energy footprint. As efficiency improvements approach physical limits, reasoning AI continues to scale exponentially with compute investments.

Method: The paper discusses the fundamental challenge that reasoning AI performance is no longer limited by training data but scales with exponential compute investments in both training and inference. It analyzes how this differs from historical computing patterns where efficiency gains and demand saturation stabilized energy consumption.

Result: The analysis reveals that efficiency alone cannot address the sustainability challenges of reasoning AI systems, as they lack the natural saturation mechanisms that previously contained computing's energy footprint.

Conclusion: The paper concludes that research and policy must move beyond efficiency-focused approaches and embed explicit limits into the optimization and governance of reasoning AI systems to ensure long-term sustainability.

Abstract: AI research is increasingly moving toward complex problem solving, where models are optimized not only for pattern recognition but for multi-step reasoning. Historically, computing's global energy footprint has been stabilized by sustained efficiency gains and natural saturation thresholds in demand. But as efficiency improvements are approaching physical limits, emerging reasoning AI lacks comparable saturation points: performance is no longer limited by the amount of available training data but continues to scale with exponential compute investments in both training and inference. This paper argues that efficiency alone will not lead to sustainable reasoning AI and discusses research and policy directions to embed explicit limits into the optimization and governance of such systems.

</details>


### [43] [Realist and Pluralist Conceptions of Intelligence and Their Implications on AI Research](https://arxiv.org/abs/2511.15282)
*Ninell Oldenburg,Ruchira Dhar,Anders Søgaard*

Main category: cs.AI

TL;DR: The paper identifies two competing conceptions of intelligence in AI research: Intelligence Realism (universal, measurable intelligence) and Intelligence Pluralism (diverse, context-dependent capacities), and shows how these implicit assumptions shape research methodologies, interpretations of evidence, and risk assessments.


<details>
  <summary>Details</summary>
Motivation: To clarify fundamental disagreements in AI research by making explicit the underlying assumptions about intelligence that shape how researchers approach methodology, interpret empirical findings, and assess AI risks.

Method: Analysis of current debates in AI research to demonstrate how implicit conceptions of intelligence influence empirical evidence interpretation across multiple areas, examining methodological approaches, interpretive frameworks, and risk assessments.

Result: Identified that Intelligence Realism and Pluralism generate fundamentally different research approaches: in methodology (model selection, benchmarks, validation), interpretation (capability emergence, system limitations), and risk assessment (superintelligence vs. diverse domain-specific threats).

Conclusion: Making explicit these underlying assumptions about intelligence can contribute to clearer understanding of disagreements in AI research and help researchers recognize how their fundamental conceptions shape their approaches across methodology, interpretation, and risk assessment.

Abstract: In this paper, we argue that current AI research operates on a spectrum between two different underlying conceptions of intelligence: Intelligence Realism, which holds that intelligence represents a single, universal capacity measurable across all systems, and Intelligence Pluralism, which views intelligence as diverse, context-dependent capacities that cannot be reduced to a single universal measure. Through an analysis of current debates in AI research, we demonstrate how the conceptions remain largely implicit yet fundamentally shape how empirical evidence gets interpreted across a wide range of areas. These underlying views generate fundamentally different research approaches across three areas. Methodologically, they produce different approaches to model selection, benchmark design, and experimental validation. Interpretively, they lead to contradictory readings of the same empirical phenomena, from capability emergence to system limitations. Regarding AI risk, they generate categorically different assessments: realists view superintelligence as the primary risk and search for unified alignment solutions, while pluralists see diverse threats across different domains requiring context-specific solutions. We argue that making explicit these underlying assumptions can contribute to a clearer understanding of disagreements in AI research.

</details>


### [44] [Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration](https://arxiv.org/abs/2511.15351)
*Yifu Guo,Zishan Xu,Zhiyuan Yao,Yuquan Lu,Jiaye Lin,Sen Hu,Zhenheng Tang,Yingchao Li,Huacan Wang,Ronghao Chen*

Main category: cs.AI

TL;DR: Octopus is a new multimodal reasoning paradigm that autonomously explores diverse reasoning pathways and dynamically selects appropriate capabilities, achieving state-of-the-art performance on comprehensive benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal reasoning models lack the human-like ability to autonomously explore diverse reasoning pathways and adapt to dynamically changing capability requirements, while humans exhibit complementary thinking abilities that current methods only partially cover.

Method: Proposes Octopus with six core capabilities for multimodal reasoning, enabling autonomous exploration during reasoning and dynamic selection of the most appropriate capability based on current state.

Result: Experimental results show Octopus achieves the best performance on the vast majority of tasks in the comprehensive Octopus-Bench evaluation benchmark.

Conclusion: The research highlights the crucial role of capability coordination in agentic multimodal reasoning, demonstrating that autonomous exploration and dynamic capability selection significantly improve performance across diverse reasoning tasks.

Abstract: Existing multimodal reasoning models and frameworks suffer from fundamental architectural limitations: most lack the human-like ability to autonomously explore diverse reasoning pathways-whether in direct inference, tool-driven visual exploration, programmatic visual manipulation, or intrinsic visual imagination. Consequently, they struggle to adapt to dynamically changing capability requirements in real-world tasks. Meanwhile, humans exhibit a complementary set of thinking abilities when addressing such tasks, whereas existing methods typically cover only a subset of these dimensions. Inspired by this, we propose Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration, a new paradigm for multimodal agentic reasoning. We define six core capabilities essential for multimodal reasoning and organize a comprehensive evaluation benchmark, Octopus-Bench, accordingly. Octopus is capable of autonomously exploring during reasoning and dynamically selecting the most appropriate capability based on the current state. Experimental results show that Octopus achieves the best performance on the vast majority of tasks in Octopus-Bench, highlighting the crucial role of capability coordination in agentic multimodal reasoning.

</details>


### [45] [Terra Nova: A Comprehensive Challenge Environment for Intelligent Agents](https://arxiv.org/abs/2511.15378)
*Trevor McInroe*

Main category: cs.AI

TL;DR: Terra Nova is a new comprehensive challenge environment for RL research inspired by Civilization V that integrates multiple canonical RL challenges simultaneously in a single environment, requiring deep reasoning across interacting variables rather than just task switching.


<details>
  <summary>Details</summary>
Motivation: To create a benchmark that tests integrated, long-horizon understanding across multiple interacting RL challenges, distinguishing from multitask benchmarks that only assess policy cataloging and switching.

Method: Developed a Civilization V-inspired environment where partial observability, credit assignment, representation learning, enormous action spaces, and other canonical RL challenges arise simultaneously in a single integrated environment.

Result: Created Terra Nova as a comprehensive challenge environment that demands deep reasoning across many interacting variables rather than simple task aggregation.

Conclusion: Terra Nova provides a more meaningful benchmark for evaluating RL agents' ability to perform integrated reasoning across multiple simultaneous challenges, advancing beyond traditional multitask environments.

Abstract: We introduce Terra Nova, a new comprehensive challenge environment (CCE) for reinforcement learning (RL) research inspired by Civilization V. A CCE is a single environment in which multiple canonical RL challenges (e.g., partial observability, credit assignment, representation learning, enormous action spaces, etc.) arise simultaneously. Mastery therefore demands integrated, long-horizon understanding across many interacting variables. We emphasize that this definition excludes challenges that only aggregate unrelated tasks in independent, parallel streams (e.g., learning to play all Atari games at once). These aggregated multitask benchmarks primarily asses whether an agent can catalog and switch among unrelated policies rather than test an agent's ability to perform deep reasoning across many interacting challenges.

</details>


### [46] [IPR-1: Interactive Physical Reasoner](https://arxiv.org/abs/2511.15407)
*Mingyu Zhang,Lifeng Zhuo,Tianxi Tan,Guocan Xie,Xian Nie,Yan Li,Renjie Zhao,Zizhu He,Ziyu Wang,Jiting Cai,Yong-Lu Li*

Main category: cs.AI

TL;DR: IPR (Interactive Physical Reasoner) combines world-model rollouts with VLM policies and PhysCode action representation to achieve human-like physical reasoning across Survival, Curiosity, and Utility levels, matching GPT-5 performance and showing improvement with more training games.


<details>
  <summary>Details</summary>
Motivation: To investigate whether agents can acquire human-like reasoning through interaction and improve with experience, similar to how humans learn physics and causality from observation and interaction.

Method: Proposed IPR using world-model rollouts to score and reinforce VLM policies, with PhysCode providing physics-centric action representation that aligns semantic intent with dynamics for shared action space.

Result: IPR performs robustly across three human-like reasoning levels, matches GPT-5 overall, surpasses it on Curiosity, improves with more training games and interaction steps, and zero-shot transfers to unseen games.

Conclusion: Physics-centric interaction provides a path to steadily improving physical reasoning, with complementary strengths of world models and VLMs enabling human-like reasoning capabilities.

Abstract: Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. We study this in a Game-to-Unseen (G2U) setting, curating 1,000+ heterogeneous games with diverse physical and causal mechanisms, and evaluate at three human-like levels: Survival, Curiosity, Utility, from primitive intuition to goal-driven reasoning. Our analysis reveals complementary failures: VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on three levels, matches GPT-5 overall, and surpasses it on Curiosity. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning.

</details>


### [47] [Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining](https://arxiv.org/abs/2511.15456)
*Qian'ang Mao,Yuxuan Zhang,Jiaman Chen,Wenjun Zhou,Jiaqi Yan*

Main category: cs.AI

TL;DR: The paper proposes TIM, a framework that uses a multi-agent LLM system to infer user intents in DeFi transactions by decomposing complex analyses into subtasks handled by specialized agents, significantly outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Understanding user intent in DeFi transactions is challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs, with existing methods lacking deep semantic insight.

Method: TIM framework with DeFi intent taxonomy based on grounded theory, multi-agent LLM system featuring Meta-Level Planner for coordination, Question Solvers for multi-modal data tasks, and Cognitive Evaluator to mitigate hallucinations and ensure verifiability.

Result: TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines in experiments, while also analyzing core challenges in intent inference.

Conclusion: The work provides more reliable understanding of user motivations in DeFi and offers context-aware explanations for complex blockchain activity.

Abstract: As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on/off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.

</details>


### [48] [Exploring the use of AI authors and reviewers at Agents4Science](https://arxiv.org/abs/2511.15534)
*Federico Bianchi,Owen Queen,Nitya Thakkar,Eric Sun,James Zou*

Main category: cs.AI

TL;DR: AI agents served as both authors and reviewers in the first Agents4Science conference, with humans as collaborators, exploring AI capabilities in scientific research and review processes.


<details>
  <summary>Details</summary>
Motivation: To investigate fundamental questions about AI agents' capabilities as scientists and reviewers, and explore human-AI collaboration in scientific research.

Method: Organized Agents4Science conference where AI agents acted as primary authors and reviewers, with humans serving as co-authors and co-reviewers.

Result: Key learnings were obtained about AI agents' performance in scientific roles and human-AI collaboration dynamics.

Conclusion: The conference provided important insights into AI capabilities for scientific research and review, with implications for future human-AI collaboration in science.

Abstract: There is growing interest in using AI agents for scientific research, yet fundamental questions remain about their capabilities as scientists and reviewers. To explore these questions, we organized Agents4Science, the first conference in which AI agents serve as both primary authors and reviewers, with humans as co-authors and co-reviewers. Here, we discuss the key learnings from the conference and their implications for human-AI collaboration in science.

</details>


### [49] [What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity](https://arxiv.org/abs/2511.15593)
*Alexis Audran-Reiss,Jordi Armengol Estapé,Karen Hambardzumyan,Amar Budhiraja,Martin Josifoski,Edan Toledo,Rishi Hazra,Despoina Magka,Michael Shvartsman,Parth Pathak,Justine T Kao,Lucia Cipolina-Kun,Bhavul Gauri,Jean-Christophe Gagnon-Audet,Emanuel Tewolde,Jenny Zhang,Taco Cohen,Yossi Adi,Tatiana Shavrina,Yoram Bachrach*

Main category: cs.AI

TL;DR: The paper examines how ideation diversity affects AI research agent performance, finding that higher diversity correlates with better results across different models and agent scaffolds.


<details>
  <summary>Details</summary>
Motivation: To understand the key factors driving AI research agent success, particularly the role of ideation diversity in accelerating scientific progress through automated machine learning model development.

Method: Analyzed agent trajectories on MLE-bench across different models and scaffolds, conducted controlled experiments manipulating ideation diversity, and used multiple evaluation metrics beyond standard scoring.

Result: Higher-performing agents showed increased ideation diversity, and controlled experiments confirmed that higher ideation diversity directly leads to stronger performance across various metrics.

Conclusion: Ideation diversity is a crucial factor in AI research agent success, with increased diversity consistently correlating with improved performance across different evaluation approaches.

Abstract: AI research agents offer the promise to accelerate scientific progress by automating the design, implementation, and training of machine learning models. However, the field is still in its infancy, and the key factors driving the success or failure of agent trajectories are not fully understood. We examine the role that ideation diversity plays in agent performance. First, we analyse agent trajectories on MLE-bench, a well-known benchmark to evaluate AI research agents, across different models and agent scaffolds. Our analysis reveals that different models and agent scaffolds yield varying degrees of ideation diversity, and that higher-performing agents tend to have increased ideation diversity. Further, we run a controlled experiment where we modify the degree of ideation diversity, demonstrating that higher ideation diversity results in stronger performance. Finally, we strengthen our results by examining additional evaluation metrics beyond the standard medal-based scoring of MLE-bench, showing that our findings still hold across other agent performance metrics.

</details>
